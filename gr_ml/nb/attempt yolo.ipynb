{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# tutorial https://pytorch.org/tutorials/intermediate/torchvision_tutorial.html\n",
    "\n",
    "## Note to Jose\n",
    "Working version but not fully understood yet\n",
    "The next commented think can allow to remove stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fatal: destination path 'vision' already exists and is not an empty directory.\n",
      "error: pathspec 'v0.3.0' did not match any file(s) known to git.\n",
      "/home/jose/ros_ws/src/gr_perception/gr_ml/nb\n"
     ]
    }
   ],
   "source": [
    "# Download TorchVision repo to use some files from\n",
    "# references/detection\n",
    "!git clone https://github.com/pytorch/vision.git\n",
    "!git checkout v0.3.0\n",
    "!pwd\n",
    "!cp vision/references/detection/utils.py .\n",
    "!cp vision/references/detection/transforms.py .\n",
    "!cp vision/references/detection/coco_eval.py .\n",
    "!cp vision/references/detection/engine.py .\n",
    "!cp vision/references/detection/coco_utils.py ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fatal: destination path 'yolov3' already exists and is not an empty directory.\r\n"
     ]
    }
   ],
   "source": [
    "# Download TorchVision repo to use some files from\n",
    "# references/detection\n",
    "!git clone git@github.com:jcmayoral/PyTorch-YOLOv3.git yolov3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "107\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(107, 107)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from yolov3.models import Darknet\n",
    "\n",
    "yolobackbone = Darknet(config_path=\"yolov3/config/yolov3-custom.cfg\")\n",
    "len(yolobackbone.module_list), yolobackbone.out_channels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from PIL import Image\n",
    "\n",
    "\n",
    "class PennFudanDataset(object):\n",
    "    def __init__(self, root, transforms):\n",
    "        self.root = root\n",
    "        self.transforms = transforms\n",
    "        # load all image files, sorting them to\n",
    "        # ensure that they are aligned\n",
    "        self.imgs = list(sorted(os.listdir(os.path.join(root, \"PNGImages\"))))\n",
    "        self.masks = list(sorted(os.listdir(os.path.join(root, \"PedMasks\"))))\n",
    "    \n",
    "    def concat_masks(self, mask):\n",
    "        pshape = mask.shape\n",
    "        arr = np.zeros((pshape[2],pshape[3]),dtype=np.uint8)\n",
    "        #for i in range(pshape[0]):\n",
    "        #    arr += mask[i,0].mul(255).byte().cpu().numpy()\n",
    "        return Image.fromarray(arr)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # load images ad masks\n",
    "        img_path = os.path.join(self.root, \"PNGImages\", self.imgs[idx])\n",
    "        mask_path = os.path.join(self.root, \"PedMasks\", self.masks[idx])\n",
    "        img = Image.open(img_path).convert(\"RGB\")\n",
    "        # note that we haven't converted the mask to RGB,\n",
    "        # because each color corresponds to a different instance\n",
    "        # with 0 being background\n",
    "        mask = Image.open(mask_path)\n",
    "        # convert the PIL Image into a numpy array\n",
    "        mask = np.array(mask)\n",
    "        # instances are encoded as different colors\n",
    "        obj_ids = np.unique(mask)\n",
    "        # first id is the background, so remove it\n",
    "        obj_ids = obj_ids[1:]\n",
    "\n",
    "        # split the color-encoded mask into a set\n",
    "        # of binary masks\n",
    "        masks = mask == obj_ids[:, None, None]\n",
    "\n",
    "        # get bounding box coordinates for each mask\n",
    "        num_objs = len(obj_ids)\n",
    "        boxes = []\n",
    "        for i in range(num_objs):\n",
    "            pos = np.where(masks[i])\n",
    "            xmin = np.min(pos[1])\n",
    "            xmax = np.max(pos[1])\n",
    "            ymin = np.min(pos[0])\n",
    "            ymax = np.max(pos[0])\n",
    "            boxes.append([xmin, ymin, xmax, ymax])\n",
    "\n",
    "        # convert everything into a torch.Tensor\n",
    "        boxes = torch.as_tensor(boxes, dtype=torch.float32)\n",
    "        # there is only one class\n",
    "        labels = torch.ones((num_objs,), dtype=torch.int64)\n",
    "        masks = torch.as_tensor(masks, dtype=torch.uint8)\n",
    "\n",
    "        image_id = torch.tensor([idx])\n",
    "        area = (boxes[:, 3] - boxes[:, 1]) * (boxes[:, 2] - boxes[:, 0])\n",
    "        # suppose all instances are not crowd\n",
    "        iscrowd = torch.zeros((num_objs,), dtype=torch.int64)\n",
    "\n",
    "        target = {}\n",
    "        target[\"boxes\"] = boxes\n",
    "        target[\"labels\"] = labels\n",
    "        target[\"masks\"] = masks\n",
    "        target[\"image_id\"] = image_id\n",
    "        target[\"area\"] = area\n",
    "        target[\"iscrowd\"] = iscrowd\n",
    "        #target[\"safety_level\"] = 0.5\n",
    "        masks = self.concat_masks(masks)\n",
    "\n",
    "        if self.transforms is not None:\n",
    "            img, target = self.transforms(img, target)\n",
    "        \n",
    "        print(img.shape, masks.shape)\n",
    "        #iou_scores, class_mask, obj_mask, noobj_mask, tx, ty, tw, th, tcls, tconf = build_targets\n",
    "        target = [1,labels, mask]\n",
    "        return img, target #target\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.imgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = yolobackbone\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "82,352,981\n",
      "61,529,119\n",
      "Is model smaller than default?  True\n",
      "Scale Factor  1.3384391380607936\n"
     ]
    }
   ],
   "source": [
    "default_nparameters = 82352981\n",
    "new_nparams = sum(p.numel() for p in model.parameters())\n",
    "print (f\"{default_nparameters:,}\")\n",
    "print (f\"{new_nparams:,}\")\n",
    "print (\"Is model smaller than default? \" , new_nparams<default_nparameters)\n",
    "print (\"Scale Factor \" , default_nparameters/new_nparams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import transforms as T\n",
    "\n",
    "\n",
    "def get_transform(train):\n",
    "    transforms = []\n",
    "    # converts the image, a PIL image, into a PyTorch Tensor\n",
    "    transforms.append(T.ToTensor())\n",
    "    if train:\n",
    "        # during training, randomly flip the training images\n",
    "        # and ground-truth for data augmentation\n",
    "        transforms.append(T.RandomHorizontalFlip(0.5))\n",
    "    return T.Compose(transforms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n# construct an optimizer\\nparams = [p for p in model.parameters() if p.requires_grad]\\noptimizer = torch.optim.SGD(params, lr=0.005,\\n                            momentum=0.9, weight_decay=0.0005)\\n# and a learning rate scheduler\\nlr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer,\\n                                               step_size=3,\\n                                               gamma=0.1)\\n\\n# let\\'s train it for 10 epochs\\nnum_epochs = 10\\n\\nfor epoch in range(num_epochs):\\n    # train for one epoch, printing every 10 iterations\\n    train_one_epoch(model, optimizer, data_loader, device, epoch, print_freq=10)\\n    # update the learning rate\\n    lr_scheduler.step()\\n    # evaluate on the test dataset\\n    evaluate(model, data_loader_test, device=device)\\n\\nprint(\"That\\'s it!\")\\n'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "# train on the GPU or on the CPU, if a GPU is not available\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "\n",
    "# our dataset has two classes only - background and person\n",
    "num_classes = 2\n",
    "# use our dataset and defined transformations\n",
    "dataset = PennFudanDataset('/media/datasets/PennFundan/PennFudanPed', get_transform(train=True))\n",
    "\n",
    "# split the dataset in train and test set\n",
    "#indices = torch.randperm(int(len(dataset))).tolist()\n",
    "indices = torch.randperm(len(dataset)).tolist()\n",
    "dataset = torch.utils.data.Subset(dataset, indices[:30])\n",
    "\n",
    "# define training and validation data loaders\n",
    "data_loader = torch.utils.data.DataLoader(\n",
    "    dataset, batch_size=10, shuffle=True, num_workers=2)\n",
    "#    collate_fn=utils.collate_fn)\n",
    "\n",
    "\n",
    "dataset_test = PennFudanDataset('/media/datasets/PennFundan/PennFudanPed', get_transform(train=False))\n",
    "\n",
    "dataset_test = torch.utils.data.Subset(dataset_test, indices[-12:])\n",
    "\n",
    "data_loader_test = torch.utils.data.DataLoader(\n",
    "    dataset_test, batch_size=1, shuffle=False, num_workers=2)\n",
    "#    collate_fn=utils.collate_fn)\n",
    "\n",
    "\n",
    "# get the model using our helper function\n",
    "#model = get_model_instance_segmentation(num_classes)\n",
    "\n",
    "# move model to the right device\n",
    "model.to(device)\n",
    "\n",
    "\"\"\"\n",
    "# construct an optimizer\n",
    "params = [p for p in model.parameters() if p.requires_grad]\n",
    "optimizer = torch.optim.SGD(params, lr=0.005,\n",
    "                            momentum=0.9, weight_decay=0.0005)\n",
    "# and a learning rate scheduler\n",
    "lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer,\n",
    "                                               step_size=3,\n",
    "                                               gamma=0.1)\n",
    "\n",
    "# let's train it for 10 epochs\n",
    "num_epochs = 10\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # train for one epoch, printing every 10 iterations\n",
    "    train_one_epoch(model, optimizer, data_loader, device, epoch, print_freq=10)\n",
    "    # update the learning rate\n",
    "    lr_scheduler.step()\n",
    "    # evaluate on the test dataset\n",
    "    evaluate(model, data_loader_test, device=device)\n",
    "\n",
    "print(\"That's it!\")\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "nepochs = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "Caught IndexError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"/home/jose/anaconda3/envs/testing2/lib/python3.7/site-packages/torch/utils/data/_utils/worker.py\", line 178, in _worker_loop\n    data = fetcher.fetch(index)\n  File \"/home/jose/anaconda3/envs/testing2/lib/python3.7/site-packages/torch/utils/data/_utils/fetch.py\", line 44, in fetch\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"/home/jose/anaconda3/envs/testing2/lib/python3.7/site-packages/torch/utils/data/_utils/fetch.py\", line 44, in <listcomp>\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"/home/jose/anaconda3/envs/testing2/lib/python3.7/site-packages/torch/utils/data/dataset.py\", line 257, in __getitem__\n    return self.dataset[self.indices[idx]]\n  File \"<ipython-input-4-f443049b6f69>\", line 72, in __getitem__\n    masks = self.concat_masks(masks)\n  File \"<ipython-input-4-f443049b6f69>\", line 17, in concat_masks\n    arr = np.zeros((pshape[2],pshape[3]),dtype=np.uint8)\nIndexError: tuple index out of range\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-51eab023e1b0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0mstart_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mbatch_i\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mimgs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m         \u001b[0mbatches_done\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_loader\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mbatch_i\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimgs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/testing2/lib/python3.7/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    817\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    818\u001b[0m                 \u001b[0;32mdel\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_task_info\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 819\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_process_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    820\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    821\u001b[0m     \u001b[0mnext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m__next__\u001b[0m  \u001b[0;31m# Python 2 compatibility\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/testing2/lib/python3.7/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_process_data\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m    844\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_try_put_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    845\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mExceptionWrapper\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 846\u001b[0;31m             \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreraise\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    847\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    848\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/testing2/lib/python3.7/site-packages/torch/_utils.py\u001b[0m in \u001b[0;36mreraise\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    383\u001b[0m             \u001b[0;31m# (https://bugs.python.org/issue2651), so we work around it.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    384\u001b[0m             \u001b[0mmsg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mKeyErrorMessage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 385\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexc_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m: Caught IndexError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"/home/jose/anaconda3/envs/testing2/lib/python3.7/site-packages/torch/utils/data/_utils/worker.py\", line 178, in _worker_loop\n    data = fetcher.fetch(index)\n  File \"/home/jose/anaconda3/envs/testing2/lib/python3.7/site-packages/torch/utils/data/_utils/fetch.py\", line 44, in fetch\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"/home/jose/anaconda3/envs/testing2/lib/python3.7/site-packages/torch/utils/data/_utils/fetch.py\", line 44, in <listcomp>\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"/home/jose/anaconda3/envs/testing2/lib/python3.7/site-packages/torch/utils/data/dataset.py\", line 257, in __getitem__\n    return self.dataset[self.indices[idx]]\n  File \"<ipython-input-4-f443049b6f69>\", line 72, in __getitem__\n    masks = self.concat_masks(masks)\n  File \"<ipython-input-4-f443049b6f69>\", line 17, in concat_masks\n    arr = np.zeros((pshape[2],pshape[3]),dtype=np.uint8)\nIndexError: tuple index out of range\n"
     ]
    }
   ],
   "source": [
    "from torch.autograd import Variable\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters())\n",
    "\n",
    "metrics = [\n",
    "    \"grid_size\",\n",
    "    \"loss\",\n",
    "    \"x\",\n",
    "    \"y\",\n",
    "    \"w\",\n",
    "    \"h\",\n",
    "    \"conf\",\n",
    "    \"cls\",\n",
    "    \"cls_acc\",\n",
    "    \"recall50\",\n",
    "    \"recall75\",\n",
    "    \"precision\",\n",
    "    \"conf_obj\",\n",
    "    \"conf_noobj\",\n",
    "]\n",
    "\n",
    "for epoch in range(nepochs):\n",
    "    model.train()\n",
    "    start_time = time.time()\n",
    "    for batch_i, (imgs, targets) in enumerate(data_loader):\n",
    "        batches_done = len(data_loader) * epoch + batch_i\n",
    "        print (type(imgs))\n",
    "        print (type(targets))\n",
    "        \n",
    "        imgs = Variable(imgs.to(device))\n",
    "        targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "        print (targets)\n",
    "\n",
    "        #imgs = Variable(imgs.to(device))\n",
    "        #targets = Variable(targets.to(device), requires_grad=False)\n",
    "\n",
    "        loss, outputs = model(imgs, targets)\n",
    "        loss.backward()\n",
    "\n",
    "        if batches_done % opt.gradient_accumulations:\n",
    "            # Accumulates gradient before each step\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "        # ----------------\n",
    "        #   Log progress\n",
    "        # ----------------\n",
    "\n",
    "        log_str = \"\\n---- [Epoch %d/%d, Batch %d/%d] ----\\n\" % (epoch, opt.epochs, batch_i, len(dataloader))\n",
    "\n",
    "        metric_table = [[\"Metrics\", *[f\"YOLO Layer {i}\" for i in range(len(model.yolo_layers))]]]\n",
    "\n",
    "        # Log metrics at each YOLO layer\n",
    "        for i, metric in enumerate(metrics):\n",
    "            formats = {m: \"%.6f\" for m in metrics}\n",
    "            formats[\"grid_size\"] = \"%2d\"\n",
    "            formats[\"cls_acc\"] = \"%.2f%%\"\n",
    "            row_metrics = [formats[metric] % yolo.metrics.get(metric, 0) for yolo in model.yolo_layers]\n",
    "            metric_table += [[metric, *row_metrics]]\n",
    "\n",
    "            # Tensorboard logging\n",
    "            tensorboard_log = []\n",
    "            for j, yolo in enumerate(model.yolo_layers):\n",
    "                for name, metric in yolo.metrics.items():\n",
    "                    if name != \"grid_size\":\n",
    "                        tensorboard_log += [(f\"{name}_{j+1}\", metric)]\n",
    "            tensorboard_log += [(\"loss\", loss.item())]\n",
    "            logger.list_of_scalars_summary(tensorboard_log, batches_done)\n",
    "\n",
    "        log_str += AsciiTable(metric_table).table\n",
    "        log_str += f\"\\nTotal loss {loss.item()}\"\n",
    "\n",
    "        # Determine approximate time left for epoch\n",
    "        epoch_batches_left = len(dataloader) - (batch_i + 1)\n",
    "        time_left = datetime.timedelta(seconds=epoch_batches_left * (time.time() - start_time) / (batch_i + 1))\n",
    "        log_str += f\"\\n---- ETA {time_left}\"\n",
    "\n",
    "        print(log_str)\n",
    "\n",
    "        model.seen += imgs.size(0)\n",
    "\n",
    "    if epoch % opt.evaluation_interval == 0:\n",
    "        print(\"\\n---- Evaluating Model ----\")\n",
    "        # Evaluate the model on the validation set\n",
    "        precision, recall, AP, f1, ap_class = evaluate(\n",
    "            model,\n",
    "            path=valid_path,\n",
    "            iou_thres=0.5,\n",
    "            conf_thres=0.5,\n",
    "            nms_thres=0.5,\n",
    "            img_size=opt.img_size,\n",
    "            batch_size=8,\n",
    "        )\n",
    "        evaluation_metrics = [\n",
    "            (\"val_precision\", precision.mean()),\n",
    "            (\"val_recall\", recall.mean()),\n",
    "            (\"val_mAP\", AP.mean()),\n",
    "            (\"val_f1\", f1.mean()),\n",
    "        ]\n",
    "        logger.list_of_scalars_summary(evaluation_metrics, epoch)\n",
    "\n",
    "        # Print class APs and mAP\n",
    "        ap_table = [[\"Index\", \"Class name\", \"AP\"]]\n",
    "        for i, c in enumerate(ap_class):\n",
    "            ap_table += [[c, class_names[c], \"%.5f\" % AP[i]]]\n",
    "        print(AsciiTable(ap_table).table)\n",
    "        print(f\"---- mAP {AP.mean()}\")\n",
    "\n",
    "    #if epoch % opt.checkpoint_interval == 0:\n",
    "    #    torch.save(model.state_dict(), f\"checkpoints/yolov3_ckpt_%d.pth\" % epoch)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del dataset\n",
    "del data_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pick one image from the test set\n",
    "img, img2 = dataset_test[2]\n",
    "# put the model in evaluation mode\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    prediction = model([img.to(device)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Image.fromarray(img.mul(255).permute(1, 2, 0).byte().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(img2.keys())\n",
    "print(img2['boxes'])\n",
    "mshape = img2['masks'].shape\n",
    "arr = np.zeros((mshape[1],mshape[2]),dtype=np.uint8)\n",
    "for i in range(mshape[0]):\n",
    "    print (\"load\")\n",
    "    arr += img2['masks'][i].mul(255).byte().numpy()\n",
    "print (arr, np.unique(arr), arr.shape, type(arr))\n",
    "image = Image.fromarray(arr)\n",
    "image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_rect(x, y, width, height):\n",
    "    rect = [(x, y), (x+width, y), (x+width, y+height), (x, y+height), (x, y)]\n",
    "    #rect = [(0, 0), (width, height)]\n",
    "    \n",
    "    return tuple(rect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import ImageDraw\n",
    "pshape = prediction[0]['masks'].shape\n",
    "boxes = prediction[0]['boxes'].cpu().numpy()\n",
    "print(prediction[0]['masks'].shape)\n",
    "\n",
    "arr = np.zeros((pshape[2],pshape[3]),dtype=np.uint8)\n",
    "for i in range(pshape[0]):\n",
    "    arr += prediction[0]['masks'][i,0].mul(255).byte().cpu().numpy()\n",
    "image = Image.fromarray(arr)\n",
    "\n",
    "\n",
    "overlay = Image.new(\"L\", (pshape[3], pshape[2]), 0)\n",
    "draw = ImageDraw.Draw(overlay)\n",
    "rects = [get_rect(*p) for p in boxes]\n",
    "\n",
    "for rect in rects:\n",
    "    draw.line([tuple(p) for p in rect], width=3, fill=500)\n",
    "\n",
    "\n",
    "print(image.size)\n",
    "print(overlay.size)\n",
    "\n",
    "img = Image.merge(\"RGB\", (image,overlay,image))\n",
    "(image,overlay)\n",
    "img"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
