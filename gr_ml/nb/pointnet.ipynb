{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model From https://github.com/fxia22/pointnet.pytorch/blob/master/pointnet/model.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.parallel\n",
    "import torch.utils.data\n",
    "from torch.autograd import Variable\n",
    "import numpy as np\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stn torch.Size([32, 3, 3])\n",
      "loss tensor(2.0153, grad_fn=<MeanBackward0>)\n",
      "stn64d torch.Size([32, 64, 64])\n",
      "loss tensor(125.9736, grad_fn=<MeanBackward0>)\n",
      "global feat torch.Size([32, 1024])\n",
      "point feat torch.Size([32, 1088, 2500])\n",
      "class torch.Size([32, 5])\n",
      "seg torch.Size([32, 2500, 3])\n"
     ]
    }
   ],
   "source": [
    "class STN3d(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(STN3d, self).__init__()\n",
    "        self.conv1 = torch.nn.Conv1d(3, 64, 1)\n",
    "        self.conv2 = torch.nn.Conv1d(64, 128, 1)\n",
    "        self.conv3 = torch.nn.Conv1d(128, 1024, 1)\n",
    "        self.fc1 = nn.Linear(1024, 512)\n",
    "        self.fc2 = nn.Linear(512, 256)\n",
    "        self.fc3 = nn.Linear(256, 9)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "        self.bn1 = nn.BatchNorm1d(64)\n",
    "        self.bn2 = nn.BatchNorm1d(128)\n",
    "        self.bn3 = nn.BatchNorm1d(1024)\n",
    "        self.bn4 = nn.BatchNorm1d(512)\n",
    "        self.bn5 = nn.BatchNorm1d(256)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        batchsize = x.size()[0]\n",
    "        x = F.relu(self.bn1(self.conv1(x)))\n",
    "        x = F.relu(self.bn2(self.conv2(x)))\n",
    "        x = F.relu(self.bn3(self.conv3(x)))\n",
    "        x = torch.max(x, 2, keepdim=True)[0]\n",
    "        x = x.view(-1, 1024)\n",
    "\n",
    "        x = F.relu(self.bn4(self.fc1(x)))\n",
    "        x = F.relu(self.bn5(self.fc2(x)))\n",
    "        x = self.fc3(x)\n",
    "\n",
    "        iden = Variable(torch.from_numpy(np.array([1,0,0,0,1,0,0,0,1]).astype(np.float32))).view(1,9).repeat(batchsize,1)\n",
    "        if x.is_cuda:\n",
    "            iden = iden.cuda()\n",
    "        x = x + iden\n",
    "        x = x.view(-1, 3, 3)\n",
    "        return x\n",
    "\n",
    "\n",
    "class STNkd(nn.Module):\n",
    "    def __init__(self, k=64):\n",
    "        super(STNkd, self).__init__()\n",
    "        self.conv1 = torch.nn.Conv1d(k, 64, 1)\n",
    "        self.conv2 = torch.nn.Conv1d(64, 128, 1)\n",
    "        self.conv3 = torch.nn.Conv1d(128, 1024, 1)\n",
    "        self.fc1 = nn.Linear(1024, 512)\n",
    "        self.fc2 = nn.Linear(512, 256)\n",
    "        self.fc3 = nn.Linear(256, k*k)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "        self.bn1 = nn.BatchNorm1d(64)\n",
    "        self.bn2 = nn.BatchNorm1d(128)\n",
    "        self.bn3 = nn.BatchNorm1d(1024)\n",
    "        self.bn4 = nn.BatchNorm1d(512)\n",
    "        self.bn5 = nn.BatchNorm1d(256)\n",
    "\n",
    "        self.k = k\n",
    "\n",
    "    def forward(self, x):\n",
    "        batchsize = x.size()[0]\n",
    "        x = F.relu(self.bn1(self.conv1(x)))\n",
    "        x = F.relu(self.bn2(self.conv2(x)))\n",
    "        x = F.relu(self.bn3(self.conv3(x)))\n",
    "        x = torch.max(x, 2, keepdim=True)[0]\n",
    "        x = x.view(-1, 1024)\n",
    "\n",
    "        x = F.relu(self.bn4(self.fc1(x)))\n",
    "        x = F.relu(self.bn5(self.fc2(x)))\n",
    "        x = self.fc3(x)\n",
    "\n",
    "        iden = Variable(torch.from_numpy(np.eye(self.k).flatten().astype(np.float32))).view(1,self.k*self.k).repeat(batchsize,1)\n",
    "        if x.is_cuda:\n",
    "            iden = iden.cuda()\n",
    "        x = x + iden\n",
    "        x = x.view(-1, self.k, self.k)\n",
    "        return x\n",
    "\n",
    "class PointNetfeat(nn.Module):\n",
    "    def __init__(self, global_feat = True, feature_transform = False):\n",
    "        super(PointNetfeat, self).__init__()\n",
    "        self.stn = STN3d()\n",
    "        self.conv1 = torch.nn.Conv1d(3, 64, 1)\n",
    "        self.conv2 = torch.nn.Conv1d(64, 128, 1)\n",
    "        self.conv3 = torch.nn.Conv1d(128, 1024, 1)\n",
    "        self.bn1 = nn.BatchNorm1d(64)\n",
    "        self.bn2 = nn.BatchNorm1d(128)\n",
    "        self.bn3 = nn.BatchNorm1d(1024)\n",
    "        self.global_feat = global_feat\n",
    "        self.feature_transform = feature_transform\n",
    "        if self.feature_transform:\n",
    "            self.fstn = STNkd(k=64)\n",
    "\n",
    "    def forward(self, x):\n",
    "        n_pts = x.size()[2]\n",
    "        trans = self.stn(x)\n",
    "        x = x.transpose(2, 1)\n",
    "        x = torch.bmm(x, trans)\n",
    "        x = x.transpose(2, 1)\n",
    "        x = F.relu(self.bn1(self.conv1(x)))\n",
    "\n",
    "        if self.feature_transform:\n",
    "            trans_feat = self.fstn(x)\n",
    "            x = x.transpose(2,1)\n",
    "            x = torch.bmm(x, trans_feat)\n",
    "            x = x.transpose(2,1)\n",
    "        else:\n",
    "            trans_feat = None\n",
    "\n",
    "        pointfeat = x\n",
    "        x = F.relu(self.bn2(self.conv2(x)))\n",
    "        x = self.bn3(self.conv3(x))\n",
    "        x = torch.max(x, 2, keepdim=True)[0]\n",
    "        x = x.view(-1, 1024)\n",
    "        if self.global_feat:\n",
    "            return x, trans, trans_feat\n",
    "        else:\n",
    "            x = x.view(-1, 1024, 1).repeat(1, 1, n_pts)\n",
    "            return torch.cat([x, pointfeat], 1), trans, trans_feat\n",
    "\n",
    "class PointNetCls(nn.Module):\n",
    "    def __init__(self, k=2, feature_transform=False):\n",
    "        super(PointNetCls, self).__init__()\n",
    "        self.feature_transform = feature_transform\n",
    "        self.feat = PointNetfeat(global_feat=True, feature_transform=feature_transform)\n",
    "        self.fc1 = nn.Linear(1024, 512)\n",
    "        self.fc2 = nn.Linear(512, 256)\n",
    "        self.fc3 = nn.Linear(256, k)\n",
    "        self.dropout = nn.Dropout(p=0.3)\n",
    "        self.bn1 = nn.BatchNorm1d(512)\n",
    "        self.bn2 = nn.BatchNorm1d(256)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x, trans, trans_feat = self.feat(x)\n",
    "        x = F.relu(self.bn1(self.fc1(x)))\n",
    "        x = F.relu(self.bn2(self.dropout(self.fc2(x))))\n",
    "        x = self.fc3(x)\n",
    "        return x, trans, trans_feat\n",
    "        #return F.log_softmax(x, dim=1), trans, trans_feat\n",
    "\n",
    "\n",
    "class PointNetDenseCls(nn.Module):\n",
    "    def __init__(self, k = 2, feature_transform=False):\n",
    "        super(PointNetDenseCls, self).__init__()\n",
    "        self.k = k\n",
    "        self.feature_transform=feature_transform\n",
    "        self.feat = PointNetfeat(global_feat=False, feature_transform=feature_transform)\n",
    "        self.conv1 = torch.nn.Conv1d(1088, 512, 1)\n",
    "        self.conv2 = torch.nn.Conv1d(512, 256, 1)\n",
    "        self.conv3 = torch.nn.Conv1d(256, 128, 1)\n",
    "        self.conv4 = torch.nn.Conv1d(128, self.k, 1)\n",
    "        self.bn1 = nn.BatchNorm1d(512)\n",
    "        self.bn2 = nn.BatchNorm1d(256)\n",
    "        self.bn3 = nn.BatchNorm1d(128)\n",
    "\n",
    "    def forward(self, x):\n",
    "        batchsize = x.size()[0]\n",
    "        n_pts = x.size()[2]\n",
    "        x, trans, trans_feat = self.feat(x)\n",
    "        x = F.relu(self.bn1(self.conv1(x)))\n",
    "        x = F.relu(self.bn2(self.conv2(x)))\n",
    "        x = F.relu(self.bn3(self.conv3(x)))\n",
    "        x = self.conv4(x)\n",
    "        x = x.transpose(2,1).contiguous()\n",
    "        #x = F.log_softmax(x.view(-1,self.k), dim=-1)\n",
    "        x = x.view(batchsize, n_pts, self.k)\n",
    "        return x, trans, trans_feat\n",
    "\n",
    "def feature_transform_regularizer(trans):\n",
    "    d = trans.size()[1]\n",
    "    batchsize = trans.size()[0]\n",
    "    I = torch.eye(d)[None, :, :]\n",
    "    if trans.is_cuda:\n",
    "        I = I.cuda()\n",
    "    loss = torch.mean(torch.norm(torch.bmm(trans, trans.transpose(2,1)) - I, dim=(1,2)))\n",
    "    return loss\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    sim_data = Variable(torch.rand(32,3,2500))\n",
    "    trans = STN3d()\n",
    "    out = trans(sim_data)\n",
    "    print('stn', out.size())\n",
    "    print('loss', feature_transform_regularizer(out))\n",
    "\n",
    "    sim_data_64d = Variable(torch.rand(32, 64, 2500))\n",
    "    trans = STNkd(k=64)\n",
    "    out = trans(sim_data_64d)\n",
    "    print('stn64d', out.size())\n",
    "    print('loss', feature_transform_regularizer(out))\n",
    "\n",
    "    pointfeat = PointNetfeat(global_feat=True)\n",
    "    out, _, _ = pointfeat(sim_data)\n",
    "    print('global feat', out.size())\n",
    "\n",
    "    pointfeat = PointNetfeat(global_feat=False)\n",
    "    out, _, _ = pointfeat(sim_data)\n",
    "    print('point feat', out.size())\n",
    "\n",
    "    cls = PointNetCls(k = 5)\n",
    "    out, _, _ = cls(sim_data)\n",
    "    print('class', out.size())\n",
    "\n",
    "    seg = PointNetDenseCls(k = 3)\n",
    "    out, _, _ = seg(sim_data)\n",
    "    print('seg', out.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 3, 500])\n",
      "3\n",
      "torch.Size([32, 500, 1])\n",
      "torch.Size([32, 3, 3])\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "#batch, dim, npoints\n",
    "sim_data = Variable(torch.rand(32,3,500))\n",
    "print(sim_data.shape)\n",
    "classifier = PointNetDenseCls(k=1, feature_transform=None)\n",
    "output = classifier(sim_data)\n",
    "print(len(output))\n",
    "print (output[0].shape)\n",
    "print (output[1].shape)\n",
    "print (output[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "stn3d=STN3d()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8, 8, 3)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from mpl_toolkits.mplot3d import Axes3D  # noqa: F401 unused import\n",
    "\n",
    "\n",
    "# prepare some coordinates\n",
    "x, y, z = np.indices((8, 8, 3))\n",
    "\n",
    "# draw cuboids in the top left and bottom right corners, and a link between them\n",
    "cube1 = (x < 1) & (y < 1) & (z < 1)\n",
    "cube2 = (x >= 2) & (y >= 2) & (z >= 2)\n",
    "link = abs(x - y) + abs(y - z) + abs(z - x) <= 2\n",
    "\n",
    "# combine the objects into a single boolean array\n",
    "voxels = cube1 | cube2 | link\n",
    "\n",
    "# set the colors of each object\n",
    "colors = np.empty(voxels.shape, dtype=object)\n",
    "colors[link] = 'red'\n",
    "colors[cube1] = 'blue'\n",
    "colors[cube2] = 'green'\n",
    "print(colors.shape)\n",
    "\n",
    "# and plot everything\n",
    "fig = plt.figure()\n",
    "ax = fig.gca(projection='3d')\n",
    "ax.voxels(voxels, facecolors=colors, edgecolor='k')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Source https://discuss.pytorch.org/t/which-part-of-pytorch-tensor-represents-channels/21778\n",
    "The first number represents the Batchsize (N) and for tensors holding data of a dimension of 1 or above the next dimension is usually referred to as channel-dimension. The following dimensions are commonly height, width and depth.\n",
    "So for 2d data (images) you have a 4d tensor of NxCxHxW which you feed into a 2d conv layer.\n",
    "\n",
    "Note that channels only exist for convolutional layers. Linear layers for example need a shape of N x #num_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyIterableDataset(torch.utils.data.IterableDataset):\n",
    "...     def __init__(self, start, end):\n",
    "...         super(MyIterableDataset).__init__()\n",
    "...         assert end > start, \"this example code only works with end >= start\"\n",
    "...         self.start = start\n",
    "...         self.end = end\n",
    "...\n",
    "...     def __iter__(self):\n",
    "...         worker_info = torch.utils.data.get_worker_info()\n",
    "...         if worker_info is None:  # single-process data loading, return the full iterator\n",
    "...             iter_start = self.start\n",
    "...             iter_end = self.end\n",
    "...         else:  # in a worker process\n",
    "...             # split workload\n",
    "...             per_worker = int(math.ceil((self.end - self.start) / float(worker_info.num_workers)))\n",
    "...             worker_id = worker_info.id\n",
    "...             iter_start = self.start + worker_id * per_worker\n",
    "...             iter_end = min(iter_start + per_worker, self.end)\n",
    "...         return iter(range(iter_start, iter_end))\n",
    "        def __next__():\n",
    "            yield (np.arange(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "#OR train_ds = TensorDataset(x_train, y_train)\n",
    "def get_data(train_ds, valid_ds, bs):\n",
    "    return (\n",
    "        DataLoader(train_ds, batch_size=bs),\n",
    "        DataLoader(valid_ds, batch_size=bs * 2),\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "itdataset = MyIterableDataset(start=1, end=10)\n",
    "traingen, valgen = get_data(itdataset,itdataset,bs=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1, 2, 3, 4, 5, 6, 7, 8, 9])\n"
     ]
    }
   ],
   "source": [
    "for i in traingen:\n",
    "    print (i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyModel(torch.nn.Module):\n",
    "    def __init__(self, n_points=50):\n",
    "        super(MyModel, self).__init__()\n",
    "        self.n_points = n_points\n",
    "        self.conv1 = torch.nn.Conv1d(3, 32, 1)\n",
    "        self.conv2 = torch.nn.Conv1d(32, 16, 1)\n",
    "        self.conv3 = torch.nn.Conv1d(16, 1, 1)\n",
    "        self.lin1 = torch.nn.Linear(n_points, 1)\n",
    "        self.bn1 = nn.BatchNorm1d(32)\n",
    "        self.bn2 = nn.BatchNorm1d(16)\n",
    "        self.criterion = nn.MSELoss()\n",
    "        self.optimizer = torch.optim.SGD(self.parameters(), lr=0.01, momentum=0.2)\n",
    "        self.batch_loss = list()\n",
    "\n",
    "    def forward(self, x):\n",
    "        batchsize = x.size()[0]\n",
    "        x = F.relu(self.bn1(self.conv1(x)))\n",
    "        x = F.relu(self.bn2(self.conv2(x)))\n",
    "        x = F.relu(self.conv3(x))\n",
    "        #x = torch.max(x, 2, keepdim=True)\n",
    "        x = x.view(batchsize,-1)\n",
    "        #x = x.flatten()\n",
    "        x = F.relu(self.lin1(x))\n",
    "        #print(x.item(), \"TEST\")\n",
    "        return x\n",
    "\n",
    "    def trainbatch(self,x,targets,batchid):\n",
    "        self.optimizer.zero_grad()\n",
    "        output = self.__call__(x)\n",
    "        \n",
    "        loss = self.criterion(output,targets)\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "        self.epoch_loss += loss.item()\n",
    "        self.batch_loss.append(loss.item())\n",
    "\n",
    "        #if batchid%20 == 0:\n",
    "        #    print(\"Batch {} - batch loss: {}\".format(batchid, loss.item()))\n",
    "\n",
    "    def train(self,nepochs=5, nbatches=100, batchsize=10, valsamples=10, device=\"cuda\"):\n",
    "        epoch_loss = list()\n",
    "        #self.batch_loss = list()\n",
    "\n",
    "        for epoch in range(nepochs):\n",
    "\n",
    "            self.epoch_loss = 0.0\n",
    "\n",
    "            #batches number\n",
    "            for b in range(nbatches):\n",
    "                x = Variable(torch.rand(batchsize,3, self.n_points)).to(device)\n",
    "                targets = torch.rand((batchsize,1)).to(device)\n",
    "                self.trainbatch(x,targets,b)\n",
    "\n",
    "            self.epoch_loss /= (nbatches*batchsize)\n",
    "            \n",
    "            print(\"Epoch {} - epoch loss: {}\".format(epoch, self.epoch_loss))\n",
    "            epoch_loss.append(self.epoch_loss)\n",
    "\n",
    "            #validation\n",
    "            self.eval()\n",
    "            with torch.no_grad():\n",
    "                valx = Variable(torch.rand(valsamples,3, self.n_points)).to(device)\n",
    "                valy = torch.rand((valsamples,1)).to(device)\n",
    "                valoutput = self.criterion(self.__call__(valx), valy)\n",
    "                print(\"EPOCH{} VAL LOSS{} \".format(epoch, valoutput.item()))\n",
    "            \n",
    "        return epoch_loss\n",
    "\n",
    "    def predict(self,x):\n",
    "        with torch.no_grad():\n",
    "            return self.__call__(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Note from pytorch tutorial for validation\n",
    "\n",
    "def loss_batch(model, loss_func, xb, yb, opt=None):\n",
    "    loss = loss_func(model(xb), yb)\n",
    "\n",
    "    if opt is not None:\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        opt.zero_grad()\n",
    "\n",
    "    return loss.item(), len(xb)\n",
    "\n",
    "...\n",
    "\n",
    "model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "\n",
    "    losses, nums = zip(\n",
    "                *[loss_batch(model, loss_func, xb, yb) for xb, yb in valid_dl]\n",
    "    )\n",
    "    val_loss = np.sum(np.multiply(losses, nums)) / np.sum(nums"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "mymodel = MyModel(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 - epoch loss: 0.0076060577109456065\n",
      "EPOCH0 VAL LOSS0.1668531596660614 \n",
      "Epoch 1 - epoch loss: 0.005089842979796231\n",
      "EPOCH1 VAL LOSS0.1082000881433487 \n",
      "Epoch 2 - epoch loss: 0.004716181536205113\n",
      "EPOCH2 VAL LOSS0.09696033596992493 \n",
      "Epoch 3 - epoch loss: 0.004608051991090179\n",
      "EPOCH3 VAL LOSS0.10555189847946167 \n",
      "Epoch 4 - epoch loss: 0.004447995126247406\n",
      "EPOCH4 VAL LOSS0.08338163048028946 \n",
      "Epoch 5 - epoch loss: 0.004393560932949185\n",
      "EPOCH5 VAL LOSS0.06092967465519905 \n",
      "Epoch 6 - epoch loss: 0.004525739448145032\n",
      "EPOCH6 VAL LOSS0.1044011265039444 \n",
      "Epoch 7 - epoch loss: 0.004643021007068455\n",
      "EPOCH7 VAL LOSS0.054503995925188065 \n",
      "Epoch 8 - epoch loss: 0.004625990092754364\n",
      "EPOCH8 VAL LOSS0.10959811508655548 \n",
      "Epoch 9 - epoch loss: 0.004408947480842471\n",
      "EPOCH9 VAL LOSS0.1157497987151146 \n",
      "Epoch 10 - epoch loss: 0.004353848691098392\n",
      "EPOCH10 VAL LOSS0.135605126619339 \n",
      "Epoch 11 - epoch loss: 0.004354234165512025\n",
      "EPOCH11 VAL LOSS0.10761024802923203 \n",
      "Epoch 12 - epoch loss: 0.0044186288816854356\n",
      "EPOCH12 VAL LOSS0.12723293900489807 \n",
      "Epoch 13 - epoch loss: 0.0042432384239509706\n",
      "EPOCH13 VAL LOSS0.08327586948871613 \n",
      "Epoch 14 - epoch loss: 0.004295752844773233\n",
      "EPOCH14 VAL LOSS0.1417645812034607 \n",
      "Epoch 15 - epoch loss: 0.004299804796464741\n",
      "EPOCH15 VAL LOSS0.10576023906469345 \n",
      "Epoch 16 - epoch loss: 0.004381252881139517\n",
      "EPOCH16 VAL LOSS0.09935502707958221 \n",
      "Epoch 17 - epoch loss: 0.004346499009989202\n",
      "EPOCH17 VAL LOSS0.0703970193862915 \n",
      "Epoch 18 - epoch loss: 0.004580338113009929\n",
      "EPOCH18 VAL LOSS0.05623779818415642 \n",
      "Epoch 19 - epoch loss: 0.004403245723806321\n",
      "EPOCH19 VAL LOSS0.09789499640464783 \n",
      "Epoch 20 - epoch loss: 0.004341940390877426\n",
      "EPOCH20 VAL LOSS0.0593094602227211 \n",
      "Epoch 21 - epoch loss: 0.004148554587736726\n",
      "EPOCH21 VAL LOSS0.10210078954696655 \n",
      "Epoch 22 - epoch loss: 0.004426409197039902\n",
      "EPOCH22 VAL LOSS0.08770538866519928 \n",
      "Epoch 23 - epoch loss: 0.004194577494636178\n",
      "EPOCH23 VAL LOSS0.04357490688562393 \n",
      "Epoch 24 - epoch loss: 0.004351304653100669\n",
      "EPOCH24 VAL LOSS0.05855732038617134 \n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cpu\")\n",
    "if torch.cuda.is_available():\n",
    "    #sim_data = Variable(torch.rand(2,3,1))\n",
    "    device = torch.device(\"cuda\")          # a CUDA device object\n",
    "    #y = torch.ones_like(sim_data, device=device)  # directly create a tensor on GPU\n",
    "    #x = sim_data.to(device)                       # or just use strings ``.to(\"cuda\")``\n",
    "    #print(x)\n",
    "    #print(x.to(\"cpu\", torch.double))       # ``.to`` can also change dtype together!\n",
    "    mymodel.cuda()\n",
    "#print(device)\n",
    "losses = mymodel.train(25, nbatches=40, batchsize=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(torch.randint(0,10,(10,1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nparams = list(mymodel.parameters())\\nprint(len(params))\\nprint(params[0].size())\\nacc = np.sum([i.sum().cpu().detach().numpy() for i in params])\\nprint([i.sum().cpu().detach().numpy() for i in params])\\nprint(acc)\\n'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\"\"\"\n",
    "params = list(mymodel.parameters())\n",
    "print(len(params))\n",
    "print(params[0].size())\n",
    "acc = np.sum([i.sum().cpu().detach().numpy() for i in params])\n",
    "print([i.sum().cpu().detach().numpy() for i in params])\n",
    "print(acc)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.23645393550395966,\n",
       " 0.262798935174942,\n",
       " 0.1733713150024414,\n",
       " 0.38149285316467285,\n",
       " 0.2534061670303345,\n",
       " 0.29921016097068787,\n",
       " 0.2787947654724121,\n",
       " 0.32195448875427246,\n",
       " 0.17292137444019318,\n",
       " 0.18531453609466553,\n",
       " 0.2167586088180542,\n",
       " 0.10247047245502472,\n",
       " 0.09285164624452591,\n",
       " 0.1614197939634323,\n",
       " 0.06339983642101288,\n",
       " 0.16612139344215393,\n",
       " 0.16139128804206848,\n",
       " 0.11294452846050262,\n",
       " 0.09682410955429077,\n",
       " 0.13233092427253723,\n",
       " 0.07097013294696808,\n",
       " 0.13906870782375336,\n",
       " 0.17193594574928284,\n",
       " 0.1296897679567337,\n",
       " 0.15391112864017487,\n",
       " 0.11696300655603409,\n",
       " 0.13369616866111755,\n",
       " 0.09540408104658127,\n",
       " 0.1383795142173767,\n",
       " 0.05473131686449051,\n",
       " 0.08303119242191315,\n",
       " 0.1392793357372284,\n",
       " 0.09095144271850586,\n",
       " 0.08990202844142914,\n",
       " 0.12417755275964737,\n",
       " 0.08269289880990982,\n",
       " 0.10190556198358536,\n",
       " 0.09461545944213867,\n",
       " 0.09860479086637497,\n",
       " 0.10270500183105469,\n",
       " 0.15267665684223175,\n",
       " 0.1175013929605484,\n",
       " 0.07270673662424088,\n",
       " 0.10706044733524323,\n",
       " 0.0635448545217514,\n",
       " 0.09222778677940369,\n",
       " 0.11400783061981201,\n",
       " 0.11700187623500824,\n",
       " 0.11249430477619171,\n",
       " 0.1294996440410614,\n",
       " 0.09874477982521057,\n",
       " 0.09673575311899185,\n",
       " 0.10603201389312744,\n",
       " 0.11041407287120819,\n",
       " 0.12028838694095612,\n",
       " 0.10010876506567001,\n",
       " 0.05778217315673828,\n",
       " 0.15384992957115173,\n",
       " 0.09414069354534149,\n",
       " 0.08573728799819946,\n",
       " 0.07768742740154266,\n",
       " 0.08524380624294281,\n",
       " 0.07919757813215256,\n",
       " 0.11057324707508087,\n",
       " 0.09010538458824158,\n",
       " 0.09984467923641205,\n",
       " 0.1226392388343811,\n",
       " 0.1007680669426918,\n",
       " 0.10146685689687729,\n",
       " 0.04230790212750435,\n",
       " 0.11049841344356537,\n",
       " 0.12254419177770615,\n",
       " 0.09068242460489273,\n",
       " 0.11621338129043579,\n",
       " 0.11166547238826752,\n",
       " 0.12284110486507416,\n",
       " 0.09606610238552094,\n",
       " 0.06933486461639404,\n",
       " 0.12250838428735733,\n",
       " 0.09713046997785568,\n",
       " 0.08121111989021301,\n",
       " 0.05503823608160019,\n",
       " 0.131063312292099,\n",
       " 0.12617912888526917,\n",
       " 0.10140017420053482,\n",
       " 0.09514908492565155,\n",
       " 0.09251730889081955,\n",
       " 0.06739085912704468,\n",
       " 0.09107386320829391,\n",
       " 0.0971064567565918,\n",
       " 0.07385358214378357,\n",
       " 0.07256819307804108,\n",
       " 0.10021042823791504,\n",
       " 0.08280528336763382,\n",
       " 0.13149698078632355,\n",
       " 0.1310814768075943,\n",
       " 0.07181438058614731,\n",
       " 0.09468435496091843,\n",
       " 0.07622181624174118,\n",
       " 0.09949807077646255,\n",
       " 0.0849514976143837,\n",
       " 0.10283324867486954,\n",
       " 0.12941892445087433,\n",
       " 0.08231367915868759,\n",
       " 0.05784875154495239,\n",
       " 0.10230004787445068,\n",
       " 0.1007264032959938,\n",
       " 0.06218020245432854,\n",
       " 0.1057620495557785,\n",
       " 0.08447609841823578,\n",
       " 0.08307569473981857,\n",
       " 0.13721568882465363,\n",
       " 0.10901687294244766,\n",
       " 0.07238481938838959,\n",
       " 0.0927031859755516,\n",
       " 0.11728562414646149,\n",
       " 0.0724315196275711,\n",
       " 0.12159746885299683,\n",
       " 0.1026029959321022,\n",
       " 0.07945634424686432,\n",
       " 0.1153990849852562,\n",
       " 0.12057014554738998,\n",
       " 0.10604381561279297,\n",
       " 0.08092979341745377,\n",
       " 0.06257811188697815,\n",
       " 0.123353973031044,\n",
       " 0.08636150509119034,\n",
       " 0.09652072191238403,\n",
       " 0.08230812847614288,\n",
       " 0.10652247816324234,\n",
       " 0.08327226340770721,\n",
       " 0.06673125177621841,\n",
       " 0.07100777328014374,\n",
       " 0.07618041336536407,\n",
       " 0.05792845040559769,\n",
       " 0.0788261741399765,\n",
       " 0.0921952947974205,\n",
       " 0.10055729001760483,\n",
       " 0.030079573392868042,\n",
       " 0.11770131438970566,\n",
       " 0.09974730759859085,\n",
       " 0.1106957197189331,\n",
       " 0.08528237044811249,\n",
       " 0.11559881269931793,\n",
       " 0.11760994046926498,\n",
       " 0.12013483047485352,\n",
       " 0.08504974842071533,\n",
       " 0.10153134167194366,\n",
       " 0.07579417526721954,\n",
       " 0.09150893986225128,\n",
       " 0.07830209285020828,\n",
       " 0.08842597156763077,\n",
       " 0.08074326813220978,\n",
       " 0.10429374873638153,\n",
       " 0.1012759581208229,\n",
       " 0.0865020602941513,\n",
       " 0.08524399995803833,\n",
       " 0.13565757870674133,\n",
       " 0.10380278527736664,\n",
       " 0.0641733855009079,\n",
       " 0.09297444671392441,\n",
       " 0.09686944633722305,\n",
       " 0.08350684493780136,\n",
       " 0.03730040416121483,\n",
       " 0.10898909717798233,\n",
       " 0.07099912315607071,\n",
       " 0.1055544838309288,\n",
       " 0.09747208654880524,\n",
       " 0.10785011947154999,\n",
       " 0.09831614047288895,\n",
       " 0.13710972666740417,\n",
       " 0.08503366261720657,\n",
       " 0.08565110713243484,\n",
       " 0.12899985909461975,\n",
       " 0.07561560720205307,\n",
       " 0.12316076457500458,\n",
       " 0.06525010615587234,\n",
       " 0.13445408642292023,\n",
       " 0.09140068292617798,\n",
       " 0.08986784517765045,\n",
       " 0.06720852106809616,\n",
       " 0.12681306898593903,\n",
       " 0.04996141046285629,\n",
       " 0.08266128599643707,\n",
       " 0.09089776873588562,\n",
       " 0.07234543561935425,\n",
       " 0.08057151734828949,\n",
       " 0.1075642853975296,\n",
       " 0.062209904193878174,\n",
       " 0.0653180181980133,\n",
       " 0.11369240283966064,\n",
       " 0.06620696932077408,\n",
       " 0.0799444168806076,\n",
       " 0.08404706418514252,\n",
       " 0.06103084608912468,\n",
       " 0.06258289515972137,\n",
       " 0.09061689674854279,\n",
       " 0.10299321264028549,\n",
       " 0.10865303128957748,\n",
       " 0.0667015090584755,\n",
       " 0.0987267941236496,\n",
       " 0.10102702677249908,\n",
       " 0.13218669593334198,\n",
       " 0.0753004401922226,\n",
       " 0.07636334747076035,\n",
       " 0.07330819219350815,\n",
       " 0.10416505485773087,\n",
       " 0.1063745990395546,\n",
       " 0.09310339391231537,\n",
       " 0.0824180468916893,\n",
       " 0.08558786660432816,\n",
       " 0.05822468549013138,\n",
       " 0.09735586494207382,\n",
       " 0.10465272516012192,\n",
       " 0.08032949268817902,\n",
       " 0.05742542818188667,\n",
       " 0.1160602942109108,\n",
       " 0.07993477582931519,\n",
       " 0.08795883506536484,\n",
       " 0.0974600538611412,\n",
       " 0.092584028840065,\n",
       " 0.08274589478969574,\n",
       " 0.0803869366645813,\n",
       " 0.08986973762512207,\n",
       " 0.09610368311405182,\n",
       " 0.054436881095170975,\n",
       " 0.09160561114549637,\n",
       " 0.07695995271205902,\n",
       " 0.06930060684680939,\n",
       " 0.08189256489276886,\n",
       " 0.0777042880654335,\n",
       " 0.0915171205997467,\n",
       " 0.0867118388414383,\n",
       " 0.09392119199037552,\n",
       " 0.11335651576519012,\n",
       " 0.12409286201000214,\n",
       " 0.0693780779838562,\n",
       " 0.07935380935668945,\n",
       " 0.06955086439847946,\n",
       " 0.08541266620159149,\n",
       " 0.10288812965154648,\n",
       " 0.06744091957807541,\n",
       " 0.10491029173135757,\n",
       " 0.09225797653198242,\n",
       " 0.10372132062911987,\n",
       " 0.05862916633486748,\n",
       " 0.06270930916070938,\n",
       " 0.10694601386785507,\n",
       " 0.12399552017450333,\n",
       " 0.10579679161310196,\n",
       " 0.05180967599153519,\n",
       " 0.11589141935110092,\n",
       " 0.03660313040018082,\n",
       " 0.08899883925914764,\n",
       " 0.11526815593242645,\n",
       " 0.08918343484401703,\n",
       " 0.11560966074466705,\n",
       " 0.10241930186748505,\n",
       " 0.06839843094348907,\n",
       " 0.07513909041881561,\n",
       " 0.06624911725521088,\n",
       " 0.12546274065971375,\n",
       " 0.08756919205188751,\n",
       " 0.07997576892375946,\n",
       " 0.07025367766618729,\n",
       " 0.09411349892616272,\n",
       " 0.10811867564916611,\n",
       " 0.14191456139087677,\n",
       " 0.0813356339931488,\n",
       " 0.09228043258190155,\n",
       " 0.10580800473690033,\n",
       " 0.09021144360303879,\n",
       " 0.08928044140338898,\n",
       " 0.07649294286966324,\n",
       " 0.07472117990255356,\n",
       " 0.062255699187517166,\n",
       " 0.1114795058965683,\n",
       " 0.08818694949150085,\n",
       " 0.09973212331533432,\n",
       " 0.08653338998556137,\n",
       " 0.12569460272789001,\n",
       " 0.08400408923625946,\n",
       " 0.044718168675899506,\n",
       " 0.11203594505786896,\n",
       " 0.09276904910802841,\n",
       " 0.11613611876964569,\n",
       " 0.09158389270305634,\n",
       " 0.04549941048026085,\n",
       " 0.07588793337345123,\n",
       " 0.0814749002456665,\n",
       " 0.10304802656173706,\n",
       " 0.08209466934204102,\n",
       " 0.08812806755304337,\n",
       " 0.16060546040534973,\n",
       " 0.11581792682409286,\n",
       " 0.10946901887655258,\n",
       " 0.09364991635084152,\n",
       " 0.07719816267490387,\n",
       " 0.10792354494333267,\n",
       " 0.11289364099502563,\n",
       " 0.09834370762109756,\n",
       " 0.08905381709337234,\n",
       " 0.09648214280605316,\n",
       " 0.0794493705034256,\n",
       " 0.07405837625265121,\n",
       " 0.07873456925153732,\n",
       " 0.08176794648170471,\n",
       " 0.08729921281337738,\n",
       " 0.0866740271449089,\n",
       " 0.08249406516551971,\n",
       " 0.11373119056224823,\n",
       " 0.07022420316934586,\n",
       " 0.05850907415151596,\n",
       " 0.0869404748082161,\n",
       " 0.09918127954006195,\n",
       " 0.09894925355911255,\n",
       " 0.10663435608148575,\n",
       " 0.09593698382377625,\n",
       " 0.11891386657953262,\n",
       " 0.09040634334087372,\n",
       " 0.08573325723409653,\n",
       " 0.0732574313879013,\n",
       " 0.10727137327194214,\n",
       " 0.09411515295505524,\n",
       " 0.08911187946796417,\n",
       " 0.08820744603872299,\n",
       " 0.10786104202270508,\n",
       " 0.08238492161035538,\n",
       " 0.11198951303958893,\n",
       " 0.13181188702583313,\n",
       " 0.08340480178594589,\n",
       " 0.1134939044713974,\n",
       " 0.1083674281835556,\n",
       " 0.07976836711168289,\n",
       " 0.06707432866096497,\n",
       " 0.1129673570394516,\n",
       " 0.0664367824792862,\n",
       " 0.05528568476438522,\n",
       " 0.1124923974275589,\n",
       " 0.12147442996501923,\n",
       " 0.05467643588781357,\n",
       " 0.0862802267074585,\n",
       " 0.1358039826154709,\n",
       " 0.06695108860731125,\n",
       " 0.06953934580087662,\n",
       " 0.12441553920507431,\n",
       " 0.11484245955944061,\n",
       " 0.09002923965454102,\n",
       " 0.09401682764291763,\n",
       " 0.07364583015441895,\n",
       " 0.06985628604888916,\n",
       " 0.09355258196592331,\n",
       " 0.0767568051815033,\n",
       " 0.10200681537389755,\n",
       " 0.11048898845911026,\n",
       " 0.07340437173843384,\n",
       " 0.09364483505487442,\n",
       " 0.06985129415988922,\n",
       " 0.10597032308578491,\n",
       " 0.10254941135644913,\n",
       " 0.0847250446677208,\n",
       " 0.10152647644281387,\n",
       " 0.07232709228992462,\n",
       " 0.1235044002532959,\n",
       " 0.0939338356256485,\n",
       " 0.10821831226348877,\n",
       " 0.0649029091000557,\n",
       " 0.1137879341840744,\n",
       " 0.058612335473299026,\n",
       " 0.07168827950954437,\n",
       " 0.10163359344005585,\n",
       " 0.12648949027061462,\n",
       " 0.06502100080251694,\n",
       " 0.0967065840959549,\n",
       " 0.0943501740694046,\n",
       " 0.07215927541255951,\n",
       " 0.09670545160770416,\n",
       " 0.13285157084465027,\n",
       " 0.0738663449883461,\n",
       " 0.04663218557834625,\n",
       " 0.06464548408985138,\n",
       " 0.053239695727825165,\n",
       " 0.10349719226360321,\n",
       " 0.08968133479356766,\n",
       " 0.12658630311489105,\n",
       " 0.08837050944566727,\n",
       " 0.13103151321411133,\n",
       " 0.060613226145505905,\n",
       " 0.08615998923778534,\n",
       " 0.06762731820344925,\n",
       " 0.0886387825012207,\n",
       " 0.08618511259555817,\n",
       " 0.07157280296087265,\n",
       " 0.11037720739841461,\n",
       " 0.08789686113595963,\n",
       " 0.08035540580749512,\n",
       " 0.08062423765659332,\n",
       " 0.0899125337600708,\n",
       " 0.07468143850564957,\n",
       " 0.08581874519586563,\n",
       " 0.07056038081645966,\n",
       " 0.0971432775259018,\n",
       " 0.08353366702795029,\n",
       " 0.08545790612697601,\n",
       " 0.07546725124120712,\n",
       " 0.08154194056987762,\n",
       " 0.060714446008205414,\n",
       " 0.05841941758990288,\n",
       " 0.087654247879982,\n",
       " 0.10625334084033966,\n",
       " 0.10168462991714478,\n",
       " 0.07768220454454422,\n",
       " 0.07951363176107407,\n",
       " 0.0660240650177002,\n",
       " 0.08864610642194748,\n",
       " 0.12317665666341782,\n",
       " 0.08356010913848877,\n",
       " 0.07343269884586334,\n",
       " 0.11127506196498871,\n",
       " 0.09247533231973648,\n",
       " 0.07518665492534637,\n",
       " 0.082993283867836,\n",
       " 0.08944688737392426,\n",
       " 0.12796059250831604,\n",
       " 0.10172148793935776,\n",
       " 0.06637777388095856,\n",
       " 0.0842854380607605,\n",
       " 0.08770354837179184,\n",
       " 0.0943508967757225,\n",
       " 0.06654392182826996,\n",
       " 0.06769280135631561,\n",
       " 0.07632545381784439,\n",
       " 0.10290338844060898,\n",
       " 0.0915433019399643,\n",
       " 0.09954263269901276,\n",
       " 0.09372097998857498,\n",
       " 0.08828582614660263,\n",
       " 0.08896945416927338,\n",
       " 0.07175753265619278,\n",
       " 0.12155072391033173,\n",
       " 0.09450545161962509,\n",
       " 0.11469855159521103,\n",
       " 0.11698374897241592,\n",
       " 0.07035080343484879,\n",
       " 0.04682037979364395,\n",
       " 0.0774022713303566,\n",
       " 0.1039983257651329,\n",
       " 0.07860548794269562,\n",
       " 0.09400047361850739,\n",
       " 0.07477996498346329,\n",
       " 0.07412226498126984,\n",
       " 0.0885395035147667,\n",
       " 0.09404683858156204,\n",
       " 0.07835576683282852,\n",
       " 0.07749076187610626,\n",
       " 0.10813860595226288,\n",
       " 0.07177649438381195,\n",
       " 0.10938087850809097,\n",
       " 0.08941139280796051,\n",
       " 0.0806981772184372,\n",
       " 0.1415071040391922,\n",
       " 0.08114106953144073,\n",
       " 0.10822920501232147,\n",
       " 0.06215177848935127,\n",
       " 0.046132639050483704,\n",
       " 0.06405241787433624,\n",
       " 0.07317821681499481,\n",
       " 0.08644653856754303,\n",
       " 0.1064913421869278,\n",
       " 0.07806748151779175,\n",
       " 0.0874798446893692,\n",
       " 0.11068268120288849,\n",
       " 0.11097152531147003,\n",
       " 0.09778501093387604,\n",
       " 0.11295479536056519,\n",
       " 0.07343935966491699,\n",
       " 0.08112183213233948,\n",
       " 0.048678912222385406,\n",
       " 0.10621380805969238,\n",
       " 0.06255562603473663,\n",
       " 0.04937462881207466,\n",
       " 0.08127196878194809,\n",
       " 0.11358492076396942,\n",
       " 0.09433624893426895,\n",
       " 0.07910843193531036,\n",
       " 0.12109465897083282,\n",
       " 0.06996925175189972,\n",
       " 0.05333384871482849,\n",
       " 0.16388168931007385,\n",
       " 0.08414499461650848,\n",
       " 0.08158773928880692,\n",
       " 0.061915427446365356,\n",
       " 0.12419172376394272,\n",
       " 0.09428785741329193,\n",
       " 0.06835723668336868,\n",
       " 0.09874054789543152,\n",
       " 0.10067819058895111,\n",
       " 0.07287364453077316,\n",
       " 0.08772575855255127,\n",
       " 0.08435629308223724,\n",
       " 0.06416592746973038,\n",
       " 0.06057488173246384,\n",
       " 0.08819245547056198,\n",
       " 0.09860287606716156,\n",
       " 0.07500427216291428,\n",
       " 0.09486188739538193,\n",
       " 0.12565019726753235,\n",
       " 0.07917564362287521,\n",
       " 0.07683989405632019,\n",
       " 0.07431306689977646,\n",
       " 0.10308142751455307,\n",
       " 0.07950378954410553,\n",
       " 0.10755838453769684,\n",
       " 0.06024868041276932,\n",
       " 0.09283239394426346,\n",
       " 0.07343269884586334,\n",
       " 0.09328310936689377,\n",
       " 0.13747742772102356,\n",
       " 0.08317571133375168,\n",
       " 0.08211331814527512,\n",
       " 0.08565159887075424,\n",
       " 0.06795861572027206,\n",
       " 0.10198569297790527,\n",
       " 0.06520305573940277,\n",
       " 0.06394682079553604,\n",
       " 0.08070578426122665,\n",
       " 0.08499961346387863,\n",
       " 0.1125115305185318,\n",
       " 0.055465810000896454,\n",
       " 0.09628594666719437,\n",
       " 0.11203192174434662,\n",
       " 0.07659240067005157,\n",
       " 0.10023313760757446,\n",
       " 0.06649746000766754,\n",
       " 0.09963182359933853,\n",
       " 0.0989614725112915,\n",
       " 0.08906127512454987,\n",
       " 0.11369495093822479,\n",
       " 0.07546871900558472,\n",
       " 0.09531198441982269,\n",
       " 0.08722110837697983,\n",
       " 0.09197647869586945,\n",
       " 0.07077603787183762,\n",
       " 0.039620012044906616,\n",
       " 0.09507498145103455,\n",
       " 0.08265037834644318,\n",
       " 0.07588586211204529,\n",
       " 0.06665847450494766,\n",
       " 0.07791518419981003,\n",
       " 0.08799006789922714,\n",
       " 0.07701227813959122,\n",
       " 0.09428824484348297,\n",
       " 0.06553991883993149,\n",
       " 0.10426149517297745,\n",
       " 0.10846860706806183,\n",
       " 0.11295925080776215,\n",
       " 0.08617117255926132,\n",
       " 0.054726529866456985,\n",
       " 0.07772840559482574,\n",
       " 0.09546663612127304,\n",
       " 0.09296700358390808,\n",
       " 0.10656680166721344,\n",
       " 0.0763215646147728,\n",
       " 0.08794653415679932,\n",
       " 0.13068172335624695,\n",
       " 0.08857307583093643,\n",
       " 0.08964147418737411,\n",
       " 0.08033791184425354,\n",
       " 0.06157653406262398,\n",
       " 0.07640188187360764,\n",
       " 0.08668611943721771,\n",
       " 0.07851091772317886,\n",
       " 0.07848693430423737,\n",
       " 0.11715419590473175,\n",
       " 0.09545393288135529,\n",
       " 0.05199592188000679,\n",
       " 0.09864234179258347,\n",
       " 0.07394792884588242,\n",
       " 0.08538191020488739,\n",
       " 0.07491922378540039,\n",
       " 0.05544667690992355,\n",
       " 0.10124953091144562,\n",
       " 0.09153970330953598,\n",
       " 0.07526389509439468,\n",
       " 0.05797380208969116,\n",
       " 0.09534697234630585,\n",
       " 0.11004088073968887,\n",
       " 0.047458309680223465,\n",
       " 0.10065724700689316,\n",
       " 0.07534785568714142,\n",
       " 0.09363311529159546,\n",
       " 0.08812715113162994,\n",
       " 0.10511086136102676,\n",
       " 0.0762099102139473,\n",
       " 0.09618760645389557,\n",
       " 0.08673347532749176,\n",
       " 0.09436234086751938,\n",
       " 0.06708361208438873,\n",
       " 0.10759542137384415,\n",
       " 0.07903997600078583,\n",
       " 0.07482212036848068,\n",
       " 0.06448932737112045,\n",
       " 0.07392539829015732,\n",
       " 0.10782221704721451,\n",
       " 0.050026338547468185,\n",
       " 0.0768166184425354,\n",
       " 0.11978385597467422,\n",
       " 0.09664459526538849,\n",
       " 0.0701063945889473,\n",
       " 0.07308752834796906,\n",
       " 0.10073523223400116,\n",
       " 0.07023007422685623,\n",
       " 0.08972571790218353,\n",
       " 0.07314936816692352,\n",
       " 0.07759968191385269,\n",
       " 0.08159156143665314,\n",
       " 0.0995442122220993,\n",
       " 0.07447564601898193,\n",
       " 0.08969621360301971,\n",
       " 0.08586881309747696,\n",
       " 0.09263680875301361,\n",
       " 0.09688061475753784,\n",
       " 0.08818183094263077,\n",
       " 0.07722143828868866,\n",
       " 0.08023016154766083,\n",
       " 0.09411889314651489,\n",
       " 0.11321365833282471,\n",
       " 0.11060541868209839,\n",
       " 0.08836881816387177,\n",
       " 0.10946468263864517,\n",
       " 0.06823143362998962,\n",
       " 0.0797298327088356,\n",
       " 0.06664132326841354,\n",
       " 0.06668639928102493,\n",
       " 0.12587356567382812,\n",
       " 0.046562351286411285,\n",
       " 0.13206477463245392,\n",
       " 0.08587997406721115,\n",
       " 0.10512735694646835,\n",
       " 0.061983585357666016,\n",
       " 0.09412669390439987,\n",
       " 0.09574297815561295,\n",
       " 0.08656557649374008,\n",
       " 0.07573983073234558,\n",
       " 0.07561860978603363,\n",
       " 0.10774388164281845,\n",
       " 0.09500954300165176,\n",
       " 0.08602504432201385,\n",
       " 0.08169494569301605,\n",
       " 0.08663914352655411,\n",
       " 0.11797885596752167,\n",
       " 0.10153894126415253,\n",
       " 0.09019879996776581,\n",
       " 0.07511938363313675,\n",
       " 0.07030988484621048,\n",
       " 0.10997514426708221,\n",
       " 0.09399793297052383,\n",
       " 0.08549097180366516,\n",
       " 0.08003060519695282,\n",
       " 0.09618110954761505,\n",
       " 0.0643659457564354,\n",
       " 0.1239454373717308,\n",
       " 0.07870994508266449,\n",
       " 0.08484587073326111,\n",
       " 0.1006375327706337,\n",
       " 0.0729525089263916,\n",
       " 0.07414255291223526,\n",
       " 0.09243125468492508,\n",
       " 0.05632667988538742,\n",
       " 0.11169453710317612,\n",
       " 0.07741345465183258,\n",
       " 0.06468047946691513,\n",
       " 0.08997848629951477,\n",
       " 0.06976546347141266,\n",
       " 0.0818258598446846,\n",
       " 0.08570311963558197,\n",
       " 0.07665922492742538,\n",
       " 0.13258470594882965,\n",
       " 0.10473126173019409,\n",
       " 0.055880106985569,\n",
       " 0.10534027963876724,\n",
       " 0.12583205103874207,\n",
       " 0.086793914437294,\n",
       " 0.06953476369380951,\n",
       " 0.06048484519124031,\n",
       " 0.0815688967704773,\n",
       " 0.063919298350811,\n",
       " 0.09365537017583847,\n",
       " 0.09685616940259933,\n",
       " 0.08533589541912079,\n",
       " 0.09268922358751297,\n",
       " 0.0768386498093605,\n",
       " 0.0898766964673996,\n",
       " 0.1232018917798996,\n",
       " 0.08803845942020416,\n",
       " 0.07097458839416504,\n",
       " 0.06530384719371796,\n",
       " 0.08176638185977936,\n",
       " 0.07817230373620987,\n",
       " 0.08778996765613556,\n",
       " 0.06465641409158707,\n",
       " 0.09532284736633301,\n",
       " 0.11251884698867798,\n",
       " 0.0707392543554306,\n",
       " 0.05904395133256912,\n",
       " 0.040569715201854706,\n",
       " 0.0830652266740799,\n",
       " 0.11421060562133789,\n",
       " 0.1010408028960228,\n",
       " 0.10802759230136871,\n",
       " 0.11368973553180695,\n",
       " 0.08677344024181366,\n",
       " 0.11778118461370468,\n",
       " 0.07523694634437561,\n",
       " 0.10536445677280426,\n",
       " 0.08414461463689804,\n",
       " 0.08241312205791473,\n",
       " 0.08921957015991211,\n",
       " 0.07498560845851898,\n",
       " 0.07442177832126617,\n",
       " 0.06625381857156754,\n",
       " 0.06786791235208511,\n",
       " 0.08941005170345306,\n",
       " 0.09866969287395477,\n",
       " 0.09150545299053192,\n",
       " 0.052556879818439484,\n",
       " 0.07372202724218369,\n",
       " 0.0953245609998703,\n",
       " 0.09136705100536346,\n",
       " 0.08934639394283295,\n",
       " 0.12855128943920135,\n",
       " 0.09906215965747833,\n",
       " 0.07703109085559845,\n",
       " 0.08698102086782455,\n",
       " 0.0849040150642395,\n",
       " 0.10151022672653198,\n",
       " 0.07358424365520477,\n",
       " 0.093385249376297,\n",
       " 0.09619438648223877,\n",
       " 0.07810351252555847,\n",
       " 0.09472941607236862,\n",
       " 0.07191203534603119,\n",
       " 0.08845484256744385,\n",
       " 0.10786470025777817,\n",
       " 0.07994873821735382,\n",
       " 0.1071716919541359,\n",
       " 0.0721813291311264,\n",
       " 0.09627898037433624,\n",
       " 0.10587407648563385,\n",
       " 0.0910823866724968,\n",
       " 0.09931154549121857,\n",
       " 0.09296920895576477,\n",
       " 0.07589797675609589,\n",
       " 0.14254173636436462,\n",
       " 0.08072222769260406,\n",
       " 0.09531863033771515,\n",
       " 0.11147858202457428,\n",
       " 0.12976740300655365,\n",
       " 0.08404557406902313,\n",
       " 0.1013883724808693,\n",
       " 0.11263765394687653,\n",
       " 0.09297095239162445,\n",
       " 0.11889956891536713,\n",
       " 0.09213049709796906,\n",
       " 0.11637219041585922,\n",
       " 0.09945907443761826,\n",
       " 0.10798916965723038,\n",
       " 0.06411822885274887,\n",
       " 0.055374789983034134,\n",
       " 0.07941792905330658,\n",
       " 0.06442220509052277,\n",
       " 0.07867908477783203,\n",
       " 0.10303066670894623,\n",
       " 0.07559503614902496,\n",
       " 0.08356662839651108,\n",
       " 0.08157471567392349,\n",
       " 0.12404243648052216,\n",
       " 0.08240900188684464,\n",
       " 0.10156967490911484,\n",
       " 0.0492367297410965,\n",
       " 0.08257150650024414,\n",
       " 0.08587722480297089,\n",
       " 0.07884778827428818,\n",
       " 0.12787608802318573,\n",
       " 0.10057935863733292,\n",
       " 0.08804367482662201,\n",
       " 0.08847507834434509,\n",
       " 0.08214611560106277,\n",
       " 0.07126006484031677,\n",
       " 0.11370842158794403,\n",
       " 0.08808275312185287,\n",
       " 0.09838931262493134,\n",
       " 0.08632373064756393,\n",
       " 0.05866879224777222,\n",
       " 0.07182369381189346,\n",
       " 0.08179748058319092,\n",
       " 0.07331840693950653,\n",
       " 0.054818131029605865,\n",
       " 0.1171051636338234,\n",
       " 0.08938755840063095,\n",
       " 0.11114940792322159,\n",
       " 0.10067439079284668,\n",
       " 0.06123511120676994,\n",
       " 0.07077818363904953,\n",
       " 0.08596985042095184,\n",
       " 0.06378872692584991,\n",
       " 0.08743882924318314,\n",
       " 0.06468361616134644,\n",
       " 0.05718248337507248,\n",
       " 0.0677834302186966,\n",
       " 0.09686507284641266,\n",
       " 0.09471973776817322,\n",
       " 0.06411929428577423,\n",
       " 0.10893132537603378,\n",
       " 0.09532278776168823,\n",
       " 0.0855783075094223,\n",
       " 0.07438243925571442,\n",
       " 0.09168799221515656,\n",
       " 0.06509391963481903,\n",
       " 0.08582495152950287,\n",
       " 0.08299994468688965,\n",
       " 0.11608114093542099,\n",
       " 0.08627273887395859,\n",
       " 0.07648037374019623,\n",
       " 0.11639614403247833,\n",
       " 0.10382964462041855,\n",
       " 0.08140181005001068,\n",
       " 0.09592876583337784,\n",
       " 0.09099279344081879,\n",
       " 0.0871252790093422,\n",
       " 0.1126088872551918,\n",
       " 0.08042416721582413,\n",
       " 0.10277114808559418,\n",
       " 0.07925873249769211,\n",
       " 0.06684411317110062,\n",
       " 0.0746077373623848,\n",
       " 0.08663837611675262,\n",
       " 0.13071677088737488,\n",
       " 0.07809744030237198,\n",
       " 0.09086644649505615,\n",
       " 0.09007935225963593,\n",
       " 0.10685193538665771,\n",
       " 0.11557439714670181,\n",
       " 0.10636351257562637,\n",
       " 0.10146836936473846,\n",
       " 0.06977006793022156,\n",
       " 0.12457399070262909,\n",
       " 0.0757044330239296,\n",
       " 0.09558983147144318,\n",
       " 0.05709023028612137,\n",
       " 0.09260262548923492,\n",
       " 0.08426398038864136,\n",
       " 0.07885801792144775,\n",
       " 0.07176169008016586,\n",
       " 0.085811085999012,\n",
       " 0.08975522965192795,\n",
       " 0.07024799287319183,\n",
       " 0.08520002663135529,\n",
       " 0.06133671849966049,\n",
       " 0.05900995060801506,\n",
       " 0.07081079483032227,\n",
       " 0.064298614859581,\n",
       " 0.07864031195640564,\n",
       " 0.07340915501117706,\n",
       " 0.07977347075939178,\n",
       " 0.06927497684955597,\n",
       " 0.04582471027970314,\n",
       " 0.07186488807201385,\n",
       " 0.09323939681053162,\n",
       " 0.07789701968431473,\n",
       " 0.09522493183612823,\n",
       " 0.09139382839202881,\n",
       " 0.10332214832305908,\n",
       " 0.08575589209794998,\n",
       " 0.0773698091506958,\n",
       " 0.0783323422074318,\n",
       " 0.06577376276254654,\n",
       " 0.09030196070671082,\n",
       " 0.09072105586528778,\n",
       " 0.09370116144418716,\n",
       " 0.09624511748552322,\n",
       " 0.08705625683069229,\n",
       " 0.08800552785396576,\n",
       " 0.11939617246389389,\n",
       " 0.08095423877239227,\n",
       " 0.06032920628786087,\n",
       " 0.07970666885375977,\n",
       " 0.10095050185918808,\n",
       " 0.09535624086856842,\n",
       " 0.08980559557676315,\n",
       " 0.0601518452167511,\n",
       " 0.07337331771850586,\n",
       " 0.10965929180383682,\n",
       " 0.08986848592758179,\n",
       " 0.07773958146572113,\n",
       " 0.10267885774374008,\n",
       " 0.05556304380297661,\n",
       " 0.07535047829151154,\n",
       " 0.08233725279569626,\n",
       " 0.1062217727303505,\n",
       " 0.06554341316223145,\n",
       " 0.10418735444545746,\n",
       " 0.10895226895809174,\n",
       " 0.08409742265939713,\n",
       " 0.08957143127918243,\n",
       " 0.054628096520900726,\n",
       " 0.11073825508356094,\n",
       " 0.06798705458641052,\n",
       " 0.08827683329582214,\n",
       " 0.10038876533508301,\n",
       " 0.06821130961179733,\n",
       " 0.11014004796743393,\n",
       " 0.09713850915431976,\n",
       " 0.12459474802017212,\n",
       " 0.06812579929828644,\n",
       " 0.0976613461971283,\n",
       " 0.10139624774456024,\n",
       " 0.11468107998371124,\n",
       " 0.0650046318769455,\n",
       " 0.08905328810214996,\n",
       " 0.0983201339840889,\n",
       " 0.09145309031009674,\n",
       " 0.0784607008099556,\n",
       " 0.07598517835140228,\n",
       " 0.0994444340467453,\n",
       " 0.1059778556227684,\n",
       " 0.07546593248844147,\n",
       " 0.08392530679702759,\n",
       " 0.10038477182388306,\n",
       " 0.11027359962463379,\n",
       " 0.08169151842594147,\n",
       " 0.06871102005243301,\n",
       " 0.06697747856378555,\n",
       " 0.0798436775803566,\n",
       " 0.10121754556894302,\n",
       " 0.0706167221069336,\n",
       " 0.09427478164434433,\n",
       " 0.1038251742720604,\n",
       " 0.10626660287380219,\n",
       " 0.09184640645980835,\n",
       " 0.08029010146856308,\n",
       " 0.06558110564947128,\n",
       " 0.07337559759616852,\n",
       " 0.10096337646245956,\n",
       " 0.07266101986169815,\n",
       " 0.06777132302522659,\n",
       " 0.08225810527801514,\n",
       " 0.09373295307159424,\n",
       " 0.068478062748909,\n",
       " 0.0639767050743103,\n",
       " 0.06467529386281967,\n",
       " 0.05164162069559097,\n",
       " 0.09389631450176239,\n",
       " 0.08091367036104202,\n",
       " 0.08351071923971176,\n",
       " 0.07870848476886749,\n",
       " 0.10001306235790253,\n",
       " 0.09424315392971039,\n",
       " 0.0716099962592125,\n",
       " 0.0823993980884552,\n",
       " 0.10391227155923843,\n",
       " 0.07417716085910797,\n",
       " 0.09215908497571945,\n",
       " 0.09309576451778412,\n",
       " 0.036353107541799545,\n",
       " 0.07486280053853989,\n",
       " 0.08490104973316193,\n",
       " 0.07497638463973999,\n",
       " 0.07368496805429459,\n",
       " 0.06496673822402954,\n",
       " 0.10025638341903687,\n",
       " 0.10022439807653427,\n",
       " 0.09804646670818329,\n",
       " 0.03819850832223892,\n",
       " 0.06715787202119827,\n",
       " 0.09905670583248138,\n",
       " 0.08679164946079254,\n",
       " 0.07054731994867325,\n",
       " 0.10867650806903839,\n",
       " 0.09004690498113632,\n",
       " 0.11901149898767471,\n",
       " 0.10206659138202667,\n",
       " 0.09966395795345306,\n",
       " 0.06946812570095062,\n",
       " 0.12036418914794922,\n",
       " 0.07654554396867752,\n",
       " 0.12726172804832458,\n",
       " 0.0724172443151474,\n",
       " 0.09934337437152863,\n",
       " 0.0783548355102539,\n",
       " 0.0812070220708847,\n",
       " 0.08493974804878235,\n",
       " 0.08269385993480682,\n",
       " 0.08378969132900238,\n",
       " 0.09062813222408295,\n",
       " 0.1091732531785965,\n",
       " 0.07475952059030533,\n",
       " 0.1041492223739624,\n",
       " 0.06819383054971695,\n",
       " 0.10492030531167984]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mymodel.batch_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f883c1550b8>]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(np.arange(len(losses)), losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f883c0e1668>]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(np.arange(len(mymodel.batch_loss)), mymodel.batch_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MSELoss()"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mymodel.criterion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n",
      "MSE 1.960543155670166\n"
     ]
    }
   ],
   "source": [
    "print(device)\n",
    "x = Variable(torch.rand(20,3,mymodel.n_points)).to(device)\n",
    "targets = torch.rand((20,1)).to(device)\n",
    "y_pred = mymodel.predict(x)\n",
    "print(\"MSE {}\".format(((targets - y_pred)**2).sum()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[0.4378],\n",
       "         [0.4684],\n",
       "         [0.4649],\n",
       "         [0.4888],\n",
       "         [0.5215],\n",
       "         [0.5383],\n",
       "         [0.4906],\n",
       "         [0.4574],\n",
       "         [0.4553],\n",
       "         [0.5588],\n",
       "         [0.4992],\n",
       "         [0.4955],\n",
       "         [0.5098],\n",
       "         [0.4727],\n",
       "         [0.4468],\n",
       "         [0.4979],\n",
       "         [0.5078],\n",
       "         [0.5143],\n",
       "         [0.4596],\n",
       "         [0.5781]], device='cuda:0'), tensor([[0.1098],\n",
       "         [0.6935],\n",
       "         [0.1217],\n",
       "         [0.5154],\n",
       "         [0.6759],\n",
       "         [0.0973],\n",
       "         [0.4619],\n",
       "         [0.3361],\n",
       "         [0.7526],\n",
       "         [0.8738],\n",
       "         [0.1802],\n",
       "         [0.1853],\n",
       "         [0.2663],\n",
       "         [0.8963],\n",
       "         [0.3137],\n",
       "         [0.1434],\n",
       "         [0.5518],\n",
       "         [0.0289],\n",
       "         [0.0224],\n",
       "         [0.0746]], device='cuda:0'))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred, targets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "_, predicted = torch.max(outputs, 1)\n",
    "\n",
    "print('Predicted: ', ' '.join('%5s' % classes[predicted[j]]\n",
    "                              for j in range(4)))\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "def preprocess(x, y):\n",
    "    return x.view(-1, 1, 28, 28), y\n",
    "\n",
    "\n",
    "class WrappedDataLoader:\n",
    "    def __init__(self, dl, func):\n",
    "        self.dl = dl\n",
    "        self.func = func\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dl)\n",
    "\n",
    "    def __iter__(self):\n",
    "        batches = iter(self.dl)\n",
    "        for b in batches:\n",
    "            yield (self.func(*b))\n",
    "\n",
    "train_dl, valid_dl = get_data(train_ds, valid_ds, bs)\n",
    "train_dl = WrappedDataLoader(train_dl, preprocess)\n",
    "valid_dl = WrappedDataLoader(valid_dl, preprocess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
