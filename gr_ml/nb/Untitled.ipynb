{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# tutorial https://pytorch.org/tutorials/intermediate/torchvision_tutorial.html\n",
    "\n",
    "## Note to Jose\n",
    "Working version but not fully understood yet\n",
    "The next commented think can allow to remove stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fatal: destination path 'vision' already exists and is not an empty directory.\n",
      "error: pathspec 'v0.3.0' did not match any file(s) known to git.\n",
      "/home/jose/ros_ws/src/gr_perception/gr_ml/nb\n"
     ]
    }
   ],
   "source": [
    "# Download TorchVision repo to use some files from\n",
    "# references/detection\n",
    "!git clone https://github.com/pytorch/vision.git\n",
    "!git checkout v0.3.0\n",
    "!pwd\n",
    "!cp vision/references/detection/utils.py .\n",
    "!cp vision/references/detection/transforms.py .\n",
    "!cp vision/references/detection/coco_eval.py .\n",
    "!cp vision/references/detection/engine.py .\n",
    "!cp vision/references/detection/coco_utils.py ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from PIL import Image\n",
    "\n",
    "\n",
    "class PennFudanDataset(object):\n",
    "    def __init__(self, root, transforms):\n",
    "        self.root = root\n",
    "        self.transforms = transforms\n",
    "        # load all image files, sorting them to\n",
    "        # ensure that they are aligned\n",
    "        self.imgs = list(sorted(os.listdir(os.path.join(root, \"PNGImages\"))))\n",
    "        self.masks = list(sorted(os.listdir(os.path.join(root, \"PedMasks\"))))\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # load images ad masks\n",
    "        img_path = os.path.join(self.root, \"PNGImages\", self.imgs[idx])\n",
    "        mask_path = os.path.join(self.root, \"PedMasks\", self.masks[idx])\n",
    "        img = Image.open(img_path).convert(\"RGB\")\n",
    "        # note that we haven't converted the mask to RGB,\n",
    "        # because each color corresponds to a different instance\n",
    "        # with 0 being background\n",
    "        mask = Image.open(mask_path)\n",
    "        # convert the PIL Image into a numpy array\n",
    "        mask = np.array(mask)\n",
    "        # instances are encoded as different colors\n",
    "        obj_ids = np.unique(mask)\n",
    "        # first id is the background, so remove it\n",
    "        obj_ids = obj_ids[1:]\n",
    "\n",
    "        # split the color-encoded mask into a set\n",
    "        # of binary masks\n",
    "        masks = mask == obj_ids[:, None, None]\n",
    "\n",
    "        # get bounding box coordinates for each mask\n",
    "        num_objs = len(obj_ids)\n",
    "        boxes = []\n",
    "        for i in range(num_objs):\n",
    "            pos = np.where(masks[i])\n",
    "            xmin = np.min(pos[1])\n",
    "            xmax = np.max(pos[1])\n",
    "            ymin = np.min(pos[0])\n",
    "            ymax = np.max(pos[0])\n",
    "            boxes.append([xmin, ymin, xmax, ymax])\n",
    "\n",
    "        # convert everything into a torch.Tensor\n",
    "        boxes = torch.as_tensor(boxes, dtype=torch.float32)\n",
    "        # there is only one class\n",
    "        labels = torch.ones((num_objs,), dtype=torch.int64)\n",
    "        masks = torch.as_tensor(masks, dtype=torch.uint8)\n",
    "\n",
    "        image_id = torch.tensor([idx])\n",
    "        area = (boxes[:, 3] - boxes[:, 1]) * (boxes[:, 2] - boxes[:, 0])\n",
    "        # suppose all instances are not crowd\n",
    "        iscrowd = torch.zeros((num_objs,), dtype=torch.int64)\n",
    "\n",
    "        target = {}\n",
    "        target[\"boxes\"] = boxes\n",
    "        target[\"labels\"] = labels\n",
    "        target[\"masks\"] = masks\n",
    "        target[\"image_id\"] = image_id\n",
    "        target[\"area\"] = area\n",
    "        target[\"iscrowd\"] = iscrowd\n",
    "\n",
    "        if self.transforms is not None:\n",
    "            img, target = self.transforms(img, target)\n",
    "\n",
    "        return img, target\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.imgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
    "\n",
    "# load a model pre-trained pre-trained on COCO\n",
    "model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True)\n",
    "\n",
    "# replace the classifier with a new one, that has\n",
    "# num_classes which is user-defined\n",
    "num_classes = 2  # 1 class (person) + background\n",
    "# get number of input features for the classifier\n",
    "in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "# replace the pre-trained head with a new one\n",
    "model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.models.detection import FasterRCNN\n",
    "from torchvision.models.detection.rpn import AnchorGenerator\n",
    "\n",
    "# load a pre-trained model for classification and return\n",
    "# only the features\n",
    "backbone = torchvision.models.mobilenet_v2(pretrained=True).features\n",
    "# FasterRCNN needs to know the number of\n",
    "# output channels in a backbone. For mobilenet_v2, it's 1280\n",
    "# so we need to add it here\n",
    "backbone.out_channels = 1280\n",
    "\n",
    "# let's make the RPN generate 5 x 3 anchors per spatial\n",
    "# location, with 5 different sizes and 3 different aspect\n",
    "# ratios. We have a Tuple[Tuple[int]] because each feature\n",
    "# map could potentially have different sizes and\n",
    "# aspect ratios\n",
    "anchor_generator = AnchorGenerator(sizes=((32, 64, 128, 256, 512),),\n",
    "                                   aspect_ratios=((0.5, 1.0, 2.0),))\n",
    "\n",
    "# let's define what are the feature maps that we will\n",
    "# use to perform the region of interest cropping, as well as\n",
    "# the size of the crop after rescaling.\n",
    "# if your backbone returns a Tensor, featmap_names is expected to\n",
    "# be [0]. More generally, the backbone should return an\n",
    "# OrderedDict[Tensor], and in featmap_names you can choose which\n",
    "# feature maps to use.\n",
    "roi_pooler = torchvision.ops.MultiScaleRoIAlign(featmap_names=[0],\n",
    "                                                output_size=7,\n",
    "                                                sampling_ratio=2)\n",
    "\n",
    "# put the pieces together inside a FasterRCNN model\n",
    "model = FasterRCNN(backbone,\n",
    "                   num_classes=2,\n",
    "                   rpn_anchor_generator=anchor_generator,\n",
    "                   box_roi_pool=roi_pooler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
    "from torchvision.models.detection.mask_rcnn import MaskRCNNPredictor\n",
    "\n",
    "\n",
    "def get_model_instance_segmentation(num_classes):\n",
    "    # load an instance segmentation model pre-trained pre-trained on COCO\n",
    "    model = torchvision.models.detection.maskrcnn_resnet50_fpn(pretrained=True)\n",
    "\n",
    "    # get number of input features for the classifier\n",
    "    in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "    # replace the pre-trained head with a new one\n",
    "    model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n",
    "\n",
    "    # now get the number of input features for the mask classifier\n",
    "    in_features_mask = model.roi_heads.mask_predictor.conv5_mask.in_channels\n",
    "    hidden_layer = 256\n",
    "    # and replace the mask predictor with a new one\n",
    "    model.roi_heads.mask_predictor = MaskRCNNPredictor(in_features_mask,\n",
    "                                                       hidden_layer,\n",
    "                                                       num_classes)\n",
    "\n",
    "    return model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.transforms import functional as F\n",
    "import random\n",
    "\n",
    "class Compose(object):\n",
    "    def __init__(self, transforms):\n",
    "        self.transforms = transforms\n",
    "\n",
    "    def __call__(self, image, target):\n",
    "        for t in self.transforms:\n",
    "            image, target = t(image, target)\n",
    "        return image, target\n",
    "\n",
    "\n",
    "class RandomHorizontalFlip(object):\n",
    "    def __init__(self, prob):\n",
    "        self.prob = prob\n",
    "\n",
    "    def __call__(self, image, target):\n",
    "        if random.random() < self.prob:\n",
    "            height, width = image.shape[-2:]\n",
    "            image = image.flip(-1)\n",
    "            bbox = target[\"boxes\"]\n",
    "            bbox[:, [0, 2]] = width - bbox[:, [2, 0]]\n",
    "            target[\"boxes\"] = bbox\n",
    "            if \"masks\" in target:\n",
    "                target[\"masks\"] = target[\"masks\"].flip(-1)\n",
    "            if \"keypoints\" in target:\n",
    "                keypoints = target[\"keypoints\"]\n",
    "                keypoints = _flip_coco_person_keypoints(keypoints, width)\n",
    "                target[\"keypoints\"] = keypoints\n",
    "        return image, target\n",
    "\n",
    "\n",
    "class ToTensor(object):\n",
    "    def __call__(self, image, target):\n",
    "        image = F.to_tensor(image)\n",
    "        return image, target\n",
    "\n",
    "def get_transform(train):\n",
    "    transforms = []\n",
    "    print(\"C\")\n",
    "    transforms.append(ToTensor())\n",
    "    print(\"A\")\n",
    "    if train:\n",
    "        transforms.append(RandomHorizontalFlip(0.5))\n",
    "    return Compose(transforms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    return tuple(zip(*batch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "if False:\n",
    "    model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True)#.cuda()\n",
    "    dataset = PennFudanDataset('/media/datasets/PennFundan/PennFudanPed',transforms=get_transform(train=True))\n",
    "    data_loader = torch.utils.data.DataLoader(\n",
    "        dataset, batch_size=1, shuffle=True, num_workers=4,\n",
    "        collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For Testing Training\n",
    "if False:\n",
    "    images,targets = next(iter(data_loader))\n",
    "    images = list(image for image in images)\n",
    "    targets = [{k: v for k, v in t.items()} for t in targets]\n",
    "    output = model(images,targets)   # Returns losses and detections\n",
    "    # For inference\n",
    "    model.eval()\n",
    "    x = [torch.rand(3, 300, 400), torch.rand(3, 500, 400)]\n",
    "    predictions = model(x)           # Returns predictions\n",
    "    predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reduce_dict(input_dict, average=True):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        input_dict (dict): all the values will be reduced\n",
    "        average (bool): whether to do average or sum\n",
    "    Reduce the values in the dictionary from all processes so that all processes\n",
    "    have the averaged results. Returns a dict with the same fields as\n",
    "    input_dict, after reduction.\n",
    "    \"\"\"\n",
    "    world_size = get_world_size()\n",
    "    if world_size < 2:\n",
    "        return input_dict\n",
    "    with torch.no_grad():\n",
    "        names = []\n",
    "        values = []\n",
    "        # sort the keys so that they are consistent across processes\n",
    "        for k in sorted(input_dict.keys()):\n",
    "            names.append(k)\n",
    "            values.append(input_dict[k])\n",
    "        values = torch.stack(values, dim=0)\n",
    "        dist.all_reduce(values)\n",
    "        if average:\n",
    "            values /= world_size\n",
    "        reduced_dict = {k: v for k, v in zip(names, values)}\n",
    "    return reduced_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import sys\n",
    "import time\n",
    "import torch\n",
    "\n",
    "import torchvision.models.detection.mask_rcnn\n",
    "\n",
    "\n",
    "def train_one_epoch(model, optimizer, data_loader, device, epoch, print_freq):\n",
    "    model.train()\n",
    "    metric_logger = MetricLogger(delimiter=\"  \")\n",
    "    metric_logger.add_meter('lr', SmoothedValue(window_size=1, fmt='{value:.6f}'))\n",
    "    header = 'Epoch: [{}]'.format(epoch)\n",
    "\n",
    "    lr_scheduler = None\n",
    "    if epoch == 0:\n",
    "        warmup_factor = 1. / 1000\n",
    "        warmup_iters = min(1000, len(data_loader) - 1)\n",
    "\n",
    "        lr_scheduler = warmup_lr_scheduler(optimizer, warmup_iters, warmup_factor)\n",
    "\n",
    "    for images, targets in metric_logger.log_every(data_loader, print_freq, header):\n",
    "        images = list(image.to(device) for image in images)\n",
    "        targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "\n",
    "        loss_dict = model(images, targets)\n",
    "\n",
    "        losses = sum(loss for loss in loss_dict.values())\n",
    "\n",
    "        # reduce losses over all GPUs for logging purposes\n",
    "        loss_dict_reduced = reduce_dict(loss_dict)\n",
    "        losses_reduced = sum(loss for loss in loss_dict_reduced.values())\n",
    "\n",
    "        loss_value = losses_reduced.item()\n",
    "\n",
    "        if not math.isfinite(loss_value):\n",
    "            print(\"Loss is {}, stopping training\".format(loss_value))\n",
    "            print(loss_dict_reduced)\n",
    "            sys.exit(1)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        losses.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if lr_scheduler is not None:\n",
    "            lr_scheduler.step()\n",
    "\n",
    "        metric_logger.update(loss=losses_reduced, **loss_dict_reduced)\n",
    "        metric_logger.update(lr=optimizer.param_groups[0][\"lr\"])\n",
    "\n",
    "    return metric_logger\n",
    "\n",
    "\n",
    "def _get_iou_types(model):\n",
    "    model_without_ddp = model\n",
    "    if isinstance(model, torch.nn.parallel.DistributedDataParallel):\n",
    "        model_without_ddp = model.module\n",
    "    iou_types = [\"bbox\"]\n",
    "    if isinstance(model_without_ddp, torchvision.models.detection.MaskRCNN):\n",
    "        iou_types.append(\"segm\")\n",
    "    if isinstance(model_without_ddp, torchvision.models.detection.KeypointRCNN):\n",
    "        iou_types.append(\"keypoints\")\n",
    "    return iou_types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def all_gather(data):\n",
    "    \"\"\"\n",
    "    Run all_gather on arbitrary picklable data (not necessarily tensors)\n",
    "    Args:\n",
    "        data: any picklable object\n",
    "    Returns:\n",
    "        list[data]: list of data gathered from each rank\n",
    "    \"\"\"\n",
    "    world_size = get_world_size()\n",
    "    if world_size == 1:\n",
    "        return [data]\n",
    "\n",
    "    # serialized to a Tensor\n",
    "    buffer = pickle.dumps(data)\n",
    "    storage = torch.ByteStorage.from_buffer(buffer)\n",
    "    tensor = torch.ByteTensor(storage).to(\"cuda\")\n",
    "\n",
    "    # obtain Tensor size of each rank\n",
    "    local_size = torch.tensor([tensor.numel()], device=\"cuda\")\n",
    "    size_list = [torch.tensor([0], device=\"cuda\") for _ in range(world_size)]\n",
    "    dist.all_gather(size_list, local_size)\n",
    "    size_list = [int(size.item()) for size in size_list]\n",
    "    max_size = max(size_list)\n",
    "\n",
    "    # receiving Tensor from all ranks\n",
    "    # we pad the tensor because torch all_gather does not support\n",
    "    # gathering tensors of different shapes\n",
    "    tensor_list = []\n",
    "    for _ in size_list:\n",
    "        tensor_list.append(torch.empty((max_size,), dtype=torch.uint8, device=\"cuda\"))\n",
    "    if local_size != max_size:\n",
    "        padding = torch.empty(size=(max_size - local_size,), dtype=torch.uint8, device=\"cuda\")\n",
    "        tensor = torch.cat((tensor, padding), dim=0)\n",
    "    dist.all_gather(tensor_list, tensor)\n",
    "\n",
    "    data_list = []\n",
    "    for size, tensor in zip(size_list, tensor_list):\n",
    "        buffer = tensor.cpu().numpy().tobytes()[:size]\n",
    "        data_list.append(pickle.loads(buffer))\n",
    "\n",
    "    return data_list\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import tempfile\n",
    "\n",
    "import numpy as np\n",
    "import copy\n",
    "import time\n",
    "import torch\n",
    "import torch._six\n",
    "\n",
    "from pycocotools.cocoeval import COCOeval\n",
    "from pycocotools.coco import COCO\n",
    "import pycocotools.mask as mask_util\n",
    "\n",
    "from collections import defaultdict\n",
    "from collections import defaultdict, deque\n",
    "\n",
    "\n",
    "class CocoEvaluator(object):\n",
    "    def __init__(self, coco_gt, iou_types):\n",
    "        assert isinstance(iou_types, (list, tuple))\n",
    "        coco_gt = copy.deepcopy(coco_gt)\n",
    "        self.coco_gt = coco_gt\n",
    "\n",
    "        self.iou_types = iou_types\n",
    "        self.coco_eval = {}\n",
    "        for iou_type in iou_types:\n",
    "            self.coco_eval[iou_type] = COCOeval(coco_gt, iouType=iou_type)\n",
    "\n",
    "        self.img_ids = []\n",
    "        self.eval_imgs = {k: [] for k in iou_types}\n",
    "\n",
    "    def update(self, predictions):\n",
    "        img_ids = list(np.unique(list(predictions.keys())))\n",
    "        self.img_ids.extend(img_ids)\n",
    "\n",
    "        for iou_type in self.iou_types:\n",
    "            results = self.prepare(predictions, iou_type)\n",
    "            coco_dt = loadRes(self.coco_gt, results) if results else COCO()\n",
    "            coco_eval = self.coco_eval[iou_type]\n",
    "\n",
    "            coco_eval.cocoDt = coco_dt\n",
    "            coco_eval.params.imgIds = list(img_ids)\n",
    "            img_ids, eval_imgs = evaluate(coco_eval)\n",
    "\n",
    "            self.eval_imgs[iou_type].append(eval_imgs)\n",
    "\n",
    "    def synchronize_between_processes(self):\n",
    "        for iou_type in self.iou_types:\n",
    "            self.eval_imgs[iou_type] = np.concatenate(self.eval_imgs[iou_type], 2)\n",
    "            create_common_coco_eval(self.coco_eval[iou_type], self.img_ids, self.eval_imgs[iou_type])\n",
    "\n",
    "    def accumulate(self):\n",
    "        for coco_eval in self.coco_eval.values():\n",
    "            coco_eval.accumulate()\n",
    "\n",
    "    def summarize(self):\n",
    "        for iou_type, coco_eval in self.coco_eval.items():\n",
    "            print(\"IoU metric: {}\".format(iou_type))\n",
    "            coco_eval.summarize()\n",
    "\n",
    "    def prepare(self, predictions, iou_type):\n",
    "        if iou_type == \"bbox\":\n",
    "            return self.prepare_for_coco_detection(predictions)\n",
    "        elif iou_type == \"segm\":\n",
    "            return self.prepare_for_coco_segmentation(predictions)\n",
    "        elif iou_type == \"keypoints\":\n",
    "            return self.prepare_for_coco_keypoint(predictions)\n",
    "        else:\n",
    "            raise ValueError(\"Unknown iou type {}\".format(iou_type))\n",
    "\n",
    "    def prepare_for_coco_detection(self, predictions):\n",
    "        coco_results = []\n",
    "        for original_id, prediction in predictions.items():\n",
    "            if len(prediction) == 0:\n",
    "                continue\n",
    "\n",
    "            boxes = prediction[\"boxes\"]\n",
    "            boxes = convert_to_xywh(boxes).tolist()\n",
    "            scores = prediction[\"scores\"].tolist()\n",
    "            labels = prediction[\"labels\"].tolist()\n",
    "\n",
    "            coco_results.extend(\n",
    "                [\n",
    "                    {\n",
    "                        \"image_id\": original_id,\n",
    "                        \"category_id\": labels[k],\n",
    "                        \"bbox\": box,\n",
    "                        \"score\": scores[k],\n",
    "                    }\n",
    "                    for k, box in enumerate(boxes)\n",
    "                ]\n",
    "            )\n",
    "        return coco_results\n",
    "\n",
    "    def prepare_for_coco_segmentation(self, predictions):\n",
    "        coco_results = []\n",
    "        for original_id, prediction in predictions.items():\n",
    "            if len(prediction) == 0:\n",
    "                continue\n",
    "\n",
    "            scores = prediction[\"scores\"]\n",
    "            labels = prediction[\"labels\"]\n",
    "            masks = prediction[\"masks\"]\n",
    "\n",
    "            masks = masks > 0.5\n",
    "\n",
    "            scores = prediction[\"scores\"].tolist()\n",
    "            labels = prediction[\"labels\"].tolist()\n",
    "\n",
    "            rles = [\n",
    "                mask_util.encode(np.array(mask[0, :, :, np.newaxis], dtype=np.uint8, order=\"F\"))[0]\n",
    "                for mask in masks\n",
    "            ]\n",
    "            for rle in rles:\n",
    "                rle[\"counts\"] = rle[\"counts\"].decode(\"utf-8\")\n",
    "\n",
    "            coco_results.extend(\n",
    "                [\n",
    "                    {\n",
    "                        \"image_id\": original_id,\n",
    "                        \"category_id\": labels[k],\n",
    "                        \"segmentation\": rle,\n",
    "                        \"score\": scores[k],\n",
    "                    }\n",
    "                    for k, rle in enumerate(rles)\n",
    "                ]\n",
    "            )\n",
    "        return coco_results\n",
    "\n",
    "    def prepare_for_coco_keypoint(self, predictions):\n",
    "        coco_results = []\n",
    "        for original_id, prediction in predictions.items():\n",
    "            if len(prediction) == 0:\n",
    "                continue\n",
    "\n",
    "            boxes = prediction[\"boxes\"]\n",
    "            boxes = convert_to_xywh(boxes).tolist()\n",
    "            scores = prediction[\"scores\"].tolist()\n",
    "            labels = prediction[\"labels\"].tolist()\n",
    "            keypoints = prediction[\"keypoints\"]\n",
    "            keypoints = keypoints.flatten(start_dim=1).tolist()\n",
    "\n",
    "            coco_results.extend(\n",
    "                [\n",
    "                    {\n",
    "                        \"image_id\": original_id,\n",
    "                        \"category_id\": labels[k],\n",
    "                        'keypoints': keypoint,\n",
    "                        \"score\": scores[k],\n",
    "                    }\n",
    "                    for k, keypoint in enumerate(keypoints)\n",
    "                ]\n",
    "            )\n",
    "        return coco_results\n",
    "\n",
    "\n",
    "def convert_to_xywh(boxes):\n",
    "    xmin, ymin, xmax, ymax = boxes.unbind(1)\n",
    "    return torch.stack((xmin, ymin, xmax - xmin, ymax - ymin), dim=1)\n",
    "\n",
    "\n",
    "def merge(img_ids, eval_imgs):\n",
    "    all_img_ids = all_gather(img_ids)\n",
    "    all_eval_imgs = all_gather(eval_imgs)\n",
    "\n",
    "    merged_img_ids = []\n",
    "    for p in all_img_ids:\n",
    "        merged_img_ids.extend(p)\n",
    "\n",
    "    merged_eval_imgs = []\n",
    "    for p in all_eval_imgs:\n",
    "        merged_eval_imgs.append(p)\n",
    "\n",
    "    merged_img_ids = np.array(merged_img_ids)\n",
    "    merged_eval_imgs = np.concatenate(merged_eval_imgs, 2)\n",
    "\n",
    "    # keep only unique (and in sorted order) images\n",
    "    merged_img_ids, idx = np.unique(merged_img_ids, return_index=True)\n",
    "    merged_eval_imgs = merged_eval_imgs[..., idx]\n",
    "\n",
    "    return merged_img_ids, merged_eval_imgs\n",
    "\n",
    "\n",
    "def create_common_coco_eval(coco_eval, img_ids, eval_imgs):\n",
    "    img_ids, eval_imgs = merge(img_ids, eval_imgs)\n",
    "    img_ids = list(img_ids)\n",
    "    eval_imgs = list(eval_imgs.flatten())\n",
    "\n",
    "    coco_eval.evalImgs = eval_imgs\n",
    "    coco_eval.params.imgIds = img_ids\n",
    "    coco_eval._paramsEval = copy.deepcopy(coco_eval.params)\n",
    "\n",
    "\n",
    "#################################################################\n",
    "# From pycocotools, just removed the prints and fixed\n",
    "# a Python3 bug about unicode not defined\n",
    "#################################################################\n",
    "\n",
    "# Ideally, pycocotools wouldn't have hard-coded prints\n",
    "# so that we could avoid copy-pasting those two functions\n",
    "\n",
    "def createIndex(self):\n",
    "    # create index\n",
    "    # print('creating index...')\n",
    "    anns, cats, imgs = {}, {}, {}\n",
    "    imgToAnns, catToImgs = defaultdict(list), defaultdict(list)\n",
    "    if 'annotations' in self.dataset:\n",
    "        for ann in self.dataset['annotations']:\n",
    "            imgToAnns[ann['image_id']].append(ann)\n",
    "            anns[ann['id']] = ann\n",
    "\n",
    "    if 'images' in self.dataset:\n",
    "        for img in self.dataset['images']:\n",
    "            imgs[img['id']] = img\n",
    "\n",
    "    if 'categories' in self.dataset:\n",
    "        for cat in self.dataset['categories']:\n",
    "            cats[cat['id']] = cat\n",
    "\n",
    "    if 'annotations' in self.dataset and 'categories' in self.dataset:\n",
    "        for ann in self.dataset['annotations']:\n",
    "            catToImgs[ann['category_id']].append(ann['image_id'])\n",
    "\n",
    "    # print('index created!')\n",
    "\n",
    "    # create class members\n",
    "    self.anns = anns\n",
    "    self.imgToAnns = imgToAnns\n",
    "    self.catToImgs = catToImgs\n",
    "    self.imgs = imgs\n",
    "    self.cats = cats\n",
    "\n",
    "\n",
    "maskUtils = mask_util\n",
    "\n",
    "\n",
    "def loadRes(self, resFile):\n",
    "    \"\"\"\n",
    "    Load result file and return a result api object.\n",
    "    :param   resFile (str)     : file name of result file\n",
    "    :return: res (obj)         : result api object\n",
    "    \"\"\"\n",
    "    res = COCO()\n",
    "    res.dataset['images'] = [img for img in self.dataset['images']]\n",
    "\n",
    "    # print('Loading and preparing results...')\n",
    "    # tic = time.time()\n",
    "    if isinstance(resFile, torch._six.string_classes):\n",
    "        anns = json.load(open(resFile))\n",
    "    elif type(resFile) == np.ndarray:\n",
    "        anns = self.loadNumpyAnnotations(resFile)\n",
    "    else:\n",
    "        anns = resFile\n",
    "    assert type(anns) == list, 'results in not an array of objects'\n",
    "    annsImgIds = [ann['image_id'] for ann in anns]\n",
    "    assert set(annsImgIds) == (set(annsImgIds) & set(self.getImgIds())), \\\n",
    "        'Results do not correspond to current coco set'\n",
    "    if 'caption' in anns[0]:\n",
    "        imgIds = set([img['id'] for img in res.dataset['images']]) & set([ann['image_id'] for ann in anns])\n",
    "        res.dataset['images'] = [img for img in res.dataset['images'] if img['id'] in imgIds]\n",
    "        for id, ann in enumerate(anns):\n",
    "            ann['id'] = id + 1\n",
    "    elif 'bbox' in anns[0] and not anns[0]['bbox'] == []:\n",
    "        res.dataset['categories'] = copy.deepcopy(self.dataset['categories'])\n",
    "        for id, ann in enumerate(anns):\n",
    "            bb = ann['bbox']\n",
    "            x1, x2, y1, y2 = [bb[0], bb[0] + bb[2], bb[1], bb[1] + bb[3]]\n",
    "            if 'segmentation' not in ann:\n",
    "                ann['segmentation'] = [[x1, y1, x1, y2, x2, y2, x2, y1]]\n",
    "            ann['area'] = bb[2] * bb[3]\n",
    "            ann['id'] = id + 1\n",
    "            ann['iscrowd'] = 0\n",
    "    elif 'segmentation' in anns[0]:\n",
    "        res.dataset['categories'] = copy.deepcopy(self.dataset['categories'])\n",
    "        for id, ann in enumerate(anns):\n",
    "            # now only support compressed RLE format as segmentation results\n",
    "            ann['area'] = maskUtils.area(ann['segmentation'])\n",
    "            if 'bbox' not in ann:\n",
    "                ann['bbox'] = maskUtils.toBbox(ann['segmentation'])\n",
    "            ann['id'] = id + 1\n",
    "            ann['iscrowd'] = 0\n",
    "    elif 'keypoints' in anns[0]:\n",
    "        res.dataset['categories'] = copy.deepcopy(self.dataset['categories'])\n",
    "        for id, ann in enumerate(anns):\n",
    "            s = ann['keypoints']\n",
    "            x = s[0::3]\n",
    "            y = s[1::3]\n",
    "            x1, x2, y1, y2 = np.min(x), np.max(x), np.min(y), np.max(y)\n",
    "            ann['area'] = (x2 - x1) * (y2 - y1)\n",
    "            ann['id'] = id + 1\n",
    "            ann['bbox'] = [x1, y1, x2 - x1, y2 - y1]\n",
    "    # print('DONE (t={:0.2f}s)'.format(time.time()- tic))\n",
    "\n",
    "    res.dataset['annotations'] = anns\n",
    "    createIndex(res)\n",
    "    return res\n",
    "\n",
    "\n",
    "def evaluate(self):\n",
    "    '''\n",
    "    Run per image evaluation on given images and store results (a list of dict) in self.evalImgs\n",
    "    :return: None\n",
    "    '''\n",
    "    # tic = time.time()\n",
    "    # print('Running per image evaluation...')\n",
    "    p = self.params\n",
    "    # add backward compatibility if useSegm is specified in params\n",
    "    if p.useSegm is not None:\n",
    "        p.iouType = 'segm' if p.useSegm == 1 else 'bbox'\n",
    "        print('useSegm (deprecated) is not None. Running {} evaluation'.format(p.iouType))\n",
    "    # print('Evaluate annotation type *{}*'.format(p.iouType))\n",
    "    p.imgIds = list(np.unique(p.imgIds))\n",
    "    if p.useCats:\n",
    "        p.catIds = list(np.unique(p.catIds))\n",
    "    p.maxDets = sorted(p.maxDets)\n",
    "    self.params = p\n",
    "\n",
    "    self._prepare()\n",
    "    # loop through images, area range, max detection number\n",
    "    catIds = p.catIds if p.useCats else [-1]\n",
    "\n",
    "    if p.iouType == 'segm' or p.iouType == 'bbox':\n",
    "        computeIoU = self.computeIoU\n",
    "    elif p.iouType == 'keypoints':\n",
    "        computeIoU = self.computeOks\n",
    "    self.ious = {\n",
    "        (imgId, catId): computeIoU(imgId, catId)\n",
    "        for imgId in p.imgIds\n",
    "        for catId in catIds}\n",
    "\n",
    "    evaluateImg = self.evaluateImg\n",
    "    maxDet = p.maxDets[-1]\n",
    "    evalImgs = [\n",
    "        evaluateImg(imgId, catId, areaRng, maxDet)\n",
    "        for catId in catIds\n",
    "        for areaRng in p.areaRng\n",
    "        for imgId in p.imgIds\n",
    "    ]\n",
    "    # this is NOT in the pycocotools code, but could be done outside\n",
    "    evalImgs = np.asarray(evalImgs).reshape(len(catIds), len(p.areaRng), len(p.imgIds))\n",
    "    self._paramsEval = copy.deepcopy(self.params)\n",
    "    # toc = time.time()\n",
    "    # print('DONE (t={:0.2f}s).'.format(toc-tic))\n",
    "    return p.imgIds, evalImgs\n",
    "\n",
    "#################################################################\n",
    "# end of straight copy from pycocotools, just removing the prints\n",
    "#################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_world_size():\n",
    "    if not is_dist_avail_and_initialized():\n",
    "        return 1\n",
    "    return dist.get_world_size()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MetricLogger(object):\n",
    "    def __init__(self, delimiter=\"\\t\"):\n",
    "        self.meters = defaultdict(SmoothedValue)\n",
    "        self.delimiter = delimiter\n",
    "\n",
    "    def update(self, **kwargs):\n",
    "        for k, v in kwargs.items():\n",
    "            if isinstance(v, torch.Tensor):\n",
    "                v = v.item()\n",
    "            assert isinstance(v, (float, int))\n",
    "            self.meters[k].update(v)\n",
    "\n",
    "    def __getattr__(self, attr):\n",
    "        if attr in self.meters:\n",
    "            return self.meters[attr]\n",
    "        if attr in self.__dict__:\n",
    "            return self.__dict__[attr]\n",
    "        raise AttributeError(\"'{}' object has no attribute '{}'\".format(\n",
    "            type(self).__name__, attr))\n",
    "\n",
    "    def __str__(self):\n",
    "        loss_str = []\n",
    "        for name, meter in self.meters.items():\n",
    "            loss_str.append(\n",
    "                \"{}: {}\".format(name, str(meter))\n",
    "            )\n",
    "        return self.delimiter.join(loss_str)\n",
    "\n",
    "    def synchronize_between_processes(self):\n",
    "        for meter in self.meters.values():\n",
    "            meter.synchronize_between_processes()\n",
    "\n",
    "    def add_meter(self, name, meter):\n",
    "        self.meters[name] = meter\n",
    "\n",
    "    def log_every(self, iterable, print_freq, header=None):\n",
    "        i = 0\n",
    "        if not header:\n",
    "            header = ''\n",
    "        start_time = time.time()\n",
    "        end = time.time()\n",
    "        iter_time = SmoothedValue(fmt='{avg:.4f}')\n",
    "        data_time = SmoothedValue(fmt='{avg:.4f}')\n",
    "        space_fmt = ':' + str(len(str(len(iterable)))) + 'd'\n",
    "        if torch.cuda.is_available():\n",
    "            log_msg = self.delimiter.join([\n",
    "                header,\n",
    "                '[{0' + space_fmt + '}/{1}]',\n",
    "                'eta: {eta}',\n",
    "                '{meters}',\n",
    "                'time: {time}',\n",
    "                'data: {data}',\n",
    "                'max mem: {memory:.0f}'\n",
    "            ])\n",
    "        else:\n",
    "            log_msg = self.delimiter.join([\n",
    "                header,\n",
    "                '[{0' + space_fmt + '}/{1}]',\n",
    "                'eta: {eta}',\n",
    "                '{meters}',\n",
    "                'time: {time}',\n",
    "                'data: {data}'\n",
    "            ])\n",
    "        MB = 1024.0 * 1024.0\n",
    "        for obj in iterable:\n",
    "            data_time.update(time.time() - end)\n",
    "            yield obj\n",
    "            iter_time.update(time.time() - end)\n",
    "            if i % print_freq == 0 or i == len(iterable) - 1:\n",
    "                eta_seconds = iter_time.global_avg * (len(iterable) - i)\n",
    "                eta_string = str(datetime.timedelta(seconds=int(eta_seconds)))\n",
    "                if torch.cuda.is_available():\n",
    "                    print(log_msg.format(\n",
    "                        i, len(iterable), eta=eta_string,\n",
    "                        meters=str(self),\n",
    "                        time=str(iter_time), data=str(data_time),\n",
    "                        memory=torch.cuda.max_memory_allocated() / MB))\n",
    "                else:\n",
    "                    print(log_msg.format(\n",
    "                        i, len(iterable), eta=eta_string,\n",
    "                        meters=str(self),\n",
    "                        time=str(iter_time), data=str(data_time)))\n",
    "            i += 1\n",
    "            end = time.time()\n",
    "        total_time = time.time() - start_time\n",
    "        total_time_str = str(datetime.timedelta(seconds=int(total_time)))\n",
    "        print('{} Total time: {} ({:.4f} s / it)'.format(\n",
    "            header, total_time_str, total_time / len(iterable)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def evaluate(model, data_loader, device):\n",
    "    n_threads = torch.get_num_threads()\n",
    "    # FIXME remove this and make paste_masks_in_image run on the GPU\n",
    "    torch.set_num_threads(1)\n",
    "    cpu_device = torch.device(\"cpu\")\n",
    "    model.eval()\n",
    "    metric_logger = MetricLogger(delimiter=\"  \")\n",
    "    header = 'Test:'\n",
    "\n",
    "    coco = get_coco_api_from_dataset(data_loader.dataset)\n",
    "    iou_types = _get_iou_types(model)\n",
    "    coco_evaluator = CocoEvaluator(coco, iou_types)\n",
    "\n",
    "    for images, targets in metric_logger.log_every(data_loader, 100, header):\n",
    "        images = list(img.to(device) for img in images)\n",
    "        targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "\n",
    "        torch.cuda.synchronize()\n",
    "        model_time = time.time()\n",
    "        outputs = model(images)\n",
    "\n",
    "        outputs = [{k: v.to(cpu_device) for k, v in t.items()} for t in outputs]\n",
    "        model_time = time.time() - model_time\n",
    "\n",
    "        res = {target[\"image_id\"].item(): output for target, output in zip(targets, outputs)}\n",
    "        evaluator_time = time.time()\n",
    "        coco_evaluator.update(res)\n",
    "        evaluator_time = time.time() - evaluator_time\n",
    "        metric_logger.update(model_time=model_time, evaluator_time=evaluator_time)\n",
    "\n",
    "    # gather the stats from all processes\n",
    "    metric_logger.synchronize_between_processes()\n",
    "    print(\"Averaged stats:\", metric_logger)\n",
    "    coco_evaluator.synchronize_between_processes()\n",
    "\n",
    "    # accumulate predictions from all images\n",
    "    coco_evaluator.accumulate()\n",
    "    coco_evaluator.summarize()\n",
    "    torch.set_num_threads(n_threads)\n",
    "    return coco_evaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SmoothedValue(object):\n",
    "    \"\"\"Track a series of values and provide access to smoothed values over a\n",
    "    window or the global series average.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, window_size=20, fmt=None):\n",
    "        if fmt is None:\n",
    "            fmt = \"{median:.4f} ({global_avg:.4f})\"\n",
    "        self.deque = deque(maxlen=window_size)\n",
    "        self.total = 0.0\n",
    "        self.count = 0\n",
    "        self.fmt = fmt\n",
    "\n",
    "    def update(self, value, n=1):\n",
    "        self.deque.append(value)\n",
    "        self.count += n\n",
    "        self.total += value * n\n",
    "\n",
    "    def synchronize_between_processes(self):\n",
    "        \"\"\"\n",
    "        Warning: does not synchronize the deque!\n",
    "        \"\"\"\n",
    "        if not is_dist_avail_and_initialized():\n",
    "            return\n",
    "        t = torch.tensor([self.count, self.total], dtype=torch.float64, device='cuda')\n",
    "        dist.barrier()\n",
    "        dist.all_reduce(t)\n",
    "        t = t.tolist()\n",
    "        self.count = int(t[0])\n",
    "        self.total = t[1]\n",
    "\n",
    "    @property\n",
    "    def median(self):\n",
    "        d = torch.tensor(list(self.deque))\n",
    "        return d.median().item()\n",
    "\n",
    "    @property\n",
    "    def avg(self):\n",
    "        d = torch.tensor(list(self.deque), dtype=torch.float32)\n",
    "        return d.mean().item()\n",
    "\n",
    "    @property\n",
    "    def global_avg(self):\n",
    "        return self.total / self.count\n",
    "\n",
    "    @property\n",
    "    def max(self):\n",
    "        return max(self.deque)\n",
    "\n",
    "    @property\n",
    "    def value(self):\n",
    "        return self.deque[-1]\n",
    "\n",
    "    def __str__(self):\n",
    "        return self.fmt.format(\n",
    "            median=self.median,\n",
    "            avg=self.avg,\n",
    "            global_avg=self.global_avg,\n",
    "            max=self.max,\n",
    "            value=self.value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    # train on the GPU or on the CPU, if a GPU is not available\n",
    "    device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "\n",
    "    # our dataset has two classes only - background and person\n",
    "    num_classes = 2\n",
    "    # use our dataset and defined transformations\n",
    "    dataset = PennFudanDataset('/media/datasets/PennFundan/PennFudanPed', get_transform(train=True))\n",
    "    dataset_test = PennFudanDataset('/media/datasets/PennFundan/PennFudanPed', get_transform(train=False))\n",
    "\n",
    "    # split the dataset in train and test set\n",
    "    indices = torch.randperm(int(len(dataset)*.5)).tolist()\n",
    "    dataset = torch.utils.data.Subset(dataset, indices[0:20])\n",
    "    dataset_test = torch.utils.data.Subset(dataset_test, indices[20:30])\n",
    "\n",
    "    # define training and validation data loaders\n",
    "    data_loader = torch.utils.data.DataLoader(\n",
    "        dataset, batch_size=2, shuffle=True, num_workers=4,\n",
    "        collate_fn=collate_fn)\n",
    "\n",
    "    data_loader_test = torch.utils.data.DataLoader(\n",
    "        dataset_test, batch_size=1, shuffle=False, num_workers=4,\n",
    "        collate_fn=collate_fn)\n",
    "\n",
    "    # get the model using our helper function\n",
    "    model = get_model_instance_segmentation(num_classes)\n",
    "\n",
    "    # move model to the right device\n",
    "    model.to(device)\n",
    "\n",
    "    # construct an optimizer\n",
    "    params = [p for p in model.parameters() if p.requires_grad]\n",
    "    optimizer = torch.optim.SGD(params, lr=0.005,\n",
    "                                momentum=0.9, weight_decay=0.0005)\n",
    "    # and a learning rate scheduler\n",
    "    lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer,\n",
    "                                                   step_size=3,\n",
    "                                                   gamma=0.1)\n",
    "\n",
    "    # let's train it for 10 epochs\n",
    "    num_epochs = 10\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        # train for one epoch, printing every 10 iterations\n",
    "        train_one_epoch(model, optimizer, data_loader, device, epoch, print_freq=10)\n",
    "        # update the learning rate\n",
    "        lr_scheduler.step()\n",
    "        # evaluate on the test dataset\n",
    "        evaluate(model, data_loader_test, device=device)\n",
    "\n",
    "    print(\"That's it!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def warmup_lr_scheduler(optimizer, warmup_iters, warmup_factor):\n",
    "\n",
    "    def f(x):\n",
    "        if x >= warmup_iters:\n",
    "            return 1\n",
    "        alpha = float(x) / warmup_iters\n",
    "        return warmup_factor * (1 - alpha) + alpha\n",
    "\n",
    "    return torch.optim.lr_scheduler.LambdaLR(optimizer, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.distributed as dist\n",
    "import datetime\n",
    "\n",
    "def is_dist_avail_and_initialized():\n",
    "    if not dist.is_available():\n",
    "        return False\n",
    "    if not dist.is_initialized():\n",
    "        return False\n",
    "    return True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_coco_api_from_dataset(dataset):\n",
    "    for _ in range(10):\n",
    "        if isinstance(dataset, torchvision.datasets.CocoDetection):\n",
    "            break\n",
    "        if isinstance(dataset, torch.utils.data.Subset):\n",
    "            dataset = dataset.dataset\n",
    "    if isinstance(dataset, torchvision.datasets.CocoDetection):\n",
    "        return dataset.coco\n",
    "    return convert_to_coco_api(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C\n",
      "A\n",
      "C\n",
      "A\n",
      "Epoch: [0]  [ 0/10]  eta: 0:00:29  lr: 0.000560  loss: 4.9417 (4.9417)  loss_classifier: 0.5653 (0.5653)  loss_box_reg: 0.3494 (0.3494)  loss_mask: 3.9879 (3.9879)  loss_objectness: 0.0189 (0.0189)  loss_rpn_box_reg: 0.0202 (0.0202)  time: 2.9058  data: 0.1264  max mem: 2254\n",
      "Epoch: [0]  [ 9/10]  eta: 0:00:02  lr: 0.005000  loss: 1.1166 (2.1842)  loss_classifier: 0.1863 (0.2977)  loss_box_reg: 0.1787 (0.1941)  loss_mask: 0.7192 (1.6609)  loss_objectness: 0.0109 (0.0188)  loss_rpn_box_reg: 0.0133 (0.0127)  time: 2.0925  data: 0.0176  max mem: 2679\n",
      "Epoch: [0] Total time: 0:00:20 (2.0941 s / it)\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'convert_to_coco_api' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-21-263240bbee7e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-15-67f2d2ee339c>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     47\u001b[0m         \u001b[0mlr_scheduler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0;31m# evaluate on the test dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m         \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_loader_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"That's it!\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/testing2/lib/python3.7/site-packages/torch/autograd/grad_mode.py\u001b[0m in \u001b[0;36mdecorate_no_grad\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     47\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_no_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mdecorate_no_grad\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-13-7cdd16531a96>\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(model, data_loader, device)\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mheader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'Test:'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m     \u001b[0mcoco\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_coco_api_from_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_loader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m     \u001b[0miou_types\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_get_iou_types\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0mcoco_evaluator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCocoEvaluator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcoco\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miou_types\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-18-823c1c8acca2>\u001b[0m in \u001b[0;36mget_coco_api_from_dataset\u001b[0;34m(dataset)\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorchvision\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatasets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCocoDetection\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcoco\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mconvert_to_coco_api\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'convert_to_coco_api' is not defined"
     ]
    }
   ],
   "source": [
    "main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
