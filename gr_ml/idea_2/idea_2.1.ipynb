{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2503c145",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/jose/anaconda3/envs/test/bin/python\r\n"
     ]
    }
   ],
   "source": [
    "!which python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "67457d7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e9002982",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n"
     ]
    }
   ],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print('Using {} device'.format(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e5a6e310",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(),\n",
    "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7afead72",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1d791762",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_data = pd.read_csv('my_data/fake_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "18120f3d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>t</th>\n",
       "      <th>obj</th>\n",
       "      <th>x</th>\n",
       "      <th>y</th>\n",
       "      <th>z</th>\n",
       "      <th>vx</th>\n",
       "      <th>vy</th>\n",
       "      <th>vz</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>-2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>-2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>-2</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>-3</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>-4</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>-3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>-4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>-2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>-2</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    t  obj  x  y  z  vx  vy  vz\n",
       "0   0    1 -1 -1 -1  -1  -1  -1\n",
       "1   0    2 -2  0  0   0   2   3\n",
       "2   1    1 -1  1  1   1   1   3\n",
       "3   1    2 -2  2  2   2   2   3\n",
       "4   2    1 -1  3  3   3   1   3\n",
       "5   2    2 -2  4  4   4   2   3\n",
       "6   3    1 -1  5  5   5   1   3\n",
       "7   3    2 -3  6  6   6   2   3\n",
       "8   4    1 -4 -1 -1  -1  -1  -1\n",
       "9   4    2 -3  0  0   0   2   3\n",
       "10  5    1 -4  1  1   1   1   3\n",
       "11  5    2 -2  2  2   2   2   3\n",
       "12  6    1 -1  3  3   3   1   3\n",
       "13  6    2 -2  4  4   4   2   3\n",
       "14  7    1  1  5  5   5   1   3\n",
       "15  7    2  2  6  6   6   2   3"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_data.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8031c77c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(16, 8)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e3af0968",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "import os\n",
    "import numpy as np\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "abb3c0a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyCustomDataset(Dataset):\n",
    "    def __init__(self, csv_file, root_dir: str, transform=None,maxnobjects=2,deep=0,\n",
    "                nfeatures=6, future_prediction=1):\n",
    "        self.data = pd.read_csv(csv_file)\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        self.maxnobjects = maxnobjects\n",
    "        self.deep = deep\n",
    "        self.nfeatures = nfeatures\n",
    "        self.obj_id = list()\n",
    "        self.future_prediction = future_prediction\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.data['t'].max()-self.data['t'].min() -self.deep+1 -self.future_predicition\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "            \n",
    "        #FOr image        \n",
    "        data = np.zeros((self.maxnobjects,self.nfeatures, self.deep+1))\n",
    "        #images = np.zeros((self.maxnobjects,1, self.deep))\n",
    "        \n",
    "\n",
    "        for t in range(self.deep+1):\n",
    "            query_data= self.data[self.data['t']== idx+t].iloc[:,1:]\n",
    "            objects_ids = np.array(query_data.iloc[:,0])\n",
    "            for oi,o_id in enumerate(objects_ids):\n",
    "                #print(o_id, \"in \", self.obj_id, \"time\", t)\n",
    "                ind = self.obj_id.index(o_id) if o_id in self.obj_id else -1\n",
    "                if ind != -1:\n",
    "                    #print(\"updating \", o_id, \" index \", ind)\n",
    "                    data[ind, :,t] = query_data.iloc[oi,1:]\n",
    "                else:\n",
    "                    #print (\"current ids\", self.obj_id, \" adding \", o_id)\n",
    "                    self.obj_id.append(o_id)\n",
    "                    new_id = len(self.obj_id)-1\n",
    "                    #print (new_id, \" -> NEW ID\")\n",
    "                    data[new_id, :,t] = query_data.iloc[oi,1:]\n",
    "            #print(objects_ids)#, query_data.iloc[:,2:-1])\n",
    "            #nobjects, _ = query_data.shape\n",
    "            #data[:nobjects,:,t] = query_data.iloc[:,1:]\n",
    "            #images.append(query_data.iloc[:,-1])\n",
    "            #images[:nobjects,0,t] = query_data.iloc[:,-1]\n",
    "        #print(data, data.shape)\n",
    "\n",
    "        image_path = os.path.join(self.root_dir, \n",
    "                                    str(self.future_prediction+1+self.deep)+\".jpg\")\n",
    "        image = cv2.imread(image_path)\n",
    "        #data = np.array(self.data.iloc[idx, 0:-1]).reshape(-1,8)\n",
    "        sample = {'image': image, 'data': data}\n",
    "        \n",
    "        if self.transform:\n",
    "            sample = self.transform(sample)\n",
    "        return sample\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "82231cc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyCustomDataset2(Dataset):\n",
    "    def __init__(self, csv_file, root_dir: str, transform=None,maxnobjects=2,\n",
    "                nfeatures=6, future_prediction=1):\n",
    "        self.data = pd.read_csv(csv_file)\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        self.maxnobjects = maxnobjects\n",
    "        self.nfeatures = nfeatures\n",
    "        self.obj_id = list()\n",
    "        self.future_prediction = future_prediction\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.data['t'].max()-self.data['t'].min()+1 -self.future_prediction\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "            \n",
    "        #FOr image        \n",
    "        data = np.zeros((self.maxnobjects,self.nfeatures, 1))\n",
    "        query_data= self.data[self.data['t']== idx].iloc[:,1:]\n",
    "        objects_ids = np.array(query_data.iloc[:,0])\n",
    "        for oi,o_id in enumerate(objects_ids):\n",
    "            ind = self.obj_id.index(o_id) if o_id in self.obj_id else -1\n",
    "            if ind != -1:\n",
    "                data[ind, :] = np.array(query_data.iloc[oi,1:]).reshape(self.nfeatures,1)\n",
    "            else:\n",
    "                self.obj_id.append(o_id)\n",
    "                new_id = len(self.obj_id)-1\n",
    "                data[new_id, :,0] = query_data.iloc[oi,1:]\n",
    "\n",
    "        image_path = os.path.join(self.root_dir, str(idx+\n",
    "                            self.future_prediction)+\".jpg\")\n",
    "        image = cv2.imread(image_path)\n",
    "        #data = np.array(self.data.iloc[idx, 0:-1]).reshape(-1,8)\n",
    "        sample = {'image': image, 'data': data}\n",
    "        \n",
    "        if self.transform:\n",
    "            sample = self.transform(sample)\n",
    "        return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "aa4c2bed",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import transforms, utils\n",
    "from skimage import transform "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "ba887803",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Rescale(object):\n",
    "    \"\"\"Rescale the image in a sample to a given size.\n",
    "\n",
    "    Args:\n",
    "        output_size (tuple or int): Desired output size. If tuple, output is\n",
    "            matched to output_size. If int, smaller of image edges is matched\n",
    "            to output_size keeping aspect ratio the same.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, output_size):\n",
    "        assert isinstance(output_size, (int, tuple))\n",
    "        self.output_size = output_size\n",
    "\n",
    "    def __call__(self, sample):\n",
    "        image, data = sample['image'], sample['data']\n",
    "\n",
    "        #h, w = image.shape[:2]\n",
    "        #if isinstance(self.output_size, int):\n",
    "        #    if h > w:\n",
    "        #        new_h, new_w = self.output_size * h / w, self.output_size\n",
    "        #    else:\n",
    "        #        new_h, new_w = self.output_size, self.output_size * w / h\n",
    "        #else:\n",
    "        #    new_h, new_w = self.output_size\n",
    "\n",
    "        new_h, new_w = 256,256 #int(new_h), int(new_w)\n",
    "\n",
    "        img = transform.resize(image, (new_h, new_w))\n",
    "\n",
    "        # h and w are swapped for landmarks because for images,\n",
    "        # x and y axes are axis 1 and 0 respectively\n",
    "        #data = data * [new_w / w, new_h / h]\n",
    "\n",
    "        return {'image': img, 'data': data}\n",
    "\n",
    "class Normalize(object):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "        #self.norm = transform.Normalize()\n",
    "        \n",
    "    def __call__(self, sample):\n",
    "        image, data = sample['image'], sample['data']\n",
    "        image = image/255\n",
    "\n",
    "        return {'image': image,\n",
    "                'data': data}\n",
    "    \n",
    "class ToTensor(object):\n",
    "    \"\"\"Convert ndarrays in sample to Tensors.\"\"\"\n",
    "\n",
    "    def __call__(self, sample):\n",
    "        image, data = sample['image'], sample['data']\n",
    "\n",
    "        # swap color axis because\n",
    "        # numpy image: H x W x C\n",
    "        # torch image: C x H x W\n",
    "        image = image.transpose((2, 0, 1))\n",
    "        return {'image': torch.from_numpy(image),\n",
    "                'data': torch.from_numpy(data)}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12323cd5",
   "metadata": {},
   "source": [
    "#https://pytorch.org/tutorials/beginner/data_loading_tutorial.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "57d550e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#FROM TUTORIAL\n",
    "mytransform=transforms.Compose([\n",
    "                                Rescale(256),\n",
    "                                Normalize(),\n",
    "                                ToTensor()\n",
    "                               ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "id": "1b27e22e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#FOr testing\n",
    "#shape maxobject, nfeatures, deep\n",
    "my_train_dataset = MyCustomDataset2(\"my_data/fake_data.csv\", \"my_data/images/train\", \n",
    "                                    transform=mytransform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "6648614b",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_test_dataset = MyCustomDataset2(\"my_data/fake_testdata.csv\", \"my_data/images/test\", \n",
    "                                   transform=mytransform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "045cc8bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(my_train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "id": "014d6d4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INLINE torch.Size([3, 256, 256])\n",
      "INLINE torch.Size([3, 256, 256])\n",
      "INLINE torch.Size([3, 256, 256])\n",
      "INLINE torch.Size([3, 256, 256])\n",
      "INLINE torch.Size([3, 256, 256])\n",
      "INLINE torch.Size([3, 256, 256])\n",
      "INLINE torch.Size([3, 256, 256])\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(my_train_dataset)):\n",
    "    print(\"INLINE\", my_train_dataset[i]['image'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "1a60e85b",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 4\n",
    "\n",
    "trainloader = torch.utils.data.DataLoader(my_train_dataset, batch_size=batch_size,\n",
    "                                          shuffle=True, num_workers=2)\n",
    "testloader = torch.utils.data.DataLoader(my_test_dataset, batch_size=batch_size,\n",
    "                                         shuffle=False, num_workers=2)\n",
    "\n",
    "classes = ('plane', 'car', 'bird', 'cat',\n",
    "           'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "id": "f80a04b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 2)"
      ]
     },
     "execution_count": 199,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(trainloader), len(testloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "id": "f7ae9a13",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# functions to show an image\n",
    "\n",
    "\n",
    "def imshow(img):\n",
    "    img = img / 2 + 0.5     # unnormalize\n",
    "    npimg = img.numpy()\n",
    "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "id": "05ee6784",
   "metadata": {},
   "outputs": [],
   "source": [
    "def imshow2(images):\n",
    "    grid = utils.make_grid(images)\n",
    "    plt.imshow(grid.numpy().transpose((1,2,0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "id": "53e8552e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXkAAAB3CAYAAADmfjD5AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjQuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/MnkTPAAAACXBIWXMAAAsTAAALEwEAmpwYAAAI0klEQVR4nO3db8iddR3H8fenTc200GXK2kZOGNF6kjaG/SEkC82i+URYYC0w9sRAK4iZD6JnFhERUTDUWmWOoZJDiJIl+CT8m+XmWt62cncuV0hlPdCsbw/OTzqt+7/38Wy/837B4VzX91zXOb/ru/t87uv8zjm7U1VIkvr0mnEPQJI0Ooa8JHXMkJekjhnyktQxQ16SOmbIS1LHRhbySS5PcijJVJIdo3ocSdLsMorPySdZAfwG+CAwDTwEfKyqnlj2B5MkzWpUZ/Kbgamq+m1VvQjsBraM6LEkSbMYVcivAY4MrU+3miTpVbRyRPebGWr/My+UZDuwva2+c0TjkKSe/bmq3jTXBqMK+Wlg3dD6WuCZ4Q2qaiewEyCJ/4GOJC3e7+fbYFTTNQ8BG5KsT3IqsBXYO6LHkiTNYiRn8lX1UpJPAz8BVgC3VtWBUTyWJGl2I/kI5aIH4XSNJC3FI1W1aa4N/MarJHXMkJekjhnyktQxQ16SOmbIS1LHDHlJ6pghL0kdM+QlqWOGvCR1zJCXpI4Z8pLUMUNekjpmyEtSxwx5SeqYIS9JHTPkJaljhrwkdcyQl6SOGfKS1DFDXpI6ZshLUscMeUnqmCEvSR0z5CWpY/OGfJJbkxxLsn+otirJvUmebNdnD912Q5KpJIeSXDaqgUuS5reQM/nvApcfV9sB7KuqDcC+tk6SjcBW4O1tn28lWbFso5UkLcq8IV9V9wPPHVfeAuxqy7uAK4fqu6vqhao6DEwBm5dnqJKkxVrqnPx5VXUUoF2f2+prgCND20232v9Jsj3Jw0keXuIYJEnzWLnM95cZajXThlW1E9gJkGTGbSRJr8xSz+SfTbIaoF0fa/VpYN3QdmuBZ5Y+PEnSK7HUkN8LbGvL24C7h+pbk5yWZD2wAXjwlQ1RkrRU807XJLkduAQ4J8k08EXgJmBPkmuAp4GrAKrqQJI9wBPAS8C1VfWvEY1dkjSPVI1/Otw5eUlakkeqatNcG/iNV0nqmCEvSR0z5CWpY4a8JHXMkJekjhnyktQxQ16SOmbIS1LHDHlJ6pghL0kdM+QlqWOGvCR1zJCXpI4Z8pLUMUNekjpmyEtSxwx5SeqYIS9JHTPkJaljhrwkdcyQl6SOGfKS1DFDXpI6Nm/IJ1mX5L4kB5McSHJdq69Kcm+SJ9v12UP73JBkKsmhJJeN8gAkSbNbyJn8S8DnquptwMXAtUk2AjuAfVW1AdjX1mm3bQXeDlwOfCvJilEMXpI0t3lDvqqOVtWjbfl54CCwBtgC7Gqb7QKubMtbgN1V9UJVHQamgM3LPG5J0gIsak4+yfnAhcADwHlVdRQGvwiAc9tma4AjQ7tNt5ok6VW2cqEbJjkTuBO4vqr+lmTWTWeo1Qz3tx3YvtDHlyQt3oLO5JOcwiDgb6uqu1r52SSr2+2rgWOtPg2sG9p9LfDM8fdZVTuralNVbVrq4CVJc1vIp2sC3AIcrKqvDd20F9jWlrcBdw/VtyY5Lcl6YAPw4PINWZK0UAuZrnkP8HHg8SSPtdoXgJuAPUmuAZ4GrgKoqgNJ9gBPMPhkzrVV9a/lHrgkaX6p+r/p8ld/EMn4ByFJJ59H5pvy9huvktQxQ16SOmbIS1LHDHlJ6pghL0kdM+QlqWOGvCR1zJCXpI4Z8pLUMUNekjpmyEtSxwx5SeqYIS9JHTPkJaljC/7zfyP2d+DQuAcxZucAfx73IMbMHtgDsAeLOf63zLfBiRLyhyb9zwAmedge2AN7YA+W+/idrpGkjhnyktSxEyXkd457ACcAe2APwB6APVjW4z8h/sarJGk0TpQzeUnSCIw95JNcnuRQkqkkO8Y9nlFIsi7JfUkOJjmQ5LpWX5Xk3iRPtuuzh/a5ofXkUJLLxjf65ZVkRZJfJLmnrU9UD5KcleSOJL9uPw/vmsAefKY9D/YnuT3Ja3vvQZJbkxxLsn+otuhjTvLOJI+3276RJPM+eFWN7QKsAJ4CLgBOBX4JbBznmEZ0nKuBi9ry64HfABuBrwA7Wn0H8OW2vLH14jRgfevRinEfxzL14rPAD4F72vpE9QDYBXyqLZ8KnDVJPQDWAIeB09v6HuCTvfcAeB9wEbB/qLboYwYeBN4FBPgx8KH5HnvcZ/Kbgamq+m1VvQjsBraMeUzLrqqOVtWjbfl54CCDH/YtDJ70tOsr2/IWYHdVvVBVh4EpBr06qSVZC3wYuHmoPDE9SPIGBk/2WwCq6sWq+gsT1INmJXB6kpXA64Bn6LwHVXU/8Nxx5UUdc5LVwBuq6uc1SPzvDe0zq3GH/BrgyND6dKt1K8n5wIXAA8B5VXUUBr8IgHPbZr325evA54F/D9UmqQcXAH8CvtOmrG5OcgYT1IOq+gPwVeBp4Cjw16r6KRPUgyGLPeY1bfn4+pzGHfIzzSd1+3GfJGcCdwLXV9Xf5tp0htpJ3ZckHwGOVdUjC91lhtpJ3QMGZ7AXAd+uqguBfzB4mT6b7nrQ5p23MJiGeDNwRpKr59plhtpJ3YMFmO2Yl9SLcYf8NLBuaH0tg5du3UlyCoOAv62q7mrlZ9tLMNr1sVbvsS/vAT6a5HcMpuXen+QHTFYPpoHpqnqgrd/BIPQnqQcfAA5X1Z+q6p/AXcC7mawevGyxxzzdlo+vz2ncIf8QsCHJ+iSnAluBvWMe07Jr74DfAhysqq8N3bQX2NaWtwF3D9W3JjktyXpgA4M3XE5aVXVDVa2tqvMZ/Dv/rKquZrJ68EfgSJK3ttKlwBNMUA8YTNNcnOR17XlxKYP3qCapBy9b1DG3KZ3nk1zceveJoX1mdwK863wFg0+bPAXcOO7xjOgY38vgZdWvgMfa5QrgjcA+4Ml2vWponxtbTw6xgHfQT6YLcAn//XTNRPUAeAfwcPtZ+BFw9gT24EvAr4H9wPcZfIqk6x4AtzN4D+KfDM7Ir1nKMQObWt+eAr5J+0LrXBe/8SpJHRv3dI0kaYQMeUnqmCEvSR0z5CWpY4a8JHXMkJekjhnyktQxQ16SOvYfbF/Ju6jIQukAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# get some random training images\n",
    "dataiter = iter(trainloader)\n",
    "newdata = dataiter.next()\n",
    "images, data, = newdata[\"image\"], newdata[\"data\"]\n",
    "# show images\n",
    "#imshow(images[0])\n",
    "#print (images[0].shape)\n",
    "imshow2(images)\n",
    "# print labels\n",
    "#print(' '.join('%5s' % classes[labels[j]] for j in range(batch_size)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "id": "7fd879af",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataiter = iter(trainloader)\n",
    "newdata = dataiter.next()\n",
    "test_image, test_data = newdata[\"image\"].to(device, dtype=torch.float), newdata[\"data\"].to(device, dtype=torch.float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "id": "b7b6cb6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.uconv1 = nn.ConvTranspose2d(2,10,kernel_size=(3,3))\n",
    "        self.uconv2 = nn.ConvTranspose2d(10,40,kernel_size=(5,5))\n",
    "        self.uconv3 = nn.ConvTranspose2d(40,3, kernel_size=(10,10))\n",
    "        self.up1 = nn.Upsample(size=(256,256))#,scale_factor = 10)\n",
    "\n",
    "        #(2,10,kernel_size=3)\n",
    "\n",
    "        self.conv1 = nn.Conv2d(3, 6, 5)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "        self.fc1 = nn.Linear(12, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 1)\n",
    "    \n",
    "    def test(self,x):\n",
    "        print (x.shape)\n",
    "        x1 =  self.uconv3(self.uconv2(self.uconv1(x)))\n",
    "        print (x1.shape)\n",
    "        print (self.up1(x1).shape)\n",
    "\n",
    "    def forward(self, input1):\n",
    "        x = F.relu(self.uconv1(input1))\n",
    "        x = F.relu(self.uconv2(x))\n",
    "        x = F.relu(self.uconv3(x))\n",
    "        output1 = F.sigmoid(self.up1(x))\n",
    "\n",
    "        x2 = torch.flatten(input1, 1) # flatten all dimensions except batch\n",
    "        x2 = F.sigmoid(self.fc1(x2))\n",
    "        x2 = F.relu(self.fc2(x2))\n",
    "        x2 = F.sigmoid(self.fc3(x2))\n",
    "        return [output1, x2]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "id": "7b49bb4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (uconv1): ConvTranspose2d(2, 10, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (uconv2): ConvTranspose2d(10, 40, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (uconv3): ConvTranspose2d(40, 3, kernel_size=(10, 10), stride=(1, 1))\n",
      "  (up1): Upsample(size=(256, 256), mode=nearest)\n",
      "  (conv1): Conv2d(3, 6, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (conv2): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (fc1): Linear(in_features=12, out_features=120, bias=True)\n",
      "  (fc2): Linear(in_features=120, out_features=84, bias=True)\n",
      "  (fc3): Linear(in_features=84, out_features=1, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "net = Net().to(device)\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "id": "b9e00ecc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jose/anaconda3/envs/test/lib/python3.9/site-packages/torch/nn/functional.py:1805: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    }
   ],
   "source": [
    "output = net(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "id": "d28fcc83",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([4, 3, 256, 256]), torch.Size([4, 1]))"
      ]
     },
     "execution_count": 210,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output[0].shape, output[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "id": "58bc8f12",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "criterion_image = nn.MSELoss(reduction='sum')\n",
    "criterion_result = nn.MSELoss()\n",
    "\n",
    "optimizer_image = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "id": "2669d5bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(model,val_data):\n",
    "    net.eval()\n",
    "    val_loss = 0.0\n",
    "    val_criterion_image = nn.MSELoss(reduction='sum')\n",
    "    with torch.no_grad():\n",
    "        for i, data in enumerate(val_data, 0):\n",
    "            # get the inputs; data is a st of [inputs, labels]\n",
    "            #inputs, labels = data[0].to(device), data[1].to(device)\n",
    "            labels, inputs = data[\"image\"].to(device, dtype=torch.float), newdata[\"data\"].to(device, dtype=torch.float)\n",
    "            \n",
    "            # zero the parameter gradients\n",
    "            #optimizer.zero_grad()\n",
    "\n",
    "            # forward + backward + optimize\n",
    "            outputs = net(inputs)\n",
    "            print (outputs[0].shape, outputs[1].shape, labels.shape)\n",
    "\n",
    "            loss = val_criterion_image(outputs[0], labels)\n",
    "            val_loss +=loss.item()\n",
    "    return val_loss/len(val_data)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c761739",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,     1] loss: 199418.188\n",
      "[1,     2] loss: 147384.766\n",
      "[2,     1] loss: 196513.031\n",
      "[2,     2] loss: 147384.766\n",
      "[3,     1] loss: 196513.031\n",
      "[3,     2] loss: 147384.766\n",
      "[4,     1] loss: 196513.031\n",
      "[4,     2] loss: 147384.766\n",
      "[5,     1] loss: 196513.031\n",
      "[5,     2] loss: 147384.766\n",
      "[6,     1] loss: 196513.031\n",
      "[6,     2] loss: 147384.766\n",
      "[7,     1] loss: 196513.031\n",
      "[7,     2] loss: 147384.766\n",
      "[8,     1] loss: 196513.031\n",
      "[8,     2] loss: 147384.766\n",
      "[9,     1] loss: 196513.031\n",
      "[9,     2] loss: 147384.766\n",
      "[10,     1] loss: 196513.031\n",
      "[10,     2] loss: 147384.766\n",
      "[11,     1] loss: 196513.031\n",
      "[11,     2] loss: 147384.766\n",
      "[12,     1] loss: 196513.031\n",
      "[12,     2] loss: 147384.766\n",
      "[13,     1] loss: 196513.031\n",
      "[13,     2] loss: 147384.766\n",
      "[14,     1] loss: 196513.031\n",
      "[14,     2] loss: 147384.766\n",
      "[15,     1] loss: 196513.031\n",
      "[15,     2] loss: 147384.766\n",
      "[16,     1] loss: 196513.031\n",
      "[16,     2] loss: 147384.766\n",
      "[17,     1] loss: 196513.031\n",
      "[17,     2] loss: 147384.766\n",
      "[18,     1] loss: 196513.031\n",
      "[18,     2] loss: 147384.766\n",
      "[19,     1] loss: 196513.031\n",
      "[19,     2] loss: 147384.766\n",
      "[20,     1] loss: 196513.031\n",
      "[20,     2] loss: 147384.766\n",
      "[21,     1] loss: 196513.031\n",
      "[21,     2] loss: 147384.766\n",
      "[22,     1] loss: 196513.031\n",
      "[22,     2] loss: 147384.766\n",
      "[23,     1] loss: 196513.031\n",
      "[23,     2] loss: 147384.766\n",
      "[24,     1] loss: 196513.031\n",
      "[24,     2] loss: 147384.766\n",
      "[25,     1] loss: 196513.031\n",
      "[25,     2] loss: 147384.766\n",
      "[26,     1] loss: 196513.031\n",
      "[26,     2] loss: 147384.766\n",
      "[27,     1] loss: 196513.031\n",
      "[27,     2] loss: 147384.766\n",
      "[28,     1] loss: 196513.031\n",
      "[28,     2] loss: 147384.766\n"
     ]
    }
   ],
   "source": [
    "net.train()\n",
    "for epoch in range(50):  # loop over the dataset multiple times\n",
    "    running_loss = 0.0\n",
    "    epoch_loss = 0.0\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "        # get the inputs; data is a st of [inputs, labels]\n",
    "        labels, inputs = data[\"image\"].to(device, dtype=torch.float), data[\"data\"].to(device, dtype=torch.float)\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        optimizer_image.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs = net(inputs)\n",
    "        #print (outputs[0].shape, outputs[1].shape, labels.shape)\n",
    "        loss = criterion_image(outputs[0], labels)\n",
    "        loss.backward()\n",
    "        optimizer_image.step()\n",
    "\n",
    "        # print statistics\n",
    "        running_loss += loss.item()\n",
    "        epoch_loss += loss.item()\n",
    "\n",
    "        if True:#i % 2000 == 1999:    # print every 2000 mini-batches\n",
    "            print('[%d, %5d] loss: %.3f'  %\n",
    "                  (epoch + 1, i + 1, running_loss / 1))\n",
    "            running_loss = 0.0\n",
    "\n",
    "    #val_loss = validate(net, testloader)\n",
    "    #print('[%d, %5d] epoch loss: %.3f validation loss: %.3f'  %\n",
    "    #    (epoch + 1, i + 1, epoch_loss / len(trainloader), val_loss))\n",
    "    #epoch_loss = 0.0\n",
    "\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d5e50fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3ba622b",
   "metadata": {},
   "outputs": [],
   "source": [
    "count_parameters(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9f5d936",
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = './demo_net.pth'\n",
    "torch.save(net.state_dict(), PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91d5e18f",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataiter = iter(testloader)\n",
    "data = dataiter.next()\n",
    "labels, inputs = data[\"image\"].to(device, dtype=torch.float), data[\"data\"].to(device, dtype=torch.float)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "253d1d87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print images\n",
    "imshow(torchvision.utils.make_grid(labels.to(\"cpu\")))\n",
    "#print('GroundTruth: ', ' '.join('%5s' % classes[labels[j]] for j in range(4)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c8b5dc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "net = Net()\n",
    "net.load_state_dict(torch.load(PATH))\n",
    "\n",
    "\n",
    "#_, predicted = torch.max(outputs, 1)\n",
    "\n",
    "#print('Predicted: ', ' '.join('%5s' % classes[predicted[j]]\n",
    " #                             for j in range(4)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ddd17e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "correct = 0\n",
    "total = 0\n",
    "# since we're not training, we don't need to calculate the gradients for our outputs\n",
    "with torch.no_grad():\n",
    "    for data in testloader:\n",
    "        images, labels = data\n",
    "        # calculate outputs by running images through the network\n",
    "        outputs = net(images)\n",
    "        # the class with the highest energy is what we choose as prediction\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "print('Accuracy of the network on the 10000 test images: %d %%' % (\n",
    "    100 * correct / total))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d55e9a22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare to count predictions for each class\n",
    "correct_pred = {classname: 0 for classname in classes}\n",
    "total_pred = {classname: 0 for cla#ssname in classes}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de7b9ed3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# again no gradients needed\n",
    "with torch.no_grad():\n",
    "    for data in testloader:\n",
    "        labels, inputs = data[\"image\"].to(\"cpu\", dtype=torch.float), data[\"data\"].to(\"cpu\", dtype=torch.float)\n",
    "        outputs = net(inputs)\n",
    "        imshow(torchvision.utils.make_grid(outputs[0].to(\"cpu\")))\n",
    "        #_, predictions = torch.max(outputs, 1)\n",
    "        # collect the correct predictions for each class\n",
    "        #for label, prediction in zip(labels, predictions):\n",
    "        #    if label == prediction:\n",
    "        #        correct_pred[classes[label]] += 1\n",
    "        #    total_pred[classes[label]] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "817479e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print accuracy for each class\n",
    "#for classname, correct_count in correct_pred.items():\n",
    "##    accuracy = 100 * float(correct_count) / total_pred[classname]\n",
    " #   print(\"Accuracy for class {:5s} is: {:.1f} %\".format(classname,\n",
    "                                                   accuracy))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
