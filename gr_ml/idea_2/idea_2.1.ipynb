{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2503c145",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/jose/anaconda3/envs/test/bin/python\r\n"
     ]
    }
   ],
   "source": [
    "!which python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "67457d7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e9002982",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n"
     ]
    }
   ],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print('Using {} device'.format(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e5a6e310",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(),\n",
    "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7afead72",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1d791762",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_data = pd.read_csv('my_data/fake_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "18120f3d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>t</th>\n",
       "      <th>obj</th>\n",
       "      <th>x</th>\n",
       "      <th>y</th>\n",
       "      <th>z</th>\n",
       "      <th>vx</th>\n",
       "      <th>vy</th>\n",
       "      <th>vz</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>-2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>-2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>-2</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>-3</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>-4</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>-3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>-4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>-2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>-2</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    t  obj  x  y  z  vx  vy  vz\n",
       "0   0    1 -1 -1 -1  -1  -1  -1\n",
       "1   0    2 -2  0  0   0   2   3\n",
       "2   1    1 -1  1  1   1   1   3\n",
       "3   1    2 -2  2  2   2   2   3\n",
       "4   2    1 -1  3  3   3   1   3\n",
       "5   2    2 -2  4  4   4   2   3\n",
       "6   3    1 -1  5  5   5   1   3\n",
       "7   3    2 -3  6  6   6   2   3\n",
       "8   4    1 -4 -1 -1  -1  -1  -1\n",
       "9   4    2 -3  0  0   0   2   3\n",
       "10  5    1 -4  1  1   1   1   3\n",
       "11  5    2 -2  2  2   2   2   3\n",
       "12  6    1 -1  3  3   3   1   3\n",
       "13  6    2 -2  4  4   4   2   3\n",
       "14  7    1  1  5  5   5   1   3\n",
       "15  7    2  2  6  6   6   2   3"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_data.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8031c77c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(16, 8)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e3af0968",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "import os\n",
    "import numpy as np\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "abb3c0a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyCustomDataset(Dataset):\n",
    "    def __init__(self, csv_file, root_dir: str, transform=None,maxnobjects=2,deep=0,\n",
    "                nfeatures=6, future_prediction=1):\n",
    "        self.data = pd.read_csv(csv_file)\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        self.maxnobjects = maxnobjects\n",
    "        self.deep = deep\n",
    "        self.nfeatures = nfeatures\n",
    "        self.obj_id = list()\n",
    "        self.future_prediction = future_prediction\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.data['t'].max()-self.data['t'].min() -self.deep+1 -self.future_predicition\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "            \n",
    "        #FOr image        \n",
    "        data = np.zeros((self.maxnobjects,self.nfeatures, self.deep+1))\n",
    "        #images = np.zeros((self.maxnobjects,1, self.deep))\n",
    "        \n",
    "\n",
    "        for t in range(self.deep+1):\n",
    "            query_data= self.data[self.data['t']== idx+t].iloc[:,1:]\n",
    "            objects_ids = np.array(query_data.iloc[:,0])\n",
    "            for oi,o_id in enumerate(objects_ids):\n",
    "                #print(o_id, \"in \", self.obj_id, \"time\", t)\n",
    "                ind = self.obj_id.index(o_id) if o_id in self.obj_id else -1\n",
    "                if ind != -1:\n",
    "                    #print(\"updating \", o_id, \" index \", ind)\n",
    "                    data[ind, :,t] = query_data.iloc[oi,1:]\n",
    "                else:\n",
    "                    #print (\"current ids\", self.obj_id, \" adding \", o_id)\n",
    "                    self.obj_id.append(o_id)\n",
    "                    new_id = len(self.obj_id)-1\n",
    "                    #print (new_id, \" -> NEW ID\")\n",
    "                    data[new_id, :,t] = query_data.iloc[oi,1:]\n",
    "            #print(objects_ids)#, query_data.iloc[:,2:-1])\n",
    "            #nobjects, _ = query_data.shape\n",
    "            #data[:nobjects,:,t] = query_data.iloc[:,1:]\n",
    "            #images.append(query_data.iloc[:,-1])\n",
    "            #images[:nobjects,0,t] = query_data.iloc[:,-1]\n",
    "        #print(data, data.shape)\n",
    "\n",
    "        image_path = os.path.join(self.root_dir, \n",
    "                                    str(self.future_prediction+1+self.deep)+\".jpg\")\n",
    "        image = cv2.imread(image_path)\n",
    "        #data = np.array(self.data.iloc[idx, 0:-1]).reshape(-1,8)\n",
    "        sample = {'image': image, 'data': data}\n",
    "        \n",
    "        if self.transform:\n",
    "            sample = self.transform(sample)\n",
    "        return sample\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b324f635",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyCustomDataset2(Dataset):\n",
    "    def __init__(self, csv_file, root_dir: str, transform=None,maxnobjects=2,\n",
    "                nfeatures=6, future_prediction=1):\n",
    "        self.data = pd.read_csv(csv_file)\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        self.maxnobjects = maxnobjects\n",
    "        self.nfeatures = nfeatures\n",
    "        self.obj_id = list()\n",
    "        self.future_prediction = future_prediction\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.data['t'].max()-self.data['t'].min()+1 -self.future_prediction\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "            \n",
    "        #FOr image        \n",
    "        data = np.zeros((self.maxnobjects,self.nfeatures, 1))\n",
    "        query_data= self.data[self.data['t']== idx].iloc[:,1:]\n",
    "        objects_ids = np.array(query_data.iloc[:,0])\n",
    "        for oi,o_id in enumerate(objects_ids):\n",
    "            ind = self.obj_id.index(o_id) if o_id in self.obj_id else -1\n",
    "            if ind != -1:\n",
    "                data[ind, :] = np.array(query_data.iloc[oi,1:]).reshape(self.nfeatures,1)\n",
    "            else:\n",
    "                self.obj_id.append(o_id)\n",
    "                new_id = len(self.obj_id)-1\n",
    "                data[new_id, :,0] = query_data.iloc[oi,1:]\n",
    "\n",
    "        image_path = os.path.join(self.root_dir, str(idx+\n",
    "                            self.future_prediction)+\".jpg\")\n",
    "        image = cv2.imread(image_path)\n",
    "        #data = np.array(self.data.iloc[idx, 0:-1]).reshape(-1,8)\n",
    "        sample = {'image': image, 'data': data}\n",
    "        \n",
    "        if self.transform:\n",
    "            sample = self.transform(sample)\n",
    "        return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cb63cc1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import transforms, utils\n",
    "from skimage import transform "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "9508f5d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Rescale(object):\n",
    "    \"\"\"Rescale the image in a sample to a given size.\n",
    "\n",
    "    Args:\n",
    "        output_size (tuple or int): Desired output size. If tuple, output is\n",
    "            matched to output_size. If int, smaller of image edges is matched\n",
    "            to output_size keeping aspect ratio the same.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, output_size):\n",
    "        assert isinstance(output_size, (int, tuple))\n",
    "        self.output_size = output_size\n",
    "\n",
    "    def __call__(self, sample):\n",
    "        image, data = sample['image'], sample['data']\n",
    "\n",
    "        #h, w = image.shape[:2]\n",
    "        #if isinstance(self.output_size, int):\n",
    "        #    if h > w:\n",
    "        #        new_h, new_w = self.output_size * h / w, self.output_size\n",
    "        #    else:\n",
    "        #        new_h, new_w = self.output_size, self.output_size * w / h\n",
    "        #else:\n",
    "        #    new_h, new_w = self.output_size\n",
    "\n",
    "        new_h, new_w = 256,256 #int(new_h), int(new_w)\n",
    "\n",
    "        img = transform.resize(image, (new_h, new_w))\n",
    "\n",
    "        # h and w are swapped for landmarks because for images,\n",
    "        # x and y axes are axis 1 and 0 respectively\n",
    "        #data = data * [new_w / w, new_h / h]\n",
    "\n",
    "        return {'image': img, 'data': data}\n",
    "\n",
    "class Normalize(object):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "        #self.norm = transform.Normalize()\n",
    "        \n",
    "    def __call__(self, sample):\n",
    "        image, data = sample['image'], sample['data']\n",
    "        image = image/255\n",
    "\n",
    "        return {'image': image,\n",
    "                'data': data}\n",
    "    \n",
    "class ToTensor(object):\n",
    "    \"\"\"Convert ndarrays in sample to Tensors.\"\"\"\n",
    "\n",
    "    def __call__(self, sample):\n",
    "        image, data = sample['image'], sample['data']\n",
    "\n",
    "        # swap color axis because\n",
    "        # numpy image: H x W x C\n",
    "        # torch image: C x H x W\n",
    "        image = image.transpose((2, 0, 1))\n",
    "        return {'image': torch.from_numpy(image),\n",
    "                'data': torch.from_numpy(data)}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebce151f",
   "metadata": {},
   "source": [
    "#https://pytorch.org/tutorials/beginner/data_loading_tutorial.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "87addb1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#FROM TUTORIAL\n",
    "mytransform=transforms.Compose([\n",
    "                                Rescale(256),\n",
    "                                Normalize(),\n",
    "                                ToTensor()\n",
    "                               ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "id": "1b27e22e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#FOr testing\n",
    "#shape maxobject, nfeatures, deep\n",
    "my_train_dataset = MyCustomDataset2(\"my_data/fake_data.csv\", \"my_data/images/train\", \n",
    "                                    transform=mytransform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "7205930b",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_test_dataset = MyCustomDataset2(\"my_data/fake_testdata.csv\", \"my_data/images/test\", \n",
    "                                   transform=mytransform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "045cc8bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(my_train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "id": "014d6d4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INLINE torch.Size([3, 256, 256])\n",
      "INLINE torch.Size([3, 256, 256])\n",
      "INLINE torch.Size([3, 256, 256])\n",
      "INLINE torch.Size([3, 256, 256])\n",
      "INLINE torch.Size([3, 256, 256])\n",
      "INLINE torch.Size([3, 256, 256])\n",
      "INLINE torch.Size([3, 256, 256])\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(my_train_dataset)):\n",
    "    print(\"INLINE\", my_train_dataset[i]['image'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "1a60e85b",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 4\n",
    "\n",
    "trainloader = torch.utils.data.DataLoader(my_train_dataset, batch_size=batch_size,\n",
    "                                          shuffle=True, num_workers=2)\n",
    "testloader = torch.utils.data.DataLoader(my_test_dataset, batch_size=batch_size,\n",
    "                                         shuffle=False, num_workers=2)\n",
    "\n",
    "classes = ('plane', 'car', 'bird', 'cat',\n",
    "           'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "id": "f80a04b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 2)"
      ]
     },
     "execution_count": 199,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(trainloader), len(testloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "id": "f7ae9a13",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# functions to show an image\n",
    "\n",
    "\n",
    "def imshow(img):\n",
    "    img = img / 2 + 0.5     # unnormalize\n",
    "    npimg = img.numpy()\n",
    "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "id": "0f035f58",
   "metadata": {},
   "outputs": [],
   "source": [
    "def imshow2(images):\n",
    "    grid = utils.make_grid(images)\n",
    "    plt.imshow(grid.numpy().transpose((1,2,0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "id": "53e8552e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# get some random training images\n",
    "dataiter = iter(trainloader)\n",
    "newdata = dataiter.next()\n",
    "images, data, = newdata[\"image\"], newdata[\"data\"]\n",
    "# show images\n",
    "#imshow(images[0])\n",
    "#print (images[0].shape)\n",
    "imshow2(images)\n",
    "# print labels\n",
    "#print(' '.join('%5s' % classes[labels[j]] for j in range(batch_size)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "id": "451e40b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataiter = iter(trainloader)\n",
    "newdata = dataiter.next()\n",
    "test_image, test_data = newdata[\"image\"].to(device, dtype=torch.float), newdata[\"data\"].to(device, dtype=torch.float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "id": "b7b6cb6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.uconv1 = nn.ConvTranspose2d(2,10,kernel_size=(3,3))\n",
    "        self.uconv2 = nn.ConvTranspose2d(10,40,kernel_size=(5,5))\n",
    "        self.uconv3 = nn.ConvTranspose2d(40,3, kernel_size=(10,10))\n",
    "        self.up1 = nn.Upsample(size=(256,256))#,scale_factor = 10)\n",
    "\n",
    "        #(2,10,kernel_size=3)\n",
    "\n",
    "        self.conv1 = nn.Conv2d(3, 6, 5)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "        self.fc1 = nn.Linear(12, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 1)\n",
    "    \n",
    "    def test(self,x):\n",
    "        print (x.shape)\n",
    "        x1 =  self.uconv3(self.uconv2(self.uconv1(x)))\n",
    "        print (x1.shape)\n",
    "        print (self.up1(x1).shape)\n",
    "\n",
    "    def forward(self, input1):\n",
    "        x = F.relu(self.uconv1(input1))\n",
    "        x = F.relu(self.uconv2(x))\n",
    "        x = F.relu(self.uconv3(x))\n",
    "        output1 = F.sigmoid(self.up1(x))\n",
    "\n",
    "        x2 = torch.flatten(input1, 1) # flatten all dimensions except batch\n",
    "        x2 = F.sigmoid(self.fc1(x2))\n",
    "        x2 = F.relu(self.fc2(x2))\n",
    "        x2 = F.sigmoid(self.fc3(x2))\n",
    "        return [output1, x2]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "id": "7b49bb4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (uconv1): ConvTranspose2d(2, 10, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (uconv2): ConvTranspose2d(10, 40, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (uconv3): ConvTranspose2d(40, 3, kernel_size=(10, 10), stride=(1, 1))\n",
      "  (up1): Upsample(size=(256, 256), mode=nearest)\n",
      "  (conv1): Conv2d(3, 6, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (conv2): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (fc1): Linear(in_features=12, out_features=120, bias=True)\n",
      "  (fc2): Linear(in_features=120, out_features=84, bias=True)\n",
      "  (fc3): Linear(in_features=84, out_features=1, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "net = Net().to(device)\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "id": "04d9b9d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jose/anaconda3/envs/test/lib/python3.9/site-packages/torch/nn/functional.py:1805: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    }
   ],
   "source": [
    "output = net(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "id": "13571c52",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([4, 3, 256, 256]), torch.Size([4, 1]))"
      ]
     },
     "execution_count": 210,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output[0].shape, output[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "id": "58bc8f12",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "criterion_image = nn.MSELoss(reduction='sum')\n",
    "criterion_result = nn.MSELoss()\n",
    "\n",
    "optimizer_image = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "id": "2669d5bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(model,val_data):\n",
    "    net.eval()\n",
    "    val_loss = 0.0\n",
    "    val_criterion_image = nn.MSELoss(reduction='sum')\n",
    "    with torch.no_grad():\n",
    "        for i, data in enumerate(val_data, 0):\n",
    "            # get the inputs; data is a st of [inputs, labels]\n",
    "            #inputs, labels = data[0].to(device), data[1].to(device)\n",
    "            labels, inputs = data[\"image\"].to(device, dtype=torch.float), newdata[\"data\"].to(device, dtype=torch.float)\n",
    "            \n",
    "            # zero the parameter gradients\n",
    "            #optimizer.zero_grad()\n",
    "\n",
    "            # forward + backward + optimize\n",
    "            outputs = net(inputs)\n",
    "            print (outputs[0].shape, outputs[1].shape, labels.shape)\n",
    "\n",
    "            loss = val_criterion_image(outputs[0], labels)\n",
    "            val_loss +=loss.item()\n",
    "    return val_loss/len(val_data)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "id": "2c761739",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,     1] loss: 199418.188\n",
      "[1,     2] loss: 147384.766\n",
      "[2,     1] loss: 196513.031\n",
      "[2,     2] loss: 147384.766\n",
      "[3,     1] loss: 196513.031\n",
      "[3,     2] loss: 147384.766\n",
      "[4,     1] loss: 196513.031\n",
      "[4,     2] loss: 147384.766\n",
      "[5,     1] loss: 196513.031\n",
      "[5,     2] loss: 147384.766\n",
      "[6,     1] loss: 196513.031\n",
      "[6,     2] loss: 147384.766\n",
      "[7,     1] loss: 196513.031\n",
      "[7,     2] loss: 147384.766\n",
      "[8,     1] loss: 196513.031\n",
      "[8,     2] loss: 147384.766\n",
      "[9,     1] loss: 196513.031\n",
      "[9,     2] loss: 147384.766\n",
      "[10,     1] loss: 196513.031\n",
      "[10,     2] loss: 147384.766\n",
      "[11,     1] loss: 196513.031\n",
      "[11,     2] loss: 147384.766\n",
      "[12,     1] loss: 196513.031\n",
      "[12,     2] loss: 147384.766\n",
      "[13,     1] loss: 196513.031\n",
      "[13,     2] loss: 147384.766\n",
      "[14,     1] loss: 196513.031\n",
      "[14,     2] loss: 147384.766\n",
      "[15,     1] loss: 196513.031\n",
      "[15,     2] loss: 147384.766\n",
      "[16,     1] loss: 196513.031\n",
      "[16,     2] loss: 147384.766\n",
      "[17,     1] loss: 196513.031\n",
      "[17,     2] loss: 147384.766\n",
      "[18,     1] loss: 196513.031\n",
      "[18,     2] loss: 147384.766\n",
      "[19,     1] loss: 196513.031\n",
      "[19,     2] loss: 147384.766\n",
      "[20,     1] loss: 196513.031\n",
      "[20,     2] loss: 147384.766\n",
      "[21,     1] loss: 196513.031\n",
      "[21,     2] loss: 147384.766\n",
      "[22,     1] loss: 196513.031\n",
      "[22,     2] loss: 147384.766\n",
      "[23,     1] loss: 196513.031\n",
      "[23,     2] loss: 147384.766\n",
      "[24,     1] loss: 196513.031\n",
      "[24,     2] loss: 147384.766\n",
      "[25,     1] loss: 196513.031\n",
      "[25,     2] loss: 147384.766\n",
      "[26,     1] loss: 196513.031\n",
      "[26,     2] loss: 147384.766\n",
      "[27,     1] loss: 196513.031\n",
      "[27,     2] loss: 147384.766\n",
      "[28,     1] loss: 196513.031\n",
      "[28,     2] loss: 147384.766\n",
      "[29,     1] loss: 196513.031\n",
      "[29,     2] loss: 147384.766\n",
      "[30,     1] loss: 196513.031\n",
      "[30,     2] loss: 147384.766\n",
      "[31,     1] loss: 196513.031\n",
      "[31,     2] loss: 147384.766\n",
      "[32,     1] loss: 196513.031\n",
      "[32,     2] loss: 147384.766\n",
      "[33,     1] loss: 196513.031\n",
      "[33,     2] loss: 147384.766\n",
      "[34,     1] loss: 196513.031\n",
      "[34,     2] loss: 147384.766\n",
      "[35,     1] loss: 196513.031\n",
      "[35,     2] loss: 147384.766\n",
      "[36,     1] loss: 196513.031\n",
      "[36,     2] loss: 147384.766\n",
      "[37,     1] loss: 196513.031\n",
      "[37,     2] loss: 147384.766\n",
      "[38,     1] loss: 196513.031\n",
      "[38,     2] loss: 147384.766\n",
      "[39,     1] loss: 196513.031\n",
      "[39,     2] loss: 147384.766\n",
      "[40,     1] loss: 196513.031\n",
      "[40,     2] loss: 147384.766\n",
      "[41,     1] loss: 196513.031\n",
      "[41,     2] loss: 147384.766\n",
      "[42,     1] loss: 196513.031\n",
      "[42,     2] loss: 147384.766\n",
      "[43,     1] loss: 196513.031\n",
      "[43,     2] loss: 147384.766\n",
      "[44,     1] loss: 196513.031\n",
      "[44,     2] loss: 147384.766\n",
      "[45,     1] loss: 196513.031\n",
      "[45,     2] loss: 147384.766\n",
      "[46,     1] loss: 196513.031\n",
      "[46,     2] loss: 147384.766\n",
      "[47,     1] loss: 196513.031\n",
      "[47,     2] loss: 147384.766\n",
      "[48,     1] loss: 196513.031\n",
      "[48,     2] loss: 147384.766\n",
      "[49,     1] loss: 196513.031\n",
      "[49,     2] loss: 147384.766\n",
      "[50,     1] loss: 196513.031\n",
      "[50,     2] loss: 147384.766\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "net.train()\n",
    "for epoch in range(50):  # loop over the dataset multiple times\n",
    "    running_loss = 0.0\n",
    "    epoch_loss = 0.0\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "        # get the inputs; data is a st of [inputs, labels]\n",
    "        labels, inputs = data[\"image\"].to(device, dtype=torch.float), data[\"data\"].to(device, dtype=torch.float)\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        optimizer_image.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs = net(inputs)\n",
    "        #print (outputs[0].shape, outputs[1].shape, labels.shape)\n",
    "        loss = criterion_image(outputs[0], labels)\n",
    "        loss.backward()\n",
    "        optimizer_image.step()\n",
    "\n",
    "        # print statistics\n",
    "        running_loss += loss.item()\n",
    "        epoch_loss += loss.item()\n",
    "\n",
    "        if True:#i % 2000 == 1999:    # print every 2000 mini-batches\n",
    "            print('[%d, %5d] loss: %.3f'  %\n",
    "                  (epoch + 1, i + 1, running_loss / 1))\n",
    "            running_loss = 0.0\n",
    "\n",
    "    #val_loss = validate(net, testloader)\n",
    "    #print('[%d, %5d] epoch loss: %.3f validation loss: %.3f'  %\n",
    "    #    (epoch + 1, i + 1, epoch_loss / len(trainloader), val_loss))\n",
    "    #epoch_loss = 0.0\n",
    "\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "id": "815104c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "id": "4b825e02",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "36914"
      ]
     },
     "execution_count": 216,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_parameters(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "id": "a9f5d936",
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = './demo_net.pth'\n",
    "torch.save(net.state_dict(), PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "id": "91d5e18f",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataiter = iter(testloader)\n",
    "data = dataiter.next()\n",
    "labels, inputs = data[\"image\"].to(device, dtype=torch.float), data[\"data\"].to(device, dtype=torch.float)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "id": "253d1d87",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# print images\n",
    "imshow(torchvision.utils.make_grid(labels.to(\"cpu\")))\n",
    "#print('GroundTruth: ', ' '.join('%5s' % classes[labels[j]] for j in range(4)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "id": "9c8b5dc5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 220,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net = Net()\n",
    "net.load_state_dict(torch.load(PATH))\n",
    "\n",
    "\n",
    "#_, predicted = torch.max(outputs, 1)\n",
    "\n",
    "#print('Predicted: ', ' '.join('%5s' % classes[predicted[j]]\n",
    " #                             for j in range(4)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "id": "7ddd17e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\ncorrect = 0\\ntotal = 0\\n# since we're not training, we don't need to calculate the gradients for our outputs\\nwith torch.no_grad():\\n    for data in testloader:\\n        images, labels = data\\n        # calculate outputs by running images through the network\\n        outputs = net(images)\\n        # the class with the highest energy is what we choose as prediction\\n        _, predicted = torch.max(outputs.data, 1)\\n        total += labels.size(0)\\n        correct += (predicted == labels).sum().item()\\n\\nprint('Accuracy of the network on the 10000 test images: %d %%' % (\\n    100 * correct / total))\\n\""
      ]
     },
     "execution_count": 222,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\"\"\"\n",
    "correct = 0\n",
    "total = 0\n",
    "# since we're not training, we don't need to calculate the gradients for our outputs\n",
    "with torch.no_grad():\n",
    "    for data in testloader:\n",
    "        images, labels = data\n",
    "        # calculate outputs by running images through the network\n",
    "        outputs = net(images)\n",
    "        # the class with the highest energy is what we choose as prediction\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "print('Accuracy of the network on the 10000 test images: %d %%' % (\n",
    "    100 * correct / total))\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d55e9a22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare to count predictions for each class\n",
    "#correct_pred = {classname: 0 for classname in classes}\n",
    "#total_pred = {classname: 0 for cla#ssname in classes}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "id": "de7b9ed3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# again no gradients needed\n",
    "with torch.no_grad():\n",
    "    for data in testloader:\n",
    "        labels, inputs = data[\"image\"].to(\"cpu\", dtype=torch.float), data[\"data\"].to(\"cpu\", dtype=torch.float)\n",
    "        outputs = net(inputs)\n",
    "        print (np.counts)\n",
    "        imshow(torchvision.utils.make_grid(outputs[0].to(\"cpu\")))\n",
    "        #_, predictions = torch.max(outputs, 1)\n",
    "        # collect the correct predictions for each class\n",
    "        #for label, prediction in zip(labels, predictions):\n",
    "        #    if label == prediction:\n",
    "        #        correct_pred[classes[label]] += 1\n",
    "        #    total_pred[classes[label]] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "817479e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print accuracy for each class\n",
    "#for classname, correct_count in correct_pred.items():\n",
    "##    accuracy = 100 * float(correct_count) / total_pred[classname]\n",
    " #   print(\"Accuracy for class {:5s} is: {:.1f} %\".format(classname,\n",
    "                                                   accuracy))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
