<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<meta http-equiv="Content-Type" content="text/xhtml;charset=UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=9"/>
<meta name="generator" content="Doxygen 1.8.13"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<title>GrassRobotics Common: /home/jose/ros_ws/src/gr_perception/gr_ml/nb/vision/references/classification/README.md Source File</title>
<link href="tabs.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="jquery.js"></script>
<script type="text/javascript" src="dynsections.js"></script>
<link href="search/search.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="search/searchdata.js"></script>
<script type="text/javascript" src="search/search.js"></script>
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    extensions: ["tex2jax.js", "TeX/AMSmath.js", "TeX/AMSsymbols.js"],
    jax: ["input/TeX","output/HTML-CSS"],
});
</script><script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js"></script>
<link href="doxygen.css" rel="stylesheet" type="text/css" />
</head>
<body>
<div id="top"><!-- do not remove this div, it is closed by doxygen! -->
<div id="titlearea">
<table cellspacing="0" cellpadding="0">
 <tbody>
 <tr style="height: 56px;">
  <td id="projectalign" style="padding-left: 0.5em;">
   <div id="projectname">GrassRobotics Common
   </div>
  </td>
 </tr>
 </tbody>
</table>
</div>
<!-- end header part -->
<!-- Generated by Doxygen 1.8.13 -->
<script type="text/javascript">
var searchBox = new SearchBox("searchBox", "search",false,'Search');
</script>
<script type="text/javascript" src="menudata.js"></script>
<script type="text/javascript" src="menu.js"></script>
<script type="text/javascript">
$(function() {
  initMenu('',true,false,'search.php','Search');
  $(document).ready(function() { init_search(); });
});
</script>
<div id="main-nav"></div>
</div><!-- top -->
<!-- window showing the filter options -->
<div id="MSearchSelectWindow"
     onmouseover="return searchBox.OnSearchSelectShow()"
     onmouseout="return searchBox.OnSearchSelectHide()"
     onkeydown="return searchBox.OnSearchSelectKey(event)">
</div>

<!-- iframe showing the search results (closed by default) -->
<div id="MSearchResultsWindow">
<iframe src="javascript:void(0)" frameborder="0" 
        name="MSearchResults" id="MSearchResults">
</iframe>
</div>

<div class="header">
  <div class="headertitle">
<div class="title">/home/jose/ros_ws/src/gr_perception/gr_ml/nb/vision/references/classification/README.md</div>  </div>
</div><!--header-->
<div class="contents">
<div class="fragment"><div class="line"><a name="l00001"></a><span class="lineno">    1</span>&#160;# Image classification reference training scripts</div><div class="line"><a name="l00002"></a><span class="lineno">    2</span>&#160;</div><div class="line"><a name="l00003"></a><span class="lineno">    3</span>&#160;This folder contains reference training scripts for image classification.</div><div class="line"><a name="l00004"></a><span class="lineno">    4</span>&#160;They serve as a log of how to train specific models, as provide baseline</div><div class="line"><a name="l00005"></a><span class="lineno">    5</span>&#160;training and evaluation scripts to quickly bootstrap research.</div><div class="line"><a name="l00006"></a><span class="lineno">    6</span>&#160;</div><div class="line"><a name="l00007"></a><span class="lineno">    7</span>&#160;Except otherwise noted, all models have been trained on 8x V100 GPUs with </div><div class="line"><a name="l00008"></a><span class="lineno">    8</span>&#160;the following parameters:</div><div class="line"><a name="l00009"></a><span class="lineno">    9</span>&#160;</div><div class="line"><a name="l00010"></a><span class="lineno">   10</span>&#160;| Parameter                | value  |</div><div class="line"><a name="l00011"></a><span class="lineno">   11</span>&#160;| ------------------------ | ------ |</div><div class="line"><a name="l00012"></a><span class="lineno">   12</span>&#160;| `--batch_size`           | `32`   |</div><div class="line"><a name="l00013"></a><span class="lineno">   13</span>&#160;| `--epochs`               | `90`   |</div><div class="line"><a name="l00014"></a><span class="lineno">   14</span>&#160;| `--lr`                   | `0.1`  |</div><div class="line"><a name="l00015"></a><span class="lineno">   15</span>&#160;| `--momentum`             | `0.9`  |</div><div class="line"><a name="l00016"></a><span class="lineno">   16</span>&#160;| `--wd`, `--weight-decay` | `1e-4` |</div><div class="line"><a name="l00017"></a><span class="lineno">   17</span>&#160;| `--lr-step-size`         | `30`   |</div><div class="line"><a name="l00018"></a><span class="lineno">   18</span>&#160;| `--lr-gamma`             | `0.1`  |</div><div class="line"><a name="l00019"></a><span class="lineno">   19</span>&#160;</div><div class="line"><a name="l00020"></a><span class="lineno">   20</span>&#160;### AlexNet and VGG</div><div class="line"><a name="l00021"></a><span class="lineno">   21</span>&#160;</div><div class="line"><a name="l00022"></a><span class="lineno">   22</span>&#160;Since `AlexNet` and the original `VGG` architectures do not include batch </div><div class="line"><a name="l00023"></a><span class="lineno">   23</span>&#160;normalization, the default initial learning rate `--lr 0.1` is to high.</div><div class="line"><a name="l00024"></a><span class="lineno">   24</span>&#160;</div><div class="line"><a name="l00025"></a><span class="lineno">   25</span>&#160;```</div><div class="line"><a name="l00026"></a><span class="lineno">   26</span>&#160;python main.py --model $MODEL --lr 1e-2</div><div class="line"><a name="l00027"></a><span class="lineno">   27</span>&#160;```</div><div class="line"><a name="l00028"></a><span class="lineno">   28</span>&#160;</div><div class="line"><a name="l00029"></a><span class="lineno">   29</span>&#160;Here `$MODEL` is one of `alexnet`, `vgg11`, `vgg13`, `vgg16` or `vgg19`. Note</div><div class="line"><a name="l00030"></a><span class="lineno">   30</span>&#160;that `vgg11_bn`, `vgg13_bn`, `vgg16_bn`, and `vgg19_bn` include batch</div><div class="line"><a name="l00031"></a><span class="lineno">   31</span>&#160;normalization and thus are trained with the default parameters.</div><div class="line"><a name="l00032"></a><span class="lineno">   32</span>&#160;</div><div class="line"><a name="l00033"></a><span class="lineno">   33</span>&#160;### ResNext-50 32x4d</div><div class="line"><a name="l00034"></a><span class="lineno">   34</span>&#160;```</div><div class="line"><a name="l00035"></a><span class="lineno">   35</span>&#160;python -m torch.distributed.launch --nproc_per_node=8 --use_env train.py\</div><div class="line"><a name="l00036"></a><span class="lineno">   36</span>&#160;    --model resnext50_32x4d --epochs 100</div><div class="line"><a name="l00037"></a><span class="lineno">   37</span>&#160;```</div><div class="line"><a name="l00038"></a><span class="lineno">   38</span>&#160;</div><div class="line"><a name="l00039"></a><span class="lineno">   39</span>&#160;</div><div class="line"><a name="l00040"></a><span class="lineno">   40</span>&#160;### ResNext-101 32x8d</div><div class="line"><a name="l00041"></a><span class="lineno">   41</span>&#160;</div><div class="line"><a name="l00042"></a><span class="lineno">   42</span>&#160;On 8 nodes, each with 8 GPUs (for a total of 64 GPUS)</div><div class="line"><a name="l00043"></a><span class="lineno">   43</span>&#160;```</div><div class="line"><a name="l00044"></a><span class="lineno">   44</span>&#160;python -m torch.distributed.launch --nproc_per_node=8 --use_env train.py\</div><div class="line"><a name="l00045"></a><span class="lineno">   45</span>&#160;    --model resnext101_32x8d --epochs 100</div><div class="line"><a name="l00046"></a><span class="lineno">   46</span>&#160;```</div><div class="line"><a name="l00047"></a><span class="lineno">   47</span>&#160;</div><div class="line"><a name="l00048"></a><span class="lineno">   48</span>&#160;</div><div class="line"><a name="l00049"></a><span class="lineno">   49</span>&#160;### MobileNetV2</div><div class="line"><a name="l00050"></a><span class="lineno">   50</span>&#160;```</div><div class="line"><a name="l00051"></a><span class="lineno">   51</span>&#160;python -m torch.distributed.launch --nproc_per_node=8 --use_env train.py\</div><div class="line"><a name="l00052"></a><span class="lineno">   52</span>&#160;     --model mobilenet_v2 --epochs 300 --lr 0.045 --wd 0.00004\</div><div class="line"><a name="l00053"></a><span class="lineno">   53</span>&#160;     --lr-step-size 1 --lr-gamma 0.98</div><div class="line"><a name="l00054"></a><span class="lineno">   54</span>&#160;```</div><div class="line"><a name="l00055"></a><span class="lineno">   55</span>&#160;</div><div class="line"><a name="l00056"></a><span class="lineno">   56</span>&#160;## Mixed precision training</div><div class="line"><a name="l00057"></a><span class="lineno">   57</span>&#160;Automatic Mixed Precision (AMP) training on GPU for Pytorch can be enabled with the [NVIDIA Apex extension](https://github.com/NVIDIA/apex).</div><div class="line"><a name="l00058"></a><span class="lineno">   58</span>&#160;</div><div class="line"><a name="l00059"></a><span class="lineno">   59</span>&#160;Mixed precision training makes use of both FP32 and FP16 precisions where appropriate. FP16 operations can leverage the Tensor cores on NVIDIA GPUs (Volta, Turing or newer architectures) for improved throughput, generally without loss in model accuracy. Mixed precision training also often allows larger batch sizes. GPU automatic mixed precision training for Pytorch Vision can be enabled via the flag value `--apex=True`.</div><div class="line"><a name="l00060"></a><span class="lineno">   60</span>&#160;</div><div class="line"><a name="l00061"></a><span class="lineno">   61</span>&#160;```</div><div class="line"><a name="l00062"></a><span class="lineno">   62</span>&#160;python -m torch.distributed.launch --nproc_per_node=8 --use_env train.py\</div><div class="line"><a name="l00063"></a><span class="lineno">   63</span>&#160;    --model resnext50_32x4d --epochs 100 --apex</div><div class="line"><a name="l00064"></a><span class="lineno">   64</span>&#160;```</div><div class="line"><a name="l00065"></a><span class="lineno">   65</span>&#160;</div><div class="line"><a name="l00066"></a><span class="lineno">   66</span>&#160;## Quantized</div><div class="line"><a name="l00067"></a><span class="lineno">   67</span>&#160;### INT8 models</div><div class="line"><a name="l00068"></a><span class="lineno">   68</span>&#160;We add INT8 quantized models to follow the quantization support added in PyTorch 1.3. </div><div class="line"><a name="l00069"></a><span class="lineno">   69</span>&#160;</div><div class="line"><a name="l00070"></a><span class="lineno">   70</span>&#160;Obtaining a pre-trained quantized model can be obtained with a few lines of code:</div><div class="line"><a name="l00071"></a><span class="lineno">   71</span>&#160;```</div><div class="line"><a name="l00072"></a><span class="lineno">   72</span>&#160;model = torchvision.models.quantization.mobilenet_v2(pretrained=True, quantize=True)</div><div class="line"><a name="l00073"></a><span class="lineno">   73</span>&#160;model.eval()</div><div class="line"><a name="l00074"></a><span class="lineno">   74</span>&#160;# run the model with quantized inputs and weights</div><div class="line"><a name="l00075"></a><span class="lineno">   75</span>&#160;out = model(torch.rand(1, 3, 224, 224))</div><div class="line"><a name="l00076"></a><span class="lineno">   76</span>&#160;```</div><div class="line"><a name="l00077"></a><span class="lineno">   77</span>&#160;We provide pre-trained quantized weights for the following models:</div><div class="line"><a name="l00078"></a><span class="lineno">   78</span>&#160;</div><div class="line"><a name="l00079"></a><span class="lineno">   79</span>&#160;|       Model       |  Acc@1 |  Acc@5 |</div><div class="line"><a name="l00080"></a><span class="lineno">   80</span>&#160;|:-----------------:|:------:|:------:|</div><div class="line"><a name="l00081"></a><span class="lineno">   81</span>&#160;|    MobileNet V2   | 71.658 | 90.150 |</div><div class="line"><a name="l00082"></a><span class="lineno">   82</span>&#160;|   ShuffleNet V2:  | 68.360 | 87.582 |</div><div class="line"><a name="l00083"></a><span class="lineno">   83</span>&#160;|     ResNet 18     | 69.494 | 88.882 |</div><div class="line"><a name="l00084"></a><span class="lineno">   84</span>&#160;|     ResNet 50     | 75.920 | 92.814 |</div><div class="line"><a name="l00085"></a><span class="lineno">   85</span>&#160;| ResNext 101 32x8d | 78.986 | 94.480 |</div><div class="line"><a name="l00086"></a><span class="lineno">   86</span>&#160;|    Inception V3   | 77.176 | 93.354 |</div><div class="line"><a name="l00087"></a><span class="lineno">   87</span>&#160;|     GoogleNet     | 69.826 | 89.404 |</div><div class="line"><a name="l00088"></a><span class="lineno">   88</span>&#160;</div><div class="line"><a name="l00089"></a><span class="lineno">   89</span>&#160;### Parameters used for generating quantized models:</div><div class="line"><a name="l00090"></a><span class="lineno">   90</span>&#160;</div><div class="line"><a name="l00091"></a><span class="lineno">   91</span>&#160;For all post training quantized models (All quantized models except mobilenet-v2), the settings are:</div><div class="line"><a name="l00092"></a><span class="lineno">   92</span>&#160;</div><div class="line"><a name="l00093"></a><span class="lineno">   93</span>&#160;1. num_calibration_batches: 32</div><div class="line"><a name="l00094"></a><span class="lineno">   94</span>&#160;2. num_workers: 16</div><div class="line"><a name="l00095"></a><span class="lineno">   95</span>&#160;3. batch_size: 32</div><div class="line"><a name="l00096"></a><span class="lineno">   96</span>&#160;4. eval_batch_size: 128</div><div class="line"><a name="l00097"></a><span class="lineno">   97</span>&#160;5. backend: &#39;fbgemm&#39;</div><div class="line"><a name="l00098"></a><span class="lineno">   98</span>&#160;</div><div class="line"><a name="l00099"></a><span class="lineno">   99</span>&#160;For Mobilenet-v2, the model was trained with quantization aware training, the settings used are:</div><div class="line"><a name="l00100"></a><span class="lineno">  100</span>&#160;1. num_workers: 16</div><div class="line"><a name="l00101"></a><span class="lineno">  101</span>&#160;2. batch_size: 32</div><div class="line"><a name="l00102"></a><span class="lineno">  102</span>&#160;3. eval_batch_size: 128</div><div class="line"><a name="l00103"></a><span class="lineno">  103</span>&#160;4. backend: &#39;qnnpack&#39;</div><div class="line"><a name="l00104"></a><span class="lineno">  104</span>&#160;5. learning-rate: 0.0001</div><div class="line"><a name="l00105"></a><span class="lineno">  105</span>&#160;6. num_epochs: 90</div><div class="line"><a name="l00106"></a><span class="lineno">  106</span>&#160;7. num_observer_update_epochs:4</div><div class="line"><a name="l00107"></a><span class="lineno">  107</span>&#160;8. num_batch_norm_update_epochs:3</div><div class="line"><a name="l00108"></a><span class="lineno">  108</span>&#160;9. momentum: 0.9</div><div class="line"><a name="l00109"></a><span class="lineno">  109</span>&#160;10. lr_step_size:30</div><div class="line"><a name="l00110"></a><span class="lineno">  110</span>&#160;11. lr_gamma: 0.1</div><div class="line"><a name="l00111"></a><span class="lineno">  111</span>&#160;</div><div class="line"><a name="l00112"></a><span class="lineno">  112</span>&#160;Training converges at about 10 epochs.</div><div class="line"><a name="l00113"></a><span class="lineno">  113</span>&#160;</div><div class="line"><a name="l00114"></a><span class="lineno">  114</span>&#160;For post training quant, device is set to CPU. For training, the device is set to CUDA</div><div class="line"><a name="l00115"></a><span class="lineno">  115</span>&#160;</div><div class="line"><a name="l00116"></a><span class="lineno">  116</span>&#160;### Command to evaluate quantized models using the pre-trained weights:</div><div class="line"><a name="l00117"></a><span class="lineno">  117</span>&#160;For all quantized models except inception_v3:</div><div class="line"><a name="l00118"></a><span class="lineno">  118</span>&#160;```</div><div class="line"><a name="l00119"></a><span class="lineno">  119</span>&#160;python references/classification/train_quantization.py  --data-path=&#39;imagenet_full_size/&#39; \</div><div class="line"><a name="l00120"></a><span class="lineno">  120</span>&#160;    --device=&#39;cpu&#39; --test-only --backend=&#39;fbgemm&#39; --model=&#39;&lt;model_name&gt;&#39;</div><div class="line"><a name="l00121"></a><span class="lineno">  121</span>&#160;```</div><div class="line"><a name="l00122"></a><span class="lineno">  122</span>&#160;</div><div class="line"><a name="l00123"></a><span class="lineno">  123</span>&#160;For inception_v3, since it expects tensors with a size of N x 3 x 299 x 299, before running above command,</div><div class="line"><a name="l00124"></a><span class="lineno">  124</span>&#160;need to change the input size of dataset_test in train.py to:</div><div class="line"><a name="l00125"></a><span class="lineno">  125</span>&#160;```</div><div class="line"><a name="l00126"></a><span class="lineno">  126</span>&#160;dataset_test = torchvision.datasets.ImageFolder(</div><div class="line"><a name="l00127"></a><span class="lineno">  127</span>&#160;    valdir,</div><div class="line"><a name="l00128"></a><span class="lineno">  128</span>&#160;    transforms.Compose([</div><div class="line"><a name="l00129"></a><span class="lineno">  129</span>&#160;        transforms.Resize(342),</div><div class="line"><a name="l00130"></a><span class="lineno">  130</span>&#160;        transforms.CenterCrop(299),</div><div class="line"><a name="l00131"></a><span class="lineno">  131</span>&#160;        transforms.ToTensor(),</div><div class="line"><a name="l00132"></a><span class="lineno">  132</span>&#160;        normalize,</div><div class="line"><a name="l00133"></a><span class="lineno">  133</span>&#160;    ]))</div><div class="line"><a name="l00134"></a><span class="lineno">  134</span>&#160;```</div><div class="line"><a name="l00135"></a><span class="lineno">  135</span>&#160;</div></div><!-- fragment --></div><!-- contents -->
<!-- start footer part -->
<hr class="footer"/><address class="footer"><small>
Generated by &#160;<a href="http://www.doxygen.org/index.html">
<img class="footer" src="doxygen.png" alt="doxygen"/>
</a> 1.8.13
</small></address>
</body>
</html>
