\hypertarget{classtorchvision_1_1datasets_1_1ucf101_1_1UCF101}{}\section{torchvision.\+datasets.\+ucf101.\+U\+C\+F101 Class Reference}
\label{classtorchvision_1_1datasets_1_1ucf101_1_1UCF101}\index{torchvision.\+datasets.\+ucf101.\+U\+C\+F101@{torchvision.\+datasets.\+ucf101.\+U\+C\+F101}}


Inheritance diagram for torchvision.\+datasets.\+ucf101.\+U\+C\+F101\+:
\nopagebreak
\begin{figure}[H]
\begin{center}
\leavevmode
\includegraphics[width=220pt]{classtorchvision_1_1datasets_1_1ucf101_1_1UCF101__inherit__graph}
\end{center}
\end{figure}


Collaboration diagram for torchvision.\+datasets.\+ucf101.\+U\+C\+F101\+:
\nopagebreak
\begin{figure}[H]
\begin{center}
\leavevmode
\includegraphics[width=220pt]{classtorchvision_1_1datasets_1_1ucf101_1_1UCF101__coll__graph}
\end{center}
\end{figure}
\subsection*{Public Member Functions}
\begin{DoxyCompactItemize}
\item 
\mbox{\Hypertarget{classtorchvision_1_1datasets_1_1ucf101_1_1UCF101_a9589cf9a9ef3379849cac59d97a93bf0}\label{classtorchvision_1_1datasets_1_1ucf101_1_1UCF101_a9589cf9a9ef3379849cac59d97a93bf0}} 
def {\bfseries \+\_\+\+\_\+init\+\_\+\+\_\+} (self, root, annotation\+\_\+path, frames\+\_\+per\+\_\+clip, step\+\_\+between\+\_\+clips=1, frame\+\_\+rate=None, fold=1, train=True, transform=None, \+\_\+precomputed\+\_\+metadata=None, num\+\_\+workers=1, \+\_\+video\+\_\+width=0, \+\_\+video\+\_\+height=0, \+\_\+video\+\_\+min\+\_\+dimension=0, \+\_\+audio\+\_\+samples=0)
\item 
\mbox{\Hypertarget{classtorchvision_1_1datasets_1_1ucf101_1_1UCF101_aef8c50560cef014c36359d409da86954}\label{classtorchvision_1_1datasets_1_1ucf101_1_1UCF101_aef8c50560cef014c36359d409da86954}} 
def {\bfseries metadata} (self)
\item 
\mbox{\Hypertarget{classtorchvision_1_1datasets_1_1ucf101_1_1UCF101_a5ff91666b1db5d2cde5fbb820f5f461c}\label{classtorchvision_1_1datasets_1_1ucf101_1_1UCF101_a5ff91666b1db5d2cde5fbb820f5f461c}} 
def {\bfseries \+\_\+\+\_\+len\+\_\+\+\_\+} (self)
\item 
\mbox{\Hypertarget{classtorchvision_1_1datasets_1_1ucf101_1_1UCF101_a322e4af0f2f80a769344384ecd077333}\label{classtorchvision_1_1datasets_1_1ucf101_1_1UCF101_a322e4af0f2f80a769344384ecd077333}} 
def {\bfseries \+\_\+\+\_\+getitem\+\_\+\+\_\+} (self, idx)
\end{DoxyCompactItemize}
\subsection*{Data Fields}
\begin{DoxyCompactItemize}
\item 
\mbox{\Hypertarget{classtorchvision_1_1datasets_1_1ucf101_1_1UCF101_a88c830ff8e638e6eff2bc7e38cfdc38b}\label{classtorchvision_1_1datasets_1_1ucf101_1_1UCF101_a88c830ff8e638e6eff2bc7e38cfdc38b}} 
{\bfseries fold}
\item 
\mbox{\Hypertarget{classtorchvision_1_1datasets_1_1ucf101_1_1UCF101_a59d3de6ba8ce12df13600b6c45e9e014}\label{classtorchvision_1_1datasets_1_1ucf101_1_1UCF101_a59d3de6ba8ce12df13600b6c45e9e014}} 
{\bfseries train}
\item 
\mbox{\Hypertarget{classtorchvision_1_1datasets_1_1ucf101_1_1UCF101_a88f0d0074e95fc7aad32a66eeeaa2299}\label{classtorchvision_1_1datasets_1_1ucf101_1_1UCF101_a88f0d0074e95fc7aad32a66eeeaa2299}} 
{\bfseries samples}
\item 
\mbox{\Hypertarget{classtorchvision_1_1datasets_1_1ucf101_1_1UCF101_a22519feec1ed5847e3f4671e994d8b13}\label{classtorchvision_1_1datasets_1_1ucf101_1_1UCF101_a22519feec1ed5847e3f4671e994d8b13}} 
{\bfseries classes}
\item 
\mbox{\Hypertarget{classtorchvision_1_1datasets_1_1ucf101_1_1UCF101_a5c6a354974d30c47ad0dbd4f7b410873}\label{classtorchvision_1_1datasets_1_1ucf101_1_1UCF101_a5c6a354974d30c47ad0dbd4f7b410873}} 
{\bfseries video\+\_\+clips\+\_\+metadata}
\item 
\mbox{\Hypertarget{classtorchvision_1_1datasets_1_1ucf101_1_1UCF101_a161b1be133b9e01fafb2206b430bde48}\label{classtorchvision_1_1datasets_1_1ucf101_1_1UCF101_a161b1be133b9e01fafb2206b430bde48}} 
{\bfseries indices}
\item 
\mbox{\Hypertarget{classtorchvision_1_1datasets_1_1ucf101_1_1UCF101_af55c9a255355f95f92c69cb7a3968a41}\label{classtorchvision_1_1datasets_1_1ucf101_1_1UCF101_af55c9a255355f95f92c69cb7a3968a41}} 
{\bfseries video\+\_\+clips}
\item 
\mbox{\Hypertarget{classtorchvision_1_1datasets_1_1ucf101_1_1UCF101_af64b5b0017c0d78b6c54216b008be1d8}\label{classtorchvision_1_1datasets_1_1ucf101_1_1UCF101_af64b5b0017c0d78b6c54216b008be1d8}} 
{\bfseries transform}
\end{DoxyCompactItemize}


\subsection{Detailed Description}
\begin{DoxyVerb}`UCF101 <https://www.crcv.ucf.edu/data/UCF101.php>`_ dataset.

UCF101 is an action recognition video dataset.
This dataset consider every video as a collection of video clips of fixed size, specified
by ``frames_per_clip``, where the step in frames between each clip is given by
``step_between_clips``.

To give an example, for 2 videos with 10 and 15 frames respectively, if ``frames_per_clip=5``
and ``step_between_clips=5``, the dataset size will be (2 + 3) = 5, where the first two
elements will come from video 1, and the next three elements from video 2.
Note that we drop clips which do not have exactly ``frames_per_clip`` elements, so not all
frames in a video might be present.

Internally, it uses a VideoClips object to handle clip creation.

Args:
    root (string): Root directory of the UCF101 Dataset.
    annotation_path (str): path to the folder containing the split files
    frames_per_clip (int): number of frames in a clip.
    step_between_clips (int, optional): number of frames between each clip.
    fold (int, optional): which fold to use. Should be between 1 and 3.
    train (bool, optional): if ``True``, creates a dataset from the train split,
        otherwise from the ``test`` split.
    transform (callable, optional): A function/transform that  takes in a TxHxWxC video
        and returns a transformed version.

Returns:
    video (Tensor[T, H, W, C]): the `T` video frames
    audio(Tensor[K, L]): the audio frames, where `K` is the number of channels
        and `L` is the number of points
    label (int): class of the video clip
\end{DoxyVerb}
 

Definition at line 10 of file ucf101.\+py.



The documentation for this class was generated from the following file\+:\begin{DoxyCompactItemize}
\item 
/home/jose/ros\+\_\+ws/src/gr\+\_\+perception/gr\+\_\+ml/nb/vision/torchvision/datasets/ucf101.\+py\end{DoxyCompactItemize}
